
2019-05-25 16:44:47
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Running Spark version 1.6.3

2019-05-25 16:44:58
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Changing view acls to: 511921540

2019-05-25 16:44:58
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Changing modify acls to: 511921540

2019-05-25 16:44:58
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(511921540); users with modify permissions: Set(511921540)

2019-05-25 16:44:59
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Successfully started service 'sparkDriver' on port 60825.

2019-05-25 16:44:59
[INFO]-[Thread: sparkDriverActorSystem-akka.actor.default-dispatcher-2]-[akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse()]: Slf4jLogger started

2019-05-25 16:44:59
[INFO]-[Thread: sparkDriverActorSystem-akka.actor.default-dispatcher-2]-[akka.event.slf4j.Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3.apply$mcV$sp()]: Starting remoting

2019-05-25 16:45:00
[INFO]-[Thread: sparkDriverActorSystem-akka.actor.default-dispatcher-2]-[akka.event.slf4j.Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3.apply$mcV$sp()]: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.12.0.198:60838]

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Successfully started service 'sparkDriverActorSystem' on port 60838.

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Registering MapOutputTracker

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Registering BlockManagerMaster

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Created local directory at C:\Users\511921540\AppData\Local\Temp\blockmgr-96e1c5cc-23c8-4904-a262-11459a7a63f4

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: MemoryStore started with capacity 2.4 GB

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Registering OutputCommitCoordinator

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.spark-project.jetty.server.Server.doStart()]: jetty-8.y.z-SNAPSHOT

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.spark-project.jetty.server.AbstractConnector.doStart()]: Started SelectChannelConnector@0.0.0.0:4040

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Successfully started service 'SparkUI' on port 4040.

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Started SparkUI at http://10.12.0.198:4040

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Starting executor ID driver on host localhost

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60851.

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Server created on 60851

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Trying to register BlockManager

2019-05-25 16:45:00
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Registering block manager localhost:60851 with 2.4 GB RAM, BlockManagerId(driver, localhost, 60851)

2019-05-25 16:45:00
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Registered BlockManager

2019-05-25 16:45:02
[WARN]-[Thread: main]-[tachyon.util.network.NetworkAddressUtils.getLocalIpAddress()]: Your hostname, LAPTOP-NFE4JUQT resolves to a loopback/non-reachable address: fe80:0:0:0:b0e1:301a:a394:7741%20, but we couldn't find any external IP address!

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Starting 1 receivers

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: ReceiverTracker started

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: metadataCleanupDelay = -1

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: metadataCleanupDelay = -1

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: metadataCleanupDelay = -1

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Slide time = 30000 ms

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Storage level = StorageLevel(false, false, false, false, 1)

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Checkpoint interval = null

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Remember duration = 30000 ms

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Initialized and validated org.apache.spark.streaming.kafka.KafkaInputDStream@2718efe2

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Slide time = 30000 ms

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Storage level = StorageLevel(false, false, false, false, 1)

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Checkpoint interval = null

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Remember duration = 30000 ms

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@2de4168c

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Slide time = 30000 ms

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Storage level = StorageLevel(false, false, false, false, 1)

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Checkpoint interval = null

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Remember duration = 30000 ms

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2d31b495

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Started timer for JobGenerator at time 1558773930000

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Started JobGenerator at 1558773930000 ms

2019-05-25 16:45:04
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Started JobScheduler

2019-05-25 16:45:04
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: StreamingContext started

2019-05-25 16:45:04
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:04
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 0 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:04
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 0 (start at AccessLogJob.scala:578)

2019-05-25 16:45:04
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:04
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:04
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:588), which has no missing parents

2019-05-25 16:45:04
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_0 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:04
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:04
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_0_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:04
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 0 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:04
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:588)

2019-05-25 16:45:04
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 0.0 with 1 tasks

2019-05-25 16:45:04
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:04
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 0.0 (TID 0)

2019-05-25 16:45:04
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773905000

2019-05-25 16:45:04
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:04
[INFO]-[Thread: Thread-18]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:04
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:04
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:04
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:04
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:04
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:04
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:04
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:04
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:04
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773904942-3234aae8], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:04
[INFO]-[Thread: ZkClient-EventThread-90-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:host.name=LAPTOP-NFE4JUQT

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.version=1.7.0_80

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.vendor=Oracle Corporation

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.home=C:\Program Files\Java\jdk1.7.0_80\jre

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.class.path=C:\Program Files\Java\jdk1.7.0_80\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\jce.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\jfxrt.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\resources.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\rt.jar;C:\Users\511921540\Desktop\datamanger\log-streaming\target\classes;D:\maven\repositories\org\scala-lang\scala-library\2.10.5\scala-library-2.10.5.jar;D:\maven\repositories\org\apache\spark\spark-core_2.10\1.6.3\spark-core_2.10-1.6.3.jar;D:\maven\repositories\org\apache\avro\avro-mapred\1.7.7\avro-mapred-1.7.7-hadoop2.jar;D:\maven\repositories\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7.jar;D:\maven\repositories\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7-tests.jar;D:\maven\repositories\com\twitter\chill_2.10\0.5.0\chill_2.10-0.5.0.jar;D:\maven\repositories\com\esotericsoftware\kryo\kryo\2.21\kryo-2.21.jar;D:\maven\repositories\com\esotericsoftware\reflectasm\reflectasm\1.07\reflectasm-1.07-shaded.jar;D:\maven\repositories\com\esotericsoftware\minlog\minlog\1.2\minlog-1.2.jar;D:\maven\repositories\org\objenesis\objenesis\1.2\objenesis-1.2.jar;D:\maven\repositories\com\twitter\chill-java\0.5.0\chill-java-0.5.0.jar;D:\maven\repositories\org\apache\xbean\xbean-asm5-shaded\4.4\xbean-asm5-shaded-4.4.jar;D:\maven\repositories\org\apache\hadoop\hadoop-client\2.2.0\hadoop-client-2.2.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-mapreduce-client-app\2.2.0\hadoop-mapreduce-client-app-2.2.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-mapreduce-client-common\2.2.0\hadoop-mapreduce-client-common-2.2.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-yarn-client\2.2.0\hadoop-yarn-client-2.2.0.jar;D:\maven\repositories\com\sun\jersey\jersey-test-framework\jersey-test-framework-grizzly2\1.9\jersey-test-framework-grizzly2-1.9.jar;D:\maven\repositories\com\sun\jersey\jersey-test-framework\jersey-test-framework-core\1.9\jersey-test-framework-core-1.9.jar;D:\maven\repositories\javax\servlet\javax.servlet-api\3.0.1\javax.servlet-api-3.0.1.jar;D:\maven\repositories\com\sun\jersey\jersey-grizzly2\1.9\jersey-grizzly2-1.9.jar;D:\maven\repositories\org\glassfish\grizzly\grizzly-http\2.1.2\grizzly-http-2.1.2.jar;D:\maven\repositories\org\glassfish\grizzly\grizzly-framework\2.1.2\grizzly-framework-2.1.2.jar;D:\maven\repositories\org\glassfish\gmbal\gmbal-api-only\3.0.0-b023\gmbal-api-only-3.0.0-b023.jar;D:\maven\repositories\org\glassfish\external\management-api\3.0.0-b012\management-api-3.0.0-b012.jar;D:\maven\repositories\org\glassfish\grizzly\grizzly-http-server\2.1.2\grizzly-http-server-2.1.2.jar;D:\maven\repositories\org\glassfish\grizzly\grizzly-rcm\2.1.2\grizzly-rcm-2.1.2.jar;D:\maven\repositories\org\glassfish\grizzly\grizzly-http-servlet\2.1.2\grizzly-http-servlet-2.1.2.jar;D:\maven\repositories\org\glassfish\javax.servlet\3.1\javax.servlet-3.1.jar;D:\maven\repositories\org\apache\hadoop\hadoop-yarn-server-common\2.2.0\hadoop-yarn-server-common-2.2.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.2.0\hadoop-mapreduce-client-shuffle-2.2.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-yarn-api\2.2.0\hadoop-yarn-api-2.2.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.2.0\hadoop-mapreduce-client-jobclient-2.2.0.jar;D:\maven\repositories\org\apache\spark\spark-launcher_2.10\1.6.3\spark-launcher_2.10-1.6.3.jar;D:\maven\repositories\org\apache\spark\spark-network-common_2.10\1.6.3\spark-network-common_2.10-1.6.3.jar;D:\maven\repositories\org\apache\spark\spark-network-shuffle_2.10\1.6.3\spark-network-shuffle_2.10-1.6.3.jar;D:\maven\repositories\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;D:\maven\repositories\com\fasterxml\jackson\core\jackson-annotations\2.4.4\jackson-annotations-2.4.4.jar;D:\maven\repositories\org\apache\spark\spark-unsafe_2.10\1.6.3\spark-unsafe_2.10-1.6.3.jar;D:\maven\repositories\net\java\dev\jets3t\jets3t\0.7.1\jets3t-0.7.1.jar;D:\maven\repositories\org\apache\curator\curator-recipes\2.4.0\curator-recipes-2.4.0.jar;D:\maven\repositories\org\apache\curator\curator-framework\2.4.0\curator-framework-2.4.0.jar;D:\maven\repositories\org\apache\curator\curator-client\2.4.0\curator-client-2.4.0.jar;D:\maven\repositories\org\eclipse\jetty\orbit\javax.servlet\3.0.0.v201112011016\javax.servlet-3.0.0.v201112011016.jar;D:\maven\repositories\org\apache\commons\commons-lang3\3.3.2\commons-lang3-3.3.2.jar;D:\maven\repositories\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;D:\maven\repositories\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;D:\maven\repositories\org\slf4j\slf4j-api\1.7.10\slf4j-api-1.7.10.jar;D:\maven\repositories\org\slf4j\jul-to-slf4j\1.7.10\jul-to-slf4j-1.7.10.jar;D:\maven\repositories\org\slf4j\jcl-over-slf4j\1.7.10\jcl-over-slf4j-1.7.10.jar;D:\maven\repositories\log4j\log4j\1.2.17\log4j-1.2.17.jar;D:\maven\repositories\org\slf4j\slf4j-log4j12\1.7.10\slf4j-log4j12-1.7.10.jar;D:\maven\repositories\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;D:\maven\repositories\org\xerial\snappy\snappy-java\1.1.2.6\snappy-java-1.1.2.6.jar;D:\maven\repositories\net\jpountz\lz4\lz4\1.3.0\lz4-1.3.0.jar;D:\maven\repositories\org\roaringbitmap\RoaringBitmap\0.5.11\RoaringBitmap-0.5.11.jar;D:\maven\repositories\commons-net\commons-net\2.2\commons-net-2.2.jar;D:\maven\repositories\com\typesafe\akka\akka-remote_2.10\2.3.11\akka-remote_2.10-2.3.11.jar;D:\maven\repositories\com\typesafe\akka\akka-actor_2.10\2.3.11\akka-actor_2.10-2.3.11.jar;D:\maven\repositories\com\typesafe\config\1.2.1\config-1.2.1.jar;D:\maven\repositories\org\uncommons\maths\uncommons-maths\1.2.2a\uncommons-maths-1.2.2a.jar;D:\maven\repositories\com\typesafe\akka\akka-slf4j_2.10\2.3.11\akka-slf4j_2.10-2.3.11.jar;D:\maven\repositories\org\json4s\json4s-jackson_2.10\3.2.10\json4s-jackson_2.10-3.2.10.jar;D:\maven\repositories\org\json4s\json4s-core_2.10\3.2.10\json4s-core_2.10-3.2.10.jar;D:\maven\repositories\org\json4s\json4s-ast_2.10\3.2.10\json4s-ast_2.10-3.2.10.jar;D:\maven\repositories\org\scala-lang\scalap\2.10.0\scalap-2.10.0.jar;D:\maven\repositories\org\scala-lang\scala-compiler\2.10.0\scala-compiler-2.10.0.jar;D:\maven\repositories\com\sun\jersey\jersey-server\1.9\jersey-server-1.9.jar;D:\maven\repositories\asm\asm\3.1\asm-3.1.jar;D:\maven\repositories\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;D:\maven\repositories\org\apache\mesos\mesos\0.21.1\mesos-0.21.1-shaded-protobuf.jar;D:\maven\repositories\io\netty\netty-all\4.0.29.Final\netty-all-4.0.29.Final.jar;D:\maven\repositories\com\clearspring\analytics\stream\2.7.0\stream-2.7.0.jar;D:\maven\repositories\io\dropwizard\metrics\metrics-core\3.1.2\metrics-core-3.1.2.jar;D:\maven\repositories\io\dropwizard\metrics\metrics-jvm\3.1.2\metrics-jvm-3.1.2.jar;D:\maven\repositories\io\dropwizard\metrics\metrics-json\3.1.2\metrics-json-3.1.2.jar;D:\maven\repositories\io\dropwizard\metrics\metrics-graphite\3.1.2\metrics-graphite-3.1.2.jar;D:\maven\repositories\com\fasterxml\jackson\core\jackson-databind\2.4.4\jackson-databind-2.4.4.jar;D:\maven\repositories\com\fasterxml\jackson\core\jackson-core\2.4.4\jackson-core-2.4.4.jar;D:\maven\repositories\com\fasterxml\jackson\module\jackson-module-scala_2.10\2.4.4\jackson-module-scala_2.10-2.4.4.jar;D:\maven\repositories\org\scala-lang\scala-reflect\2.10.4\scala-reflect-2.10.4.jar;D:\maven\repositories\com\thoughtworks\paranamer\paranamer\2.6\paranamer-2.6.jar;D:\maven\repositories\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;D:\maven\repositories\oro\oro\2.0.8\oro-2.0.8.jar;D:\maven\repositories\org\tachyonproject\tachyon-client\0.8.2\tachyon-client-0.8.2.jar;D:\maven\repositories\org\tachyonproject\tachyon-underfs-hdfs\0.8.2\tachyon-underfs-hdfs-0.8.2.jar;D:\maven\repositories\org\tachyonproject\tachyon-underfs-s3\0.8.2\tachyon-underfs-s3-0.8.2.jar;D:\maven\repositories\org\tachyonproject\tachyon-underfs-local\0.8.2\tachyon-underfs-local-0.8.2.jar;D:\maven\repositories\net\razorvine\pyrolite\4.9\pyrolite-4.9.jar;D:\maven\repositories\net\sf\py4j\py4j\0.9\py4j-0.9.jar;D:\maven\repositories\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;D:\maven\repositories\org\apache\spark\spark-streaming_2.10\1.6.3\spark-streaming_2.10-1.6.3.jar;D:\maven\repositories\org\apache\spark\spark-sql_2.10\1.6.3\spark-sql_2.10-1.6.3.jar;D:\maven\repositories\org\apache\spark\spark-catalyst_2.10\1.6.3\spark-catalyst_2.10-1.6.3.jar;D:\maven\repositories\org\codehaus\janino\janino\2.7.8\janino-2.7.8.jar;D:\maven\repositories\org\codehaus\janino\commons-compiler\2.7.8\commons-compiler-2.7.8.jar;D:\maven\repositories\org\apache\parquet\parquet-column\1.7.0\parquet-column-1.7.0.jar;D:\maven\repositories\org\apache\parquet\parquet-common\1.7.0\parquet-common-1.7.0.jar;D:\maven\repositories\org\apache\parquet\parquet-encoding\1.7.0\parquet-encoding-1.7.0.jar;D:\maven\repositories\org\apache\parquet\parquet-generator\1.7.0\parquet-generator-1.7.0.jar;D:\maven\repositories\org\apache\parquet\parquet-hadoop\1.7.0\parquet-hadoop-1.7.0.jar;D:\maven\repositories\org\apache\parquet\parquet-format\2.3.0-incubating\parquet-format-2.3.0-incubating.jar;D:\maven\repositories\org\apache\parquet\parquet-jackson\1.7.0\parquet-jackson-1.7.0.jar;D:\maven\repositories\org\apache\spark\spark-streaming-kafka_2.10\1.6.3\spark-streaming-kafka_2.10-1.6.3.jar;D:\maven\repositories\org\apache\kafka\kafka_2.10\0.8.2.1\kafka_2.10-0.8.2.1.jar;D:\maven\repositories\org\apache\kafka\kafka-clients\0.8.2.1\kafka-clients-0.8.2.1.jar;D:\maven\repositories\com\101tec\zkclient\0.3\zkclient-0.3.jar;D:\maven\repositories\net\sf\json-lib\json-lib\2.4\json-lib-2.4-jdk15.jar;D:\maven\repositories\commons-beanutils\commons-beanutils\1.8.0\commons-beanutils-1.8.0.jar;D:\maven\repositories\commons-collections\commons-collections\3.2.1\commons-collections-3.2.1.jar;D:\maven\repositories\commons-lang\commons-lang\2.5\commons-lang-2.5.jar;D:\maven\repositories\commons-logging\commons-logging\1.1.1\commons-logging-1.1.1.jar;D:\maven\repositories\net\sf\ezmorph\ezmorph\1.0.6\ezmorph-1.0.6.jar;D:\maven\repositories\mysql\mysql-connector-java\5.1.29\mysql-connector-java-5.1.29.jar;D:\maven\repositories\com\jolbox\bonecp\0.8.0.RELEASE\bonecp-0.8.0.RELEASE.jar;D:\maven\repositories\com\google\guava\guava\15.0\guava-15.0.jar;D:\maven\repositories\org\apache\hbase\hbase-client\1.0.1\hbase-client-1.0.1.jar;D:\maven\repositories\org\apache\hbase\hbase-annotations\1.0.1\hbase-annotations-1.0.1.jar;D:\maven\repositories\org\apache\hbase\hbase-common\1.0.1\hbase-common-1.0.1.jar;D:\maven\repositories\org\apache\hbase\hbase-protocol\1.0.1\hbase-protocol-1.0.1.jar;D:\maven\repositories\commons-codec\commons-codec\1.9\commons-codec-1.9.jar;D:\maven\repositories\commons-io\commons-io\2.4\commons-io-2.4.jar;D:\maven\repositories\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;D:\maven\repositories\org\apache\htrace\htrace-core\3.1.0-incubating\htrace-core-3.1.0-incubating.jar;D:\maven\repositories\org\codehaus\jackson\jackson-mapper-asl\1.8.8\jackson-mapper-asl-1.8.8.jar;D:\maven\repositories\org\jruby\jcodings\jcodings\1.0.8\jcodings-1.0.8.jar;D:\maven\repositories\org\jruby\joni\joni\2.1.2\joni-2.1.2.jar;D:\maven\repositories\org\apache\hadoop\hadoop-auth\2.5.1\hadoop-auth-2.5.1.jar;D:\maven\repositories\org\apache\httpcomponents\httpclient\4.2.5\httpclient-4.2.5.jar;D:\maven\repositories\org\apache\httpcomponents\httpcore\4.2.4\httpcore-4.2.4.jar;D:\maven\repositories\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;D:\maven\repositories\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;D:\maven\repositories\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;D:\maven\repositories\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;D:\maven\repositories\org\apache\hadoop\hadoop-common\2.5.1\hadoop-common-2.5.1.jar;D:\maven\repositories\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;D:\maven\repositories\commons-el\commons-el\1.0\commons-el-1.0.jar;D:\maven\repositories\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;D:\maven\repositories\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;D:\maven\repositories\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;D:\maven\repositories\com\jcraft\jsch\0.1.42\jsch-0.1.42.jar;D:\maven\repositories\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;D:\maven\repositories\org\tukaani\xz\1.0\xz-1.0.jar;D:\maven\repositories\com\github\stephenc\findbugs\findbugs-annotations\1.3.9-1\findbugs-annotations-1.3.9-1.jar;D:\maven\repositories\junit\junit\4.11\junit-4.11.jar;D:\maven\repositories\org\hamcrest\hamcrest-core\1.3\hamcrest-core-1.3.jar;D:\maven\repositories\org\apache\hbase\hbase-server\1.0.1\hbase-server-1.0.1.jar;D:\maven\repositories\org\apache\hbase\hbase-prefix-tree\1.0.1\hbase-prefix-tree-1.0.1.jar;D:\maven\repositories\org\apache\hbase\hbase-common\1.0.1\hbase-common-1.0.1-tests.jar;D:\maven\repositories\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;D:\maven\repositories\org\apache\hbase\hbase-hadoop-compat\1.0.1\hbase-hadoop-compat-1.0.1.jar;D:\maven\repositories\org\apache\hbase\hbase-hadoop2-compat\1.0.1\hbase-hadoop2-compat-1.0.1.jar;D:\maven\repositories\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;D:\maven\repositories\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;D:\maven\repositories\org\apache\commons\commons-math\2.2\commons-math-2.2.jar;D:\maven\repositories\org\apache\zookeeper\zookeeper\3.4.6\zookeeper-3.4.6.jar;D:\maven\repositories\org\mortbay\jetty\jetty\6.1.26\jetty-6.1.26.jar;D:\maven\repositories\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;D:\maven\repositories\org\mortbay\jetty\jetty-sslengine\6.1.26\jetty-sslengine-6.1.26.jar;D:\maven\repositories\org\mortbay\jetty\jsp-2.1\6.1.14\jsp-2.1-6.1.14.jar;D:\maven\repositories\org\mortbay\jetty\jsp-api-2.1\6.1.14\jsp-api-2.1-6.1.14.jar;D:\maven\repositories\org\mortbay\jetty\servlet-api-2.5\6.1.14\servlet-api-2.5-6.1.14.jar;D:\maven\repositories\org\codehaus\jackson\jackson-core-asl\1.8.8\jackson-core-asl-1.8.8.jar;D:\maven\repositories\org\codehaus\jackson\jackson-jaxrs\1.8.8\jackson-jaxrs-1.8.8.jar;D:\maven\repositories\tomcat\jasper-compiler\5.5.23\jasper-compiler-5.5.23.jar;D:\maven\repositories\tomcat\jasper-runtime\5.5.23\jasper-runtime-5.5.23.jar;D:\maven\repositories\org\jamon\jamon-runtime\2.3.1\jamon-runtime-2.3.1.jar;D:\maven\repositories\com\lmax\disruptor\3.3.0\disruptor-3.3.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-hdfs\2.5.1\hadoop-hdfs-2.5.1.jar;D:\maven\repositories\commons-daemon\commons-daemon\1.0.13\commons-daemon-1.0.13.jar;D:\maven\repositories\org\apache\hadoop\hadoop-mapreduce-client-core\2.6.0\hadoop-mapreduce-client-core-2.6.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-yarn-common\2.6.0\hadoop-yarn-common-2.6.0.jar;D:\maven\repositories\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;D:\maven\repositories\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;D:\maven\repositories\javax\activation\activation\1.1\activation-1.1.jar;D:\maven\repositories\javax\servlet\servlet-api\2.5\servlet-api-2.5.jar;D:\maven\repositories\com\sun\jersey\jersey-client\1.9\jersey-client-1.9.jar;D:\maven\repositories\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;D:\maven\repositories\com\google\inject\guice\3.0\guice-3.0.jar;D:\maven\repositories\javax\inject\javax.inject\1\javax.inject-1.jar;D:\maven\repositories\aopalliance\aopalliance\1.0\aopalliance-1.0.jar;D:\maven\repositories\com\sun\jersey\jersey-json\1.9\jersey-json-1.9.jar;D:\maven\repositories\org\codehaus\jettison\jettison\1.1\jettison-1.1.jar;D:\maven\repositories\com\sun\xml\bind\jaxb-impl\2.2.3-1\jaxb-impl-2.2.3-1.jar;D:\maven\repositories\com\sun\jersey\contribs\jersey-guice\1.9\jersey-guice-1.9.jar;D:\maven\repositories\org\apache\avro\avro\1.7.4\avro-1.7.4.jar;D:\maven\repositories\org\apache\hadoop\hadoop-annotations\2.6.0\hadoop-annotations-2.6.0.jar;D:\maven\repositories\com\google\inject\extensions\guice-servlet\3.0\guice-servlet-3.0.jar;D:\maven\repositories\io\netty\netty\3.6.2.Final\netty-3.6.2.Final.jar;D:\IDEA\IntelliJ IDEA 2018.1\lib\idea_rt.jar

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.library.path=C:\Program Files\Java\jdk1.7.0_80\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files (x86)\Java\jdk1.8.0_144\bin;D:\hadoop\hadoop-2.7.1\bin;D:\hadoop\hadoop-2.7.1\sbin;C:\ProgramData\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\MySQL\MySQL Utilities 1.6\;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;D:\OpenVPN\bin;D:\Program Files (x86)\OpenVPN\bin;D:\maven\apache-maven-3.5.3\bin;C:\Program Files\nodejs\;D:\git\Git\cmd;C:\Program Files\TortoiseSVN\bin;C:\Program Files (x86)\OpenVPN\bin;D:\python-anncode;D:\maven\apache-maven-3.5.3\bin;D:\python-anncode\Library\mingw-w64\bin;D:\python-anncode\Library\usr\bin;D:\python-anncode\Library\bin;D:\python-anncode\Scripts;C:\Users\511921540\AppData\Local\Microsoft\WindowsApps;C:\Users\511921540\AppData\Roaming\npm;;.

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.io.tmpdir=C:\Users\511921~1\AppData\Local\Temp\

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.compiler=<NA>

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:os.name=Windows 8.1

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:os.arch=amd64

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:os.version=6.3

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:user.name=511921540

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:user.home=C:\Users\511921540

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:user.dir=C:\Users\511921540\Desktop\datamanger\log-streaming

2019-05-25 16:45:13
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@613a1d7a

2019-05-25 16:45:14
[INFO]-[Thread: ZkClient-EventThread-90-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Terminate ZkClient event thread.

2019-05-25 16:45:14
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:14
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:14
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:14
[ERROR]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:14
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773915400

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:15
[INFO]-[Thread: Thread-18]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:15
[INFO]-[Thread: Thread-18]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:15
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:15
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 0.0 (TID 0)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:15
[WARN]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:15
[ERROR]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 0.0 failed 1 times; aborting job

2019-05-25 16:45:15
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 0.0, whose tasks have all completed, from pool 

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 0

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 0 (start at AccessLogJob.scala:578) failed in 10.683 s

2019-05-25 16:45:15
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:15
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:15
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 1 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 1 (start at AccessLogJob.scala:578)

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 1 (Receiver 0 ParallelCollectionRDD[1] at makeRDD at ReceiverTracker.scala:588), which has no missing parents

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_1 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:15
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_1_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 1 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 1 (Receiver 0 ParallelCollectionRDD[1] at makeRDD at ReceiverTracker.scala:588)

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 1.0 with 1 tasks

2019-05-25 16:45:15
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 1.0 (TID 1)

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773915600

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:15
[INFO]-[Thread: Thread-21]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:15
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773915537-a2e7492b], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@2e763664

2019-05-25 16:45:15
[INFO]-[Thread: ZkClient-EventThread-94-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:15
[ERROR]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773915800

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:15
[INFO]-[Thread: Thread-21]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:15
[INFO]-[Thread: Thread-21]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:15
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:15
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 1.0 (TID 1)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:15
[WARN]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 1.0 (TID 1, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:15
[ERROR]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 1.0 failed 1 times; aborting job

2019-05-25 16:45:15
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 1.0, whose tasks have all completed, from pool 

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 1

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 1 (start at AccessLogJob.scala:578) failed in 0.312 s

2019-05-25 16:45:15
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:15
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:15
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 2 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 2 (start at AccessLogJob.scala:578)

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 2 (Receiver 0 ParallelCollectionRDD[2] at makeRDD at ReceiverTracker.scala:588), which has no missing parents

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_2 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:15
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_2_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 2 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 2 (Receiver 0 ParallelCollectionRDD[2] at makeRDD at ReceiverTracker.scala:588)

2019-05-25 16:45:15
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 2.0 with 1 tasks

2019-05-25 16:45:15
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 2.0 (TID 2)

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773916000

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:15
[INFO]-[Thread: Thread-23]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:15
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773915888-d97da2c5], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@3d7075fb

2019-05-25 16:45:15
[INFO]-[Thread: ZkClient-EventThread-98-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:15
[ERROR]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:15
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773916200

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:16
[INFO]-[Thread: Thread-23]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:16
[INFO]-[Thread: Thread-23]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:16
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:16
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 2.0 (TID 2)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:16
[WARN]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 2.0 (TID 2, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:16
[ERROR]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 2.0 failed 1 times; aborting job

2019-05-25 16:45:16
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 2.0, whose tasks have all completed, from pool 

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 2

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 2 (start at AccessLogJob.scala:578) failed in 0.352 s

2019-05-25 16:45:16
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:16
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:16
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 3 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 3 (start at AccessLogJob.scala:578)

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 3 (Receiver 0 ParallelCollectionRDD[3] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_3 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:16
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_3_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 3 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 3 (Receiver 0 ParallelCollectionRDD[3] at start at AccessLogJob.scala:578)

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 3.0 with 1 tasks

2019-05-25 16:45:16
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 3.0 (TID 3)

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773916400

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:16
[INFO]-[Thread: Thread-25]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:16
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773916284-75aa5534], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@ac621ab

2019-05-25 16:45:16
[INFO]-[Thread: ZkClient-EventThread-102-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:16
[ERROR]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773916600

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:16
[INFO]-[Thread: Thread-25]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:16
[INFO]-[Thread: Thread-25]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:16
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:16
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 3.0 (TID 3)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:16
[WARN]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 3.0 (TID 3, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:16
[ERROR]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 3.0 failed 1 times; aborting job

2019-05-25 16:45:16
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 3.0, whose tasks have all completed, from pool 

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 3

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 3 (start at AccessLogJob.scala:578) failed in 0.351 s

2019-05-25 16:45:16
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:16
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:16
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 4 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 4 (start at AccessLogJob.scala:578)

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 4 (Receiver 0 ParallelCollectionRDD[4] at makeRDD at ReceiverTracker.scala:588), which has no missing parents

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_4 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_4_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:16
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_4_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 4 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 4 (Receiver 0 ParallelCollectionRDD[4] at makeRDD at ReceiverTracker.scala:588)

2019-05-25 16:45:16
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 4.0 with 1 tasks

2019-05-25 16:45:16
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 4.0 (TID 4)

2019-05-25 16:45:16
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_3_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:16
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 4

2019-05-25 16:45:16
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_2_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:16
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 3

2019-05-25 16:45:16
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_1_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:16
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 2

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773916800

2019-05-25 16:45:16
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_0_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:16
[INFO]-[Thread: Thread-27]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:16
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 1

2019-05-25 16:45:16
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773916703-554c850e], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@727bbb51

2019-05-25 16:45:16
[INFO]-[Thread: ZkClient-EventThread-111-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:16
[ERROR]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:16
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773917000

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:17
[INFO]-[Thread: Thread-27]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:17
[INFO]-[Thread: Thread-27]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:17
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 4.0 (TID 4)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:17
[WARN]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 4.0 (TID 4, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:17
[ERROR]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 4.0 failed 1 times; aborting job

2019-05-25 16:45:17
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 4.0, whose tasks have all completed, from pool 

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 4

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 4 (start at AccessLogJob.scala:578) failed in 0.344 s

2019-05-25 16:45:17
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:17
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:17
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 5 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 5 (start at AccessLogJob.scala:578)

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 5 (Receiver 0 ParallelCollectionRDD[5] at makeRDD at ReceiverTracker.scala:588), which has no missing parents

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_5 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:17
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_5_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 5 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 5 (Receiver 0 ParallelCollectionRDD[5] at makeRDD at ReceiverTracker.scala:588)

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 5.0 with 1 tasks

2019-05-25 16:45:17
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 5.0 (TID 5)

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773917200

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:17
[INFO]-[Thread: Thread-29]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:17
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773917060-f322b828], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@1548cbea

2019-05-25 16:45:17
[INFO]-[Thread: ZkClient-EventThread-114-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:17
[ERROR]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773917400

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:17
[INFO]-[Thread: Thread-29]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:17
[INFO]-[Thread: Thread-29]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:17
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 5.0 (TID 5)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:17
[WARN]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 5.0 (TID 5, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:17
[ERROR]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 5.0 failed 1 times; aborting job

2019-05-25 16:45:17
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 5.0, whose tasks have all completed, from pool 

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 5

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 5 (start at AccessLogJob.scala:578) failed in 0.373 s

2019-05-25 16:45:17
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:17
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:17
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 6 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 6 (start at AccessLogJob.scala:578)

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 6 (Receiver 0 ParallelCollectionRDD[6] at makeRDD at ReceiverTracker.scala:588), which has no missing parents

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_6 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:17
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_6_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 6 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 6 (Receiver 0 ParallelCollectionRDD[6] at makeRDD at ReceiverTracker.scala:588)

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 6.0 with 1 tasks

2019-05-25 16:45:17
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 6.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 6.0 (TID 6)

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773917600

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:17
[INFO]-[Thread: Thread-31]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:17
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773917459-2ccbe50b], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@272bb2d3

2019-05-25 16:45:17
[INFO]-[Thread: ZkClient-EventThread-117-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:17
[ERROR]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773917800

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:17
[INFO]-[Thread: Thread-31]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:17
[INFO]-[Thread: Thread-31]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:17
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 6.0 (TID 6)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:17
[WARN]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 6.0 (TID 6, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:17
[ERROR]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 6.0 failed 1 times; aborting job

2019-05-25 16:45:17
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 6.0, whose tasks have all completed, from pool 

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 6

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 6 (start at AccessLogJob.scala:578) failed in 0.381 s

2019-05-25 16:45:17
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:17
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:17
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 7 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 7 (start at AccessLogJob.scala:578)

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 7 (Receiver 0 ParallelCollectionRDD[7] at makeRDD at ReceiverTracker.scala:588), which has no missing parents

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_7 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_7_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:17
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_7_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 7 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 7 (Receiver 0 ParallelCollectionRDD[7] at makeRDD at ReceiverTracker.scala:588)

2019-05-25 16:45:17
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 7.0 with 1 tasks

2019-05-25 16:45:17
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 7.0 (TID 7, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 7.0 (TID 7)

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773918000

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:17
[INFO]-[Thread: Thread-33]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:17
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773917866-70932e04], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@41007906

2019-05-25 16:45:17
[INFO]-[Thread: ZkClient-EventThread-120-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:17
[ERROR]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:17
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773918200

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:18
[INFO]-[Thread: Thread-33]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:18
[INFO]-[Thread: Thread-33]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:18
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:18
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 7.0 (TID 7)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:18
[WARN]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 7.0 (TID 7, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:18
[ERROR]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 7.0 failed 1 times; aborting job

2019-05-25 16:45:18
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 7.0, whose tasks have all completed, from pool 

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 7

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 7 (start at AccessLogJob.scala:578) failed in 0.372 s

2019-05-25 16:45:18
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:18
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:18
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 8 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 8 (start at AccessLogJob.scala:578)

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 8 (Receiver 0 ParallelCollectionRDD[8] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_8 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:18
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_8_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 8 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 8 (Receiver 0 ParallelCollectionRDD[8] at start at AccessLogJob.scala:578)

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 8.0 with 1 tasks

2019-05-25 16:45:18
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 8.0 (TID 8, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 8.0 (TID 8)

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773918400

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:18
[INFO]-[Thread: Thread-35]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:18
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773918269-b4e8d463], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@3410efd7

2019-05-25 16:45:18
[INFO]-[Thread: ZkClient-EventThread-123-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:18
[ERROR]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773918600

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:18
[INFO]-[Thread: Thread-35]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:18
[INFO]-[Thread: Thread-35]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:18
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:18
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 8.0 (TID 8)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:18
[WARN]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 8.0 (TID 8, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:18
[ERROR]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 8.0 failed 1 times; aborting job

2019-05-25 16:45:18
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 8.0, whose tasks have all completed, from pool 

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 8

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 8 (start at AccessLogJob.scala:578) failed in 0.364 s

2019-05-25 16:45:18
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:18
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:18
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 9 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 9 (start at AccessLogJob.scala:578)

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 9 (Receiver 0 ParallelCollectionRDD[9] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_9 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_9_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:18
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_9_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 9 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 9 (Receiver 0 ParallelCollectionRDD[9] at start at AccessLogJob.scala:578)

2019-05-25 16:45:18
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 9.0 with 1 tasks

2019-05-25 16:45:18
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 9.0 (TID 9, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 9.0 (TID 9)

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773918800

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:18
[INFO]-[Thread: Thread-37]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:18
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773918650-a38e181e], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@4fa1c637

2019-05-25 16:45:18
[INFO]-[Thread: ZkClient-EventThread-126-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:18
[ERROR]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:18
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773919000

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:19
[INFO]-[Thread: Thread-37]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:19
[INFO]-[Thread: Thread-37]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:19
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 9.0 (TID 9)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:19
[WARN]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 9.0 (TID 9, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:19
[ERROR]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 9.0 failed 1 times; aborting job

2019-05-25 16:45:19
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 9.0, whose tasks have all completed, from pool 

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 9

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 9 (start at AccessLogJob.scala:578) failed in 0.378 s

2019-05-25 16:45:19
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:19
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 10 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 10 (start at AccessLogJob.scala:578)

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 10 (Receiver 0 ParallelCollectionRDD[10] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_10 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_10_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_10_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 10 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 10 (Receiver 0 ParallelCollectionRDD[10] at start at AccessLogJob.scala:578)

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_9_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 10.0 with 1 tasks

2019-05-25 16:45:19
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 10

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 10.0 (TID 10)

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_8_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 9

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_7_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 8

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_6_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 7

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_5_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773919200

2019-05-25 16:45:19
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 6

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:19
[INFO]-[Thread: Thread-39]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_4_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 5

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773919059-30c26af9], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@68e95c04

2019-05-25 16:45:19
[INFO]-[Thread: ZkClient-EventThread-129-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:19
[ERROR]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773919400

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:19
[INFO]-[Thread: Thread-39]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:19
[INFO]-[Thread: Thread-39]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:19
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 10.0 (TID 10)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:19
[WARN]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 10.0 (TID 10, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:19
[ERROR]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 10.0 failed 1 times; aborting job

2019-05-25 16:45:19
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 10.0, whose tasks have all completed, from pool 

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 10

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 10 (start at AccessLogJob.scala:578) failed in 0.364 s

2019-05-25 16:45:19
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:19
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 11 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 11 (start at AccessLogJob.scala:578)

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 11 (Receiver 0 ParallelCollectionRDD[11] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_11 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_11_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_11_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 11 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 11 (Receiver 0 ParallelCollectionRDD[11] at start at AccessLogJob.scala:578)

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 11.0 with 1 tasks

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 11.0 (TID 11, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 11.0 (TID 11)

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773919600

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:19
[INFO]-[Thread: Thread-41]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773919449-9b92e55a], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@1f614b16

2019-05-25 16:45:19
[INFO]-[Thread: ZkClient-EventThread-132-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:19
[ERROR]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773919800

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:19
[INFO]-[Thread: Thread-41]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:19
[INFO]-[Thread: Thread-41]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:19
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 11.0 (TID 11)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:19
[WARN]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 11.0 (TID 11, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:19
[ERROR]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 11.0 failed 1 times; aborting job

2019-05-25 16:45:19
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 11.0, whose tasks have all completed, from pool 

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 11

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 11 (start at AccessLogJob.scala:578) failed in 0.380 s

2019-05-25 16:45:19
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:19
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 12 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 12 (start at AccessLogJob.scala:578)

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 12 (Receiver 0 ParallelCollectionRDD[12] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_12 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_12_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_12_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 12 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 12 (Receiver 0 ParallelCollectionRDD[12] at start at AccessLogJob.scala:578)

2019-05-25 16:45:19
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 12.0 with 1 tasks

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 12.0 (TID 12, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 12.0 (TID 12)

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773920000

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:19
[INFO]-[Thread: Thread-43]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:19
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773919851-384eda9f], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@661cb3b8

2019-05-25 16:45:19
[INFO]-[Thread: ZkClient-EventThread-135-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:19
[ERROR]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:19
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773920200

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:20
[INFO]-[Thread: Thread-43]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:20
[INFO]-[Thread: Thread-43]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:20
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:20
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 12.0 (TID 12)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:20
[WARN]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 12.0 (TID 12, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:20
[ERROR]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 12.0 failed 1 times; aborting job

2019-05-25 16:45:20
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 12.0, whose tasks have all completed, from pool 

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 12

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 12 (start at AccessLogJob.scala:578) failed in 0.368 s

2019-05-25 16:45:20
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:20
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:20
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 13 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 13 (start at AccessLogJob.scala:578)

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 13 (Receiver 0 ParallelCollectionRDD[13] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_13 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_13_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:20
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_13_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 13 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 13 (Receiver 0 ParallelCollectionRDD[13] at start at AccessLogJob.scala:578)

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 13.0 with 1 tasks

2019-05-25 16:45:20
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 13.0 (TID 13, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 13.0 (TID 13)

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773920400

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:20
[INFO]-[Thread: Thread-45]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:20
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773920241-171f8e5e], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@7767e16e

2019-05-25 16:45:20
[INFO]-[Thread: ZkClient-EventThread-138-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:20
[ERROR]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773920600

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:20
[INFO]-[Thread: Thread-45]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:20
[INFO]-[Thread: Thread-45]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:20
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:20
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 13.0 (TID 13)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:20
[WARN]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 13.0 (TID 13, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:20
[ERROR]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 13.0 failed 1 times; aborting job

2019-05-25 16:45:20
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 13.0, whose tasks have all completed, from pool 

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 13

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 13 (start at AccessLogJob.scala:578) failed in 0.381 s

2019-05-25 16:45:20
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:20
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:20
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 14 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 14 (start at AccessLogJob.scala:578)

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 14 (Receiver 0 ParallelCollectionRDD[14] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_14 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_14_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:20
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_14_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 14 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 14 (Receiver 0 ParallelCollectionRDD[14] at start at AccessLogJob.scala:578)

2019-05-25 16:45:20
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 14.0 with 1 tasks

2019-05-25 16:45:20
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 14.0 (TID 14, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 14.0 (TID 14)

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773920800

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:20
[INFO]-[Thread: Thread-47]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:20
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773920650-8a2639c3], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@1592059

2019-05-25 16:45:20
[INFO]-[Thread: ZkClient-EventThread-141-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:20
[ERROR]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:20
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773921000

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:21
[INFO]-[Thread: Thread-47]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:21
[INFO]-[Thread: Thread-47]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:21
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 14.0 (TID 14)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:21
[WARN]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 14.0 (TID 14, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:21
[ERROR]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 14.0 failed 1 times; aborting job

2019-05-25 16:45:21
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 14.0, whose tasks have all completed, from pool 

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 14

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 14 (start at AccessLogJob.scala:578) failed in 0.384 s

2019-05-25 16:45:21
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 14, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:21
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 15 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 15 (start at AccessLogJob.scala:578)

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 15 (Receiver 0 ParallelCollectionRDD[15] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_15 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_15_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 15 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 15 (Receiver 0 ParallelCollectionRDD[15] at start at AccessLogJob.scala:578)

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 15.0 with 1 tasks

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 15.0 (TID 15, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 15.0 (TID 15)

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773921200

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:21
[INFO]-[Thread: Thread-49]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773921052-bd1baf9c], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@6fd90d63

2019-05-25 16:45:21
[INFO]-[Thread: ZkClient-EventThread-144-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:21
[ERROR]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773921400

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:21
[INFO]-[Thread: Thread-49]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:21
[INFO]-[Thread: Thread-49]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:21
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 15.0 (TID 15)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:21
[WARN]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 15.0 (TID 15, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:21
[ERROR]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 15.0 failed 1 times; aborting job

2019-05-25 16:45:21
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 15.0, whose tasks have all completed, from pool 

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 15

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 15 (start at AccessLogJob.scala:578) failed in 0.373 s

2019-05-25 16:45:21
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 15, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:21
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 16 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 16 (start at AccessLogJob.scala:578)

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 16 (Receiver 0 ParallelCollectionRDD[16] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_16 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_16_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_16_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 16 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 16 (Receiver 0 ParallelCollectionRDD[16] at start at AccessLogJob.scala:578)

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 16.0 with 1 tasks

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_15_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 16.0 (TID 16, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:21
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 16

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 16.0 (TID 16)

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_14_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 15

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_13_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 14

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_12_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 13

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_11_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 12

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_10_piece0 on localhost:60851 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773921600

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:21
[INFO]-[Thread: Thread-51]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:21
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 11

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773921459-6bd6c906], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@cc2c59d

2019-05-25 16:45:21
[INFO]-[Thread: ZkClient-EventThread-147-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:21
[ERROR]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558773921800

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:45:21
[INFO]-[Thread: Thread-51]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:45:21
[INFO]-[Thread: Thread-51]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:45:21
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 16.0 (TID 16)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:21
[WARN]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 16.0 (TID 16, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:21
[ERROR]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 16.0 failed 1 times; aborting job

2019-05-25 16:45:21
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 16.0, whose tasks have all completed, from pool 

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 16

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 16 (start at AccessLogJob.scala:578) failed in 0.372 s

2019-05-25 16:45:21
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 16, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:45:21
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 17 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 17 (start at AccessLogJob.scala:578)

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 17 (Receiver 0 ParallelCollectionRDD[17] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_17 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_17_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_17_piece0 in memory on localhost:60851 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 17 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 17 (Receiver 0 ParallelCollectionRDD[17] at start at AccessLogJob.scala:578)

2019-05-25 16:45:21
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 17.0 with 1 tasks

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 17.0 (TID 17, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 17.0 (TID 17)

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558773922000

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:45:21
[INFO]-[Thread: Thread-53]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:45:21
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60825

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558773921845-cbf556f3], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@5a5a464f

2019-05-25 16:45:21
[INFO]-[Thread: ZkClient-EventThread-150-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:45:21
[ERROR]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:45:21
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:13
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Running Spark version 1.6.3

2019-05-25 16:48:23
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Changing view acls to: 511921540

2019-05-25 16:48:23
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Changing modify acls to: 511921540

2019-05-25 16:48:23
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(511921540); users with modify permissions: Set(511921540)

2019-05-25 16:48:24
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Successfully started service 'sparkDriver' on port 60928.

2019-05-25 16:48:24
[INFO]-[Thread: sparkDriverActorSystem-akka.actor.default-dispatcher-2]-[akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse()]: Slf4jLogger started

2019-05-25 16:48:24
[INFO]-[Thread: sparkDriverActorSystem-akka.actor.default-dispatcher-2]-[akka.event.slf4j.Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3.apply$mcV$sp()]: Starting remoting

2019-05-25 16:48:24
[INFO]-[Thread: sparkDriverActorSystem-akka.actor.default-dispatcher-2]-[akka.event.slf4j.Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3.apply$mcV$sp()]: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.12.0.198:60941]

2019-05-25 16:48:24
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Successfully started service 'sparkDriverActorSystem' on port 60941.

2019-05-25 16:48:24
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Registering MapOutputTracker

2019-05-25 16:48:24
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Registering BlockManagerMaster

2019-05-25 16:48:24
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Created local directory at C:\Users\511921540\AppData\Local\Temp\blockmgr-9c94e30b-b571-4d91-93e5-b4800145e9c1

2019-05-25 16:48:24
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: MemoryStore started with capacity 2.4 GB

2019-05-25 16:48:25
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Registering OutputCommitCoordinator

2019-05-25 16:48:25
[INFO]-[Thread: main]-[org.spark-project.jetty.server.Server.doStart()]: jetty-8.y.z-SNAPSHOT

2019-05-25 16:48:25
[INFO]-[Thread: main]-[org.spark-project.jetty.server.AbstractConnector.doStart()]: Started SelectChannelConnector@0.0.0.0:4040

2019-05-25 16:48:25
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Successfully started service 'SparkUI' on port 4040.

2019-05-25 16:48:25
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Started SparkUI at http://10.12.0.198:4040

2019-05-25 16:48:25
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Starting executor ID driver on host localhost

2019-05-25 16:48:25
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60954.

2019-05-25 16:48:25
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Server created on 60954

2019-05-25 16:48:25
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Trying to register BlockManager

2019-05-25 16:48:25
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Registering block manager localhost:60954 with 2.4 GB RAM, BlockManagerId(driver, localhost, 60954)

2019-05-25 16:48:25
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: Registered BlockManager

2019-05-25 16:48:26
[WARN]-[Thread: main]-[tachyon.util.network.NetworkAddressUtils.getLocalIpAddress()]: Your hostname, LAPTOP-NFE4JUQT resolves to a loopback/non-reachable address: fe80:0:0:0:b0e1:301a:a394:7741%20, but we couldn't find any external IP address!

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Starting 1 receivers

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: ReceiverTracker started

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: metadataCleanupDelay = -1

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: metadataCleanupDelay = -1

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: metadataCleanupDelay = -1

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Slide time = 30000 ms

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Storage level = StorageLevel(false, false, false, false, 1)

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Checkpoint interval = null

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Remember duration = 30000 ms

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Initialized and validated org.apache.spark.streaming.kafka.KafkaInputDStream@486f77f0

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Slide time = 30000 ms

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Storage level = StorageLevel(false, false, false, false, 1)

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Checkpoint interval = null

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Remember duration = 30000 ms

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@33ff417c

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Slide time = 30000 ms

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Storage level = StorageLevel(false, false, false, false, 1)

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Checkpoint interval = null

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Remember duration = 30000 ms

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@607f3b3c

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Started timer for JobGenerator at time 1558774110000

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Started JobGenerator at 1558774110000 ms

2019-05-25 16:48:28
[INFO]-[Thread: streaming-start]-[org.apache.spark.Logging$class.logInfo()]: Started JobScheduler

2019-05-25 16:48:28
[INFO]-[Thread: main]-[org.apache.spark.Logging$class.logInfo()]: StreamingContext started

2019-05-25 16:48:28
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:28
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 0 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:28
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 0 (start at AccessLogJob.scala:578)

2019-05-25 16:48:28
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:28
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:28
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:588), which has no missing parents

2019-05-25 16:48:28
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_0 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:28
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:28
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_0_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:28
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 0 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:28
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:588)

2019-05-25 16:48:28
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 0.0 with 1 tasks

2019-05-25 16:48:28
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:28
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 0.0 (TID 0)

2019-05-25 16:48:28
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774108800

2019-05-25 16:48:28
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:28
[INFO]-[Thread: Thread-18]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:28
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:28
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:28
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:28
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:28
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:28
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:28
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:28
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:28
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774108798-ad06bcc8], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:28
[INFO]-[Thread: ZkClient-EventThread-90-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:30
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: Added jobs for time 1558774110000 ms

2019-05-25 16:48:30
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: Checkpointing graph for time 1558774110000 ms

2019-05-25 16:48:30
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: Updating checkpoint data for time 1558774110000 ms

2019-05-25 16:48:30
[INFO]-[Thread: JobScheduler]-[org.apache.spark.Logging$class.logInfo()]: Starting job streaming job 1558774110000 ms.0 from job set of time 1558774110000 ms

2019-05-25 16:48:30
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: Updated checkpoint data for time 1558774110000 ms

2019-05-25 16:48:30
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: Submitted checkpoint of time 1558774110000 ms writer queue

2019-05-25 16:48:30
[INFO]-[Thread: pool-24-thread-1]-[org.apache.spark.Logging$class.logInfo()]: Saving checkpoint for time 1558774110000 ms to file 'file:/C:/Users/511921540/Desktop/datamanger/log-streaming/checkpoint/checkpoint-1558774110000'

2019-05-25 16:48:30
[INFO]-[Thread: streaming-job-executor-0]-[org.apache.spark.Logging$class.logInfo()]: Starting job: foreachPartition at AccessLogJob.scala:77

2019-05-25 16:48:30
[INFO]-[Thread: streaming-job-executor-0]-[org.apache.spark.Logging$class.logInfo()]: Job 1 finished: foreachPartition at AccessLogJob.scala:77, took 0.000859 s

2019-05-25 16:48:30
[INFO]-[Thread: pool-24-thread-1]-[org.apache.spark.Logging$class.logInfo()]: Checkpoint for time 1558774110000 ms saved to file 'file:/C:/Users/511921540/Desktop/datamanger/log-streaming/checkpoint/checkpoint-1558774110000', took 3209 bytes and 91 ms

2019-05-25 16:48:32
[INFO]-[Thread: streaming-job-executor-0]-[org.apache.spark.Logging$class.logInfo()]: Starting job: collect at AccessLogJob.scala:195

2019-05-25 16:48:32
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Registering RDD 10 (collect at AccessLogJob.scala:195)

2019-05-25 16:48:32
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 2 (collect at AccessLogJob.scala:195) with 200 output partitions

2019-05-25 16:48:32
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 2 (collect at AccessLogJob.scala:195)

2019-05-25 16:48:32
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List(ShuffleMapStage 1)

2019-05-25 16:48:32
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:32
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 2 (MapPartitionsRDD[15] at collect at AccessLogJob.scala:195), which has no missing parents

2019-05-25 16:48:32
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_1 stored as values in memory (estimated size 18.7 KB, free 2.4 GB)

2019-05-25 16:48:32
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KB, free 2.4 GB)

2019-05-25 16:48:32
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_1_piece0 in memory on localhost:60954 (size: 8.5 KB, free: 2.4 GB)

2019-05-25 16:48:32
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 1 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:32
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 200 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at collect at AccessLogJob.scala:195)

2019-05-25 16:48:32
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 2.0 with 200 tasks

2019-05-25 16:48:32
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 2.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:32
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 2.0 (TID 1)

2019-05-25 16:48:32
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:32
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 4 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Code generated in 202.9821 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Code generated in 20.3336 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Code generated in 20.7 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 0.0 in stage 2.0 (TID 1). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 1.0 in stage 2.0 (TID 2, localhost, partition 1,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 1.0 in stage 2.0 (TID 2)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 0.0 in stage 2.0 (TID 1) in 390 ms on localhost (1/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 1.0 in stage 2.0 (TID 2). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 2.0 in stage 2.0 (TID 3, localhost, partition 2,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 2.0 in stage 2.0 (TID 3)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 1.0 in stage 2.0 (TID 2) in 26 ms on localhost (2/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 2.0 in stage 2.0 (TID 3). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 3.0 in stage 2.0 (TID 4, localhost, partition 3,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 3.0 in stage 2.0 (TID 4)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 2.0 in stage 2.0 (TID 3) in 21 ms on localhost (3/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 3.0 in stage 2.0 (TID 4). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 4.0 in stage 2.0 (TID 5, localhost, partition 4,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 4.0 in stage 2.0 (TID 5)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 3.0 in stage 2.0 (TID 4) in 18 ms on localhost (4/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 4.0 in stage 2.0 (TID 5). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 5.0 in stage 2.0 (TID 6, localhost, partition 5,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 5.0 in stage 2.0 (TID 6)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 4.0 in stage 2.0 (TID 5) in 17 ms on localhost (5/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 5.0 in stage 2.0 (TID 6). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 6.0 in stage 2.0 (TID 7, localhost, partition 6,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 6.0 in stage 2.0 (TID 7)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 5.0 in stage 2.0 (TID 6) in 15 ms on localhost (6/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 6.0 in stage 2.0 (TID 7). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 7.0 in stage 2.0 (TID 8, localhost, partition 7,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 7.0 in stage 2.0 (TID 8)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 6.0 in stage 2.0 (TID 7) in 18 ms on localhost (7/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 7.0 in stage 2.0 (TID 8). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 8.0 in stage 2.0 (TID 9, localhost, partition 8,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 8.0 in stage 2.0 (TID 9)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 7.0 in stage 2.0 (TID 8) in 13 ms on localhost (8/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 8.0 in stage 2.0 (TID 9). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 9.0 in stage 2.0 (TID 10, localhost, partition 9,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 9.0 in stage 2.0 (TID 10)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 8.0 in stage 2.0 (TID 9) in 13 ms on localhost (9/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 9.0 in stage 2.0 (TID 10). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 10.0 in stage 2.0 (TID 11, localhost, partition 10,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 10.0 in stage 2.0 (TID 11)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 9.0 in stage 2.0 (TID 10) in 12 ms on localhost (10/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 10.0 in stage 2.0 (TID 11). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 11.0 in stage 2.0 (TID 12, localhost, partition 11,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 11.0 in stage 2.0 (TID 12)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 10.0 in stage 2.0 (TID 11) in 14 ms on localhost (11/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 11.0 in stage 2.0 (TID 12). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 12.0 in stage 2.0 (TID 13, localhost, partition 12,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 12.0 in stage 2.0 (TID 13)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 11.0 in stage 2.0 (TID 12) in 12 ms on localhost (12/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 12.0 in stage 2.0 (TID 13). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 13.0 in stage 2.0 (TID 14, localhost, partition 13,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 13.0 in stage 2.0 (TID 14)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 12.0 in stage 2.0 (TID 13) in 10 ms on localhost (13/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 13.0 in stage 2.0 (TID 14). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 14.0 in stage 2.0 (TID 15, localhost, partition 14,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 14.0 in stage 2.0 (TID 15)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 13.0 in stage 2.0 (TID 14) in 12 ms on localhost (14/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 14.0 in stage 2.0 (TID 15). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 15.0 in stage 2.0 (TID 16, localhost, partition 15,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 15.0 in stage 2.0 (TID 16)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 14.0 in stage 2.0 (TID 15) in 11 ms on localhost (15/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 15.0 in stage 2.0 (TID 16). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 16.0 in stage 2.0 (TID 17, localhost, partition 16,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 16.0 in stage 2.0 (TID 17)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 15.0 in stage 2.0 (TID 16) in 11 ms on localhost (16/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 16.0 in stage 2.0 (TID 17). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 17.0 in stage 2.0 (TID 18, localhost, partition 17,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 17.0 in stage 2.0 (TID 18)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 16.0 in stage 2.0 (TID 17) in 10 ms on localhost (17/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 17.0 in stage 2.0 (TID 18). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 18.0 in stage 2.0 (TID 19, localhost, partition 18,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 18.0 in stage 2.0 (TID 19)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 17.0 in stage 2.0 (TID 18) in 10 ms on localhost (18/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 18.0 in stage 2.0 (TID 19). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 19.0 in stage 2.0 (TID 20, localhost, partition 19,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 19.0 in stage 2.0 (TID 20)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 18.0 in stage 2.0 (TID 19) in 8 ms on localhost (19/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 19.0 in stage 2.0 (TID 20). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 20.0 in stage 2.0 (TID 21, localhost, partition 20,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 20.0 in stage 2.0 (TID 21)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 19.0 in stage 2.0 (TID 20) in 10 ms on localhost (20/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 20.0 in stage 2.0 (TID 21). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 21.0 in stage 2.0 (TID 22, localhost, partition 21,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 21.0 in stage 2.0 (TID 22)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 20.0 in stage 2.0 (TID 21) in 9 ms on localhost (21/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 21.0 in stage 2.0 (TID 22). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 22.0 in stage 2.0 (TID 23, localhost, partition 22,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 22.0 in stage 2.0 (TID 23)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 21.0 in stage 2.0 (TID 22) in 11 ms on localhost (22/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 22.0 in stage 2.0 (TID 23). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 23.0 in stage 2.0 (TID 24, localhost, partition 23,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 23.0 in stage 2.0 (TID 24)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 22.0 in stage 2.0 (TID 23) in 8 ms on localhost (23/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 23.0 in stage 2.0 (TID 24). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 24.0 in stage 2.0 (TID 25, localhost, partition 24,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 24.0 in stage 2.0 (TID 25)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 23.0 in stage 2.0 (TID 24) in 8 ms on localhost (24/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 24.0 in stage 2.0 (TID 25). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 25.0 in stage 2.0 (TID 26, localhost, partition 25,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 25.0 in stage 2.0 (TID 26)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 24.0 in stage 2.0 (TID 25) in 8 ms on localhost (25/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 25.0 in stage 2.0 (TID 26). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 26.0 in stage 2.0 (TID 27, localhost, partition 26,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 26.0 in stage 2.0 (TID 27)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 25.0 in stage 2.0 (TID 26) in 8 ms on localhost (26/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 26.0 in stage 2.0 (TID 27). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 27.0 in stage 2.0 (TID 28, localhost, partition 27,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 27.0 in stage 2.0 (TID 28)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 26.0 in stage 2.0 (TID 27) in 9 ms on localhost (27/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 27.0 in stage 2.0 (TID 28). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 28.0 in stage 2.0 (TID 29, localhost, partition 28,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 28.0 in stage 2.0 (TID 29)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 27.0 in stage 2.0 (TID 28) in 8 ms on localhost (28/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 28.0 in stage 2.0 (TID 29). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 29.0 in stage 2.0 (TID 30, localhost, partition 29,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 29.0 in stage 2.0 (TID 30)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 28.0 in stage 2.0 (TID 29) in 7 ms on localhost (29/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 29.0 in stage 2.0 (TID 30). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 30.0 in stage 2.0 (TID 31, localhost, partition 30,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 30.0 in stage 2.0 (TID 31)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 29.0 in stage 2.0 (TID 30) in 8 ms on localhost (30/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 30.0 in stage 2.0 (TID 31). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 31.0 in stage 2.0 (TID 32, localhost, partition 31,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 31.0 in stage 2.0 (TID 32)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 30.0 in stage 2.0 (TID 31) in 8 ms on localhost (31/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 31.0 in stage 2.0 (TID 32). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 32.0 in stage 2.0 (TID 33, localhost, partition 32,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 32.0 in stage 2.0 (TID 33)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 31.0 in stage 2.0 (TID 32) in 7 ms on localhost (32/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 32.0 in stage 2.0 (TID 33). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 33.0 in stage 2.0 (TID 34, localhost, partition 33,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 33.0 in stage 2.0 (TID 34)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 32.0 in stage 2.0 (TID 33) in 7 ms on localhost (33/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 33.0 in stage 2.0 (TID 34). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 34.0 in stage 2.0 (TID 35, localhost, partition 34,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 34.0 in stage 2.0 (TID 35)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 33.0 in stage 2.0 (TID 34) in 7 ms on localhost (34/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 34.0 in stage 2.0 (TID 35). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 35.0 in stage 2.0 (TID 36, localhost, partition 35,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 35.0 in stage 2.0 (TID 36)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 34.0 in stage 2.0 (TID 35) in 8 ms on localhost (35/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 35.0 in stage 2.0 (TID 36). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 36.0 in stage 2.0 (TID 37, localhost, partition 36,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 36.0 in stage 2.0 (TID 37)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 35.0 in stage 2.0 (TID 36) in 7 ms on localhost (36/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 36.0 in stage 2.0 (TID 37). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 37.0 in stage 2.0 (TID 38, localhost, partition 37,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 37.0 in stage 2.0 (TID 38)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 36.0 in stage 2.0 (TID 37) in 6 ms on localhost (37/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 37.0 in stage 2.0 (TID 38). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 38.0 in stage 2.0 (TID 39, localhost, partition 38,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 38.0 in stage 2.0 (TID 39)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 37.0 in stage 2.0 (TID 38) in 7 ms on localhost (38/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 38.0 in stage 2.0 (TID 39). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 39.0 in stage 2.0 (TID 40, localhost, partition 39,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 39.0 in stage 2.0 (TID 40)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 38.0 in stage 2.0 (TID 39) in 8 ms on localhost (39/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 39.0 in stage 2.0 (TID 40). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 40.0 in stage 2.0 (TID 41, localhost, partition 40,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 40.0 in stage 2.0 (TID 41)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 39.0 in stage 2.0 (TID 40) in 8 ms on localhost (40/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 40.0 in stage 2.0 (TID 41). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 41.0 in stage 2.0 (TID 42, localhost, partition 41,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 41.0 in stage 2.0 (TID 42)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 40.0 in stage 2.0 (TID 41) in 7 ms on localhost (41/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 41.0 in stage 2.0 (TID 42). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 42.0 in stage 2.0 (TID 43, localhost, partition 42,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 42.0 in stage 2.0 (TID 43)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 41.0 in stage 2.0 (TID 42) in 9 ms on localhost (42/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 42.0 in stage 2.0 (TID 43). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 43.0 in stage 2.0 (TID 44, localhost, partition 43,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 43.0 in stage 2.0 (TID 44)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 42.0 in stage 2.0 (TID 43) in 8 ms on localhost (43/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 43.0 in stage 2.0 (TID 44). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 44.0 in stage 2.0 (TID 45, localhost, partition 44,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 44.0 in stage 2.0 (TID 45)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 43.0 in stage 2.0 (TID 44) in 6 ms on localhost (44/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 44.0 in stage 2.0 (TID 45). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 45.0 in stage 2.0 (TID 46, localhost, partition 45,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 45.0 in stage 2.0 (TID 46)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 44.0 in stage 2.0 (TID 45) in 6 ms on localhost (45/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 45.0 in stage 2.0 (TID 46). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 46.0 in stage 2.0 (TID 47, localhost, partition 46,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 46.0 in stage 2.0 (TID 47)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 45.0 in stage 2.0 (TID 46) in 6 ms on localhost (46/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 46.0 in stage 2.0 (TID 47). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 47.0 in stage 2.0 (TID 48, localhost, partition 47,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 47.0 in stage 2.0 (TID 48)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 46.0 in stage 2.0 (TID 47) in 6 ms on localhost (47/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 47.0 in stage 2.0 (TID 48). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 48.0 in stage 2.0 (TID 49, localhost, partition 48,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 48.0 in stage 2.0 (TID 49)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 47.0 in stage 2.0 (TID 48) in 6 ms on localhost (48/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 48.0 in stage 2.0 (TID 49). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 49.0 in stage 2.0 (TID 50, localhost, partition 49,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 49.0 in stage 2.0 (TID 50)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 48.0 in stage 2.0 (TID 49) in 7 ms on localhost (49/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 49.0 in stage 2.0 (TID 50). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 50.0 in stage 2.0 (TID 51, localhost, partition 50,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 50.0 in stage 2.0 (TID 51)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 49.0 in stage 2.0 (TID 50) in 7 ms on localhost (50/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 50.0 in stage 2.0 (TID 51). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 51.0 in stage 2.0 (TID 52, localhost, partition 51,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 51.0 in stage 2.0 (TID 52)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 50.0 in stage 2.0 (TID 51) in 7 ms on localhost (51/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 51.0 in stage 2.0 (TID 52). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 52.0 in stage 2.0 (TID 53, localhost, partition 52,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 52.0 in stage 2.0 (TID 53)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 51.0 in stage 2.0 (TID 52) in 9 ms on localhost (52/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 52.0 in stage 2.0 (TID 53). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 53.0 in stage 2.0 (TID 54, localhost, partition 53,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 53.0 in stage 2.0 (TID 54)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 52.0 in stage 2.0 (TID 53) in 7 ms on localhost (53/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 53.0 in stage 2.0 (TID 54). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 54.0 in stage 2.0 (TID 55, localhost, partition 54,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 54.0 in stage 2.0 (TID 55)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 53.0 in stage 2.0 (TID 54) in 6 ms on localhost (54/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 54.0 in stage 2.0 (TID 55). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 55.0 in stage 2.0 (TID 56, localhost, partition 55,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 55.0 in stage 2.0 (TID 56)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 54.0 in stage 2.0 (TID 55) in 6 ms on localhost (55/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 55.0 in stage 2.0 (TID 56). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 56.0 in stage 2.0 (TID 57, localhost, partition 56,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 56.0 in stage 2.0 (TID 57)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 55.0 in stage 2.0 (TID 56) in 7 ms on localhost (56/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 56.0 in stage 2.0 (TID 57). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 57.0 in stage 2.0 (TID 58, localhost, partition 57,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 57.0 in stage 2.0 (TID 58)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 56.0 in stage 2.0 (TID 57) in 7 ms on localhost (57/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 57.0 in stage 2.0 (TID 58). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 58.0 in stage 2.0 (TID 59, localhost, partition 58,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 58.0 in stage 2.0 (TID 59)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 57.0 in stage 2.0 (TID 58) in 9 ms on localhost (58/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 58.0 in stage 2.0 (TID 59). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 59.0 in stage 2.0 (TID 60, localhost, partition 59,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 59.0 in stage 2.0 (TID 60)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 58.0 in stage 2.0 (TID 59) in 7 ms on localhost (59/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 59.0 in stage 2.0 (TID 60). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 60.0 in stage 2.0 (TID 61, localhost, partition 60,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 60.0 in stage 2.0 (TID 61)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 59.0 in stage 2.0 (TID 60) in 7 ms on localhost (60/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 60.0 in stage 2.0 (TID 61). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 61.0 in stage 2.0 (TID 62, localhost, partition 61,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 61.0 in stage 2.0 (TID 62)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 60.0 in stage 2.0 (TID 61) in 9 ms on localhost (61/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 61.0 in stage 2.0 (TID 62). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 62.0 in stage 2.0 (TID 63, localhost, partition 62,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 62.0 in stage 2.0 (TID 63)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 61.0 in stage 2.0 (TID 62) in 12 ms on localhost (62/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 62.0 in stage 2.0 (TID 63). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 63.0 in stage 2.0 (TID 64, localhost, partition 63,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 63.0 in stage 2.0 (TID 64)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 62.0 in stage 2.0 (TID 63) in 7 ms on localhost (63/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 63.0 in stage 2.0 (TID 64). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 64.0 in stage 2.0 (TID 65, localhost, partition 64,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 64.0 in stage 2.0 (TID 65)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 63.0 in stage 2.0 (TID 64) in 7 ms on localhost (64/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 64.0 in stage 2.0 (TID 65). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 65.0 in stage 2.0 (TID 66, localhost, partition 65,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 65.0 in stage 2.0 (TID 66)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 64.0 in stage 2.0 (TID 65) in 8 ms on localhost (65/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 65.0 in stage 2.0 (TID 66). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 66.0 in stage 2.0 (TID 67, localhost, partition 66,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 66.0 in stage 2.0 (TID 67)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 65.0 in stage 2.0 (TID 66) in 7 ms on localhost (66/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 66.0 in stage 2.0 (TID 67). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 67.0 in stage 2.0 (TID 68, localhost, partition 67,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 67.0 in stage 2.0 (TID 68)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 66.0 in stage 2.0 (TID 67) in 7 ms on localhost (67/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 67.0 in stage 2.0 (TID 68). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 68.0 in stage 2.0 (TID 69, localhost, partition 68,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 68.0 in stage 2.0 (TID 69)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 67.0 in stage 2.0 (TID 68) in 7 ms on localhost (68/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 68.0 in stage 2.0 (TID 69). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 69.0 in stage 2.0 (TID 70, localhost, partition 69,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 69.0 in stage 2.0 (TID 70)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 68.0 in stage 2.0 (TID 69) in 8 ms on localhost (69/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 69.0 in stage 2.0 (TID 70). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 70.0 in stage 2.0 (TID 71, localhost, partition 70,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 70.0 in stage 2.0 (TID 71)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 69.0 in stage 2.0 (TID 70) in 6 ms on localhost (70/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 70.0 in stage 2.0 (TID 71). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 71.0 in stage 2.0 (TID 72, localhost, partition 71,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 71.0 in stage 2.0 (TID 72)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 70.0 in stage 2.0 (TID 71) in 7 ms on localhost (71/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 71.0 in stage 2.0 (TID 72). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 72.0 in stage 2.0 (TID 73, localhost, partition 72,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 72.0 in stage 2.0 (TID 73)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 71.0 in stage 2.0 (TID 72) in 6 ms on localhost (72/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 72.0 in stage 2.0 (TID 73). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 73.0 in stage 2.0 (TID 74, localhost, partition 73,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 73.0 in stage 2.0 (TID 74)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 72.0 in stage 2.0 (TID 73) in 9 ms on localhost (73/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 73.0 in stage 2.0 (TID 74). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 74.0 in stage 2.0 (TID 75, localhost, partition 74,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 74.0 in stage 2.0 (TID 75)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 73.0 in stage 2.0 (TID 74) in 7 ms on localhost (74/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 74.0 in stage 2.0 (TID 75). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 75.0 in stage 2.0 (TID 76, localhost, partition 75,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 75.0 in stage 2.0 (TID 76)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 74.0 in stage 2.0 (TID 75) in 6 ms on localhost (75/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 75.0 in stage 2.0 (TID 76). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 76.0 in stage 2.0 (TID 77, localhost, partition 76,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 76.0 in stage 2.0 (TID 77)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 75.0 in stage 2.0 (TID 76) in 6 ms on localhost (76/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 76.0 in stage 2.0 (TID 77). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 77.0 in stage 2.0 (TID 78, localhost, partition 77,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 77.0 in stage 2.0 (TID 78)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 76.0 in stage 2.0 (TID 77) in 6 ms on localhost (77/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 77.0 in stage 2.0 (TID 78). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 78.0 in stage 2.0 (TID 79, localhost, partition 78,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 78.0 in stage 2.0 (TID 79)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 77.0 in stage 2.0 (TID 78) in 7 ms on localhost (78/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 78.0 in stage 2.0 (TID 79). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 79.0 in stage 2.0 (TID 80, localhost, partition 79,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 79.0 in stage 2.0 (TID 80)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 78.0 in stage 2.0 (TID 79) in 10 ms on localhost (79/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 79.0 in stage 2.0 (TID 80). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 80.0 in stage 2.0 (TID 81, localhost, partition 80,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 80.0 in stage 2.0 (TID 81)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 79.0 in stage 2.0 (TID 80) in 7 ms on localhost (80/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 80.0 in stage 2.0 (TID 81). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 81.0 in stage 2.0 (TID 82, localhost, partition 81,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 81.0 in stage 2.0 (TID 82)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 80.0 in stage 2.0 (TID 81) in 7 ms on localhost (81/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 81.0 in stage 2.0 (TID 82). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 82.0 in stage 2.0 (TID 83, localhost, partition 82,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 82.0 in stage 2.0 (TID 83)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 81.0 in stage 2.0 (TID 82) in 7 ms on localhost (82/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 82.0 in stage 2.0 (TID 83). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 83.0 in stage 2.0 (TID 84, localhost, partition 83,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 83.0 in stage 2.0 (TID 84)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 82.0 in stage 2.0 (TID 83) in 10 ms on localhost (83/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 83.0 in stage 2.0 (TID 84). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 84.0 in stage 2.0 (TID 85, localhost, partition 84,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 84.0 in stage 2.0 (TID 85)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 83.0 in stage 2.0 (TID 84) in 10 ms on localhost (84/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 84.0 in stage 2.0 (TID 85). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 85.0 in stage 2.0 (TID 86, localhost, partition 85,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 85.0 in stage 2.0 (TID 86)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 84.0 in stage 2.0 (TID 85) in 10 ms on localhost (85/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 85.0 in stage 2.0 (TID 86). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 86.0 in stage 2.0 (TID 87, localhost, partition 86,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 86.0 in stage 2.0 (TID 87)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 85.0 in stage 2.0 (TID 86) in 6 ms on localhost (86/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 86.0 in stage 2.0 (TID 87). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 87.0 in stage 2.0 (TID 88, localhost, partition 87,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 87.0 in stage 2.0 (TID 88)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 86.0 in stage 2.0 (TID 87) in 6 ms on localhost (87/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 87.0 in stage 2.0 (TID 88). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 88.0 in stage 2.0 (TID 89, localhost, partition 88,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 88.0 in stage 2.0 (TID 89)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 87.0 in stage 2.0 (TID 88) in 9 ms on localhost (88/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 88.0 in stage 2.0 (TID 89). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 89.0 in stage 2.0 (TID 90, localhost, partition 89,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 89.0 in stage 2.0 (TID 90)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 88.0 in stage 2.0 (TID 89) in 9 ms on localhost (89/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 89.0 in stage 2.0 (TID 90). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 90.0 in stage 2.0 (TID 91, localhost, partition 90,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 90.0 in stage 2.0 (TID 91)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 89.0 in stage 2.0 (TID 90) in 6 ms on localhost (90/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 90.0 in stage 2.0 (TID 91). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 91.0 in stage 2.0 (TID 92, localhost, partition 91,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 91.0 in stage 2.0 (TID 92)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 90.0 in stage 2.0 (TID 91) in 7 ms on localhost (91/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 91.0 in stage 2.0 (TID 92). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 92.0 in stage 2.0 (TID 93, localhost, partition 92,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 92.0 in stage 2.0 (TID 93)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 91.0 in stage 2.0 (TID 92) in 5 ms on localhost (92/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 92.0 in stage 2.0 (TID 93). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 93.0 in stage 2.0 (TID 94, localhost, partition 93,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 93.0 in stage 2.0 (TID 94)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 92.0 in stage 2.0 (TID 93) in 6 ms on localhost (93/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 93.0 in stage 2.0 (TID 94). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 94.0 in stage 2.0 (TID 95, localhost, partition 94,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 94.0 in stage 2.0 (TID 95)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 93.0 in stage 2.0 (TID 94) in 6 ms on localhost (94/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 94.0 in stage 2.0 (TID 95). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 95.0 in stage 2.0 (TID 96, localhost, partition 95,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 95.0 in stage 2.0 (TID 96)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 94.0 in stage 2.0 (TID 95) in 7 ms on localhost (95/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 95.0 in stage 2.0 (TID 96). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 96.0 in stage 2.0 (TID 97, localhost, partition 96,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 96.0 in stage 2.0 (TID 97)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 95.0 in stage 2.0 (TID 96) in 6 ms on localhost (96/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 96.0 in stage 2.0 (TID 97). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 97.0 in stage 2.0 (TID 98, localhost, partition 97,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 97.0 in stage 2.0 (TID 98)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 96.0 in stage 2.0 (TID 97) in 6 ms on localhost (97/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 97.0 in stage 2.0 (TID 98). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 98.0 in stage 2.0 (TID 99, localhost, partition 98,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 98.0 in stage 2.0 (TID 99)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 97.0 in stage 2.0 (TID 98) in 6 ms on localhost (98/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 98.0 in stage 2.0 (TID 99). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 99.0 in stage 2.0 (TID 100, localhost, partition 99,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 99.0 in stage 2.0 (TID 100)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 98.0 in stage 2.0 (TID 99) in 6 ms on localhost (99/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 99.0 in stage 2.0 (TID 100). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 100.0 in stage 2.0 (TID 101, localhost, partition 100,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 100.0 in stage 2.0 (TID 101)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 99.0 in stage 2.0 (TID 100) in 6 ms on localhost (100/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 100.0 in stage 2.0 (TID 101). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 101.0 in stage 2.0 (TID 102, localhost, partition 101,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 101.0 in stage 2.0 (TID 102)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 100.0 in stage 2.0 (TID 101) in 5 ms on localhost (101/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 101.0 in stage 2.0 (TID 102). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 102.0 in stage 2.0 (TID 103, localhost, partition 102,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 102.0 in stage 2.0 (TID 103)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 101.0 in stage 2.0 (TID 102) in 7 ms on localhost (102/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 102.0 in stage 2.0 (TID 103). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 103.0 in stage 2.0 (TID 104, localhost, partition 103,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 103.0 in stage 2.0 (TID 104)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 102.0 in stage 2.0 (TID 103) in 6 ms on localhost (103/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 103.0 in stage 2.0 (TID 104). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 104.0 in stage 2.0 (TID 105, localhost, partition 104,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 104.0 in stage 2.0 (TID 105)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 103.0 in stage 2.0 (TID 104) in 5 ms on localhost (104/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 104.0 in stage 2.0 (TID 105). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 105.0 in stage 2.0 (TID 106, localhost, partition 105,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 105.0 in stage 2.0 (TID 106)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 104.0 in stage 2.0 (TID 105) in 6 ms on localhost (105/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 105.0 in stage 2.0 (TID 106). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 106.0 in stage 2.0 (TID 107, localhost, partition 106,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 106.0 in stage 2.0 (TID 107)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 105.0 in stage 2.0 (TID 106) in 7 ms on localhost (106/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 106.0 in stage 2.0 (TID 107). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 107.0 in stage 2.0 (TID 108, localhost, partition 107,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 107.0 in stage 2.0 (TID 108)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 106.0 in stage 2.0 (TID 107) in 7 ms on localhost (107/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 107.0 in stage 2.0 (TID 108). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 108.0 in stage 2.0 (TID 109, localhost, partition 108,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 108.0 in stage 2.0 (TID 109)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 107.0 in stage 2.0 (TID 108) in 4 ms on localhost (108/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 108.0 in stage 2.0 (TID 109). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 109.0 in stage 2.0 (TID 110, localhost, partition 109,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 109.0 in stage 2.0 (TID 110)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 108.0 in stage 2.0 (TID 109) in 6 ms on localhost (109/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 109.0 in stage 2.0 (TID 110). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 110.0 in stage 2.0 (TID 111, localhost, partition 110,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 110.0 in stage 2.0 (TID 111)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 109.0 in stage 2.0 (TID 110) in 6 ms on localhost (110/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 110.0 in stage 2.0 (TID 111). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 111.0 in stage 2.0 (TID 112, localhost, partition 111,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 111.0 in stage 2.0 (TID 112)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 110.0 in stage 2.0 (TID 111) in 7 ms on localhost (111/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 111.0 in stage 2.0 (TID 112). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 112.0 in stage 2.0 (TID 113, localhost, partition 112,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 112.0 in stage 2.0 (TID 113)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 111.0 in stage 2.0 (TID 112) in 6 ms on localhost (112/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 112.0 in stage 2.0 (TID 113). 1652 bytes result sent to driver

2019-05-25 16:48:33
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 113.0 in stage 2.0 (TID 114, localhost, partition 113,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 113.0 in stage 2.0 (TID 114)

2019-05-25 16:48:33
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 112.0 in stage 2.0 (TID 113) in 7 ms on localhost (113/200)

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:33
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 113.0 in stage 2.0 (TID 114). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 114.0 in stage 2.0 (TID 115, localhost, partition 114,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 114.0 in stage 2.0 (TID 115)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 113.0 in stage 2.0 (TID 114) in 7 ms on localhost (114/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 114.0 in stage 2.0 (TID 115). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 115.0 in stage 2.0 (TID 116, localhost, partition 115,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 115.0 in stage 2.0 (TID 116)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 114.0 in stage 2.0 (TID 115) in 11 ms on localhost (115/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 115.0 in stage 2.0 (TID 116). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 116.0 in stage 2.0 (TID 117, localhost, partition 116,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 116.0 in stage 2.0 (TID 117)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 115.0 in stage 2.0 (TID 116) in 8 ms on localhost (116/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 116.0 in stage 2.0 (TID 117). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 117.0 in stage 2.0 (TID 118, localhost, partition 117,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 117.0 in stage 2.0 (TID 118)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 116.0 in stage 2.0 (TID 117) in 7 ms on localhost (117/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 117.0 in stage 2.0 (TID 118). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 118.0 in stage 2.0 (TID 119, localhost, partition 118,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 118.0 in stage 2.0 (TID 119)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 117.0 in stage 2.0 (TID 118) in 6 ms on localhost (118/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 118.0 in stage 2.0 (TID 119). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 119.0 in stage 2.0 (TID 120, localhost, partition 119,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 119.0 in stage 2.0 (TID 120)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 118.0 in stage 2.0 (TID 119) in 6 ms on localhost (119/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 119.0 in stage 2.0 (TID 120). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 120.0 in stage 2.0 (TID 121, localhost, partition 120,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 120.0 in stage 2.0 (TID 121)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 119.0 in stage 2.0 (TID 120) in 5 ms on localhost (120/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 120.0 in stage 2.0 (TID 121). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 121.0 in stage 2.0 (TID 122, localhost, partition 121,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 121.0 in stage 2.0 (TID 122)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 120.0 in stage 2.0 (TID 121) in 5 ms on localhost (121/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 121.0 in stage 2.0 (TID 122). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 122.0 in stage 2.0 (TID 123, localhost, partition 122,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 122.0 in stage 2.0 (TID 123)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 121.0 in stage 2.0 (TID 122) in 6 ms on localhost (122/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 122.0 in stage 2.0 (TID 123). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 123.0 in stage 2.0 (TID 124, localhost, partition 123,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 123.0 in stage 2.0 (TID 124)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 122.0 in stage 2.0 (TID 123) in 4 ms on localhost (123/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 123.0 in stage 2.0 (TID 124). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 124.0 in stage 2.0 (TID 125, localhost, partition 124,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 124.0 in stage 2.0 (TID 125)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 123.0 in stage 2.0 (TID 124) in 6 ms on localhost (124/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 124.0 in stage 2.0 (TID 125). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 125.0 in stage 2.0 (TID 126, localhost, partition 125,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 125.0 in stage 2.0 (TID 126)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 124.0 in stage 2.0 (TID 125) in 6 ms on localhost (125/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 125.0 in stage 2.0 (TID 126). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 126.0 in stage 2.0 (TID 127, localhost, partition 126,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 126.0 in stage 2.0 (TID 127)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 125.0 in stage 2.0 (TID 126) in 5 ms on localhost (126/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 126.0 in stage 2.0 (TID 127). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 127.0 in stage 2.0 (TID 128, localhost, partition 127,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 127.0 in stage 2.0 (TID 128)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 126.0 in stage 2.0 (TID 127) in 5 ms on localhost (127/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 127.0 in stage 2.0 (TID 128). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 128.0 in stage 2.0 (TID 129, localhost, partition 128,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 128.0 in stage 2.0 (TID 129)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 127.0 in stage 2.0 (TID 128) in 6 ms on localhost (128/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 128.0 in stage 2.0 (TID 129). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 129.0 in stage 2.0 (TID 130, localhost, partition 129,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 129.0 in stage 2.0 (TID 130)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 128.0 in stage 2.0 (TID 129) in 5 ms on localhost (129/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 129.0 in stage 2.0 (TID 130). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 130.0 in stage 2.0 (TID 131, localhost, partition 130,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 130.0 in stage 2.0 (TID 131)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 129.0 in stage 2.0 (TID 130) in 6 ms on localhost (130/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 130.0 in stage 2.0 (TID 131). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 131.0 in stage 2.0 (TID 132, localhost, partition 131,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 131.0 in stage 2.0 (TID 132)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 130.0 in stage 2.0 (TID 131) in 5 ms on localhost (131/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 131.0 in stage 2.0 (TID 132). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 132.0 in stage 2.0 (TID 133, localhost, partition 132,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 132.0 in stage 2.0 (TID 133)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 131.0 in stage 2.0 (TID 132) in 6 ms on localhost (132/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 132.0 in stage 2.0 (TID 133). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 133.0 in stage 2.0 (TID 134, localhost, partition 133,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 133.0 in stage 2.0 (TID 134)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 132.0 in stage 2.0 (TID 133) in 7 ms on localhost (133/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 133.0 in stage 2.0 (TID 134). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 134.0 in stage 2.0 (TID 135, localhost, partition 134,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 134.0 in stage 2.0 (TID 135)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 133.0 in stage 2.0 (TID 134) in 6 ms on localhost (134/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 134.0 in stage 2.0 (TID 135). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 135.0 in stage 2.0 (TID 136, localhost, partition 135,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 135.0 in stage 2.0 (TID 136)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 134.0 in stage 2.0 (TID 135) in 5 ms on localhost (135/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 135.0 in stage 2.0 (TID 136). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 136.0 in stage 2.0 (TID 137, localhost, partition 136,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 136.0 in stage 2.0 (TID 137)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 135.0 in stage 2.0 (TID 136) in 6 ms on localhost (136/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 136.0 in stage 2.0 (TID 137). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 137.0 in stage 2.0 (TID 138, localhost, partition 137,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 137.0 in stage 2.0 (TID 138)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 136.0 in stage 2.0 (TID 137) in 6 ms on localhost (137/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 137.0 in stage 2.0 (TID 138). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 138.0 in stage 2.0 (TID 139, localhost, partition 138,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 138.0 in stage 2.0 (TID 139)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 137.0 in stage 2.0 (TID 138) in 5 ms on localhost (138/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 138.0 in stage 2.0 (TID 139). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 139.0 in stage 2.0 (TID 140, localhost, partition 139,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 139.0 in stage 2.0 (TID 140)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 138.0 in stage 2.0 (TID 139) in 7 ms on localhost (139/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 139.0 in stage 2.0 (TID 140). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 140.0 in stage 2.0 (TID 141, localhost, partition 140,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 140.0 in stage 2.0 (TID 141)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 139.0 in stage 2.0 (TID 140) in 6 ms on localhost (140/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 140.0 in stage 2.0 (TID 141). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 141.0 in stage 2.0 (TID 142, localhost, partition 141,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 141.0 in stage 2.0 (TID 142)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 140.0 in stage 2.0 (TID 141) in 6 ms on localhost (141/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 141.0 in stage 2.0 (TID 142). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 142.0 in stage 2.0 (TID 143, localhost, partition 142,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 142.0 in stage 2.0 (TID 143)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 141.0 in stage 2.0 (TID 142) in 5 ms on localhost (142/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 142.0 in stage 2.0 (TID 143). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 143.0 in stage 2.0 (TID 144, localhost, partition 143,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 143.0 in stage 2.0 (TID 144)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 142.0 in stage 2.0 (TID 143) in 5 ms on localhost (143/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 143.0 in stage 2.0 (TID 144). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 144.0 in stage 2.0 (TID 145, localhost, partition 144,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 144.0 in stage 2.0 (TID 145)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 143.0 in stage 2.0 (TID 144) in 5 ms on localhost (144/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 144.0 in stage 2.0 (TID 145). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 145.0 in stage 2.0 (TID 146, localhost, partition 145,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 145.0 in stage 2.0 (TID 146)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 144.0 in stage 2.0 (TID 145) in 6 ms on localhost (145/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 145.0 in stage 2.0 (TID 146). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 146.0 in stage 2.0 (TID 147, localhost, partition 146,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 146.0 in stage 2.0 (TID 147)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 145.0 in stage 2.0 (TID 146) in 5 ms on localhost (146/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 146.0 in stage 2.0 (TID 147). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 147.0 in stage 2.0 (TID 148, localhost, partition 147,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 147.0 in stage 2.0 (TID 148)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 146.0 in stage 2.0 (TID 147) in 6 ms on localhost (147/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 147.0 in stage 2.0 (TID 148). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 148.0 in stage 2.0 (TID 149, localhost, partition 148,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 148.0 in stage 2.0 (TID 149)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 147.0 in stage 2.0 (TID 148) in 7 ms on localhost (148/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 148.0 in stage 2.0 (TID 149). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 149.0 in stage 2.0 (TID 150, localhost, partition 149,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 149.0 in stage 2.0 (TID 150)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 148.0 in stage 2.0 (TID 149) in 7 ms on localhost (149/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 149.0 in stage 2.0 (TID 150). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 150.0 in stage 2.0 (TID 151, localhost, partition 150,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 150.0 in stage 2.0 (TID 151)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 149.0 in stage 2.0 (TID 150) in 6 ms on localhost (150/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 150.0 in stage 2.0 (TID 151). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 151.0 in stage 2.0 (TID 152, localhost, partition 151,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 151.0 in stage 2.0 (TID 152)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 150.0 in stage 2.0 (TID 151) in 6 ms on localhost (151/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 151.0 in stage 2.0 (TID 152). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 152.0 in stage 2.0 (TID 153, localhost, partition 152,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 152.0 in stage 2.0 (TID 153)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 151.0 in stage 2.0 (TID 152) in 5 ms on localhost (152/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 152.0 in stage 2.0 (TID 153). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 153.0 in stage 2.0 (TID 154, localhost, partition 153,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 153.0 in stage 2.0 (TID 154)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 152.0 in stage 2.0 (TID 153) in 6 ms on localhost (153/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 153.0 in stage 2.0 (TID 154). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 154.0 in stage 2.0 (TID 155, localhost, partition 154,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 154.0 in stage 2.0 (TID 155)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 153.0 in stage 2.0 (TID 154) in 5 ms on localhost (154/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 154.0 in stage 2.0 (TID 155). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 155.0 in stage 2.0 (TID 156, localhost, partition 155,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 155.0 in stage 2.0 (TID 156)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 154.0 in stage 2.0 (TID 155) in 5 ms on localhost (155/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 155.0 in stage 2.0 (TID 156). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 156.0 in stage 2.0 (TID 157, localhost, partition 156,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 156.0 in stage 2.0 (TID 157)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 155.0 in stage 2.0 (TID 156) in 6 ms on localhost (156/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 156.0 in stage 2.0 (TID 157). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 157.0 in stage 2.0 (TID 158, localhost, partition 157,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 157.0 in stage 2.0 (TID 158)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 156.0 in stage 2.0 (TID 157) in 6 ms on localhost (157/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 157.0 in stage 2.0 (TID 158). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 158.0 in stage 2.0 (TID 159, localhost, partition 158,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 158.0 in stage 2.0 (TID 159)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 157.0 in stage 2.0 (TID 158) in 7 ms on localhost (158/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 158.0 in stage 2.0 (TID 159). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 159.0 in stage 2.0 (TID 160, localhost, partition 159,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 159.0 in stage 2.0 (TID 160)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 158.0 in stage 2.0 (TID 159) in 7 ms on localhost (159/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 159.0 in stage 2.0 (TID 160). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 160.0 in stage 2.0 (TID 161, localhost, partition 160,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 160.0 in stage 2.0 (TID 161)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 159.0 in stage 2.0 (TID 160) in 5 ms on localhost (160/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 160.0 in stage 2.0 (TID 161). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 161.0 in stage 2.0 (TID 162, localhost, partition 161,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 161.0 in stage 2.0 (TID 162)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 160.0 in stage 2.0 (TID 161) in 5 ms on localhost (161/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 161.0 in stage 2.0 (TID 162). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 162.0 in stage 2.0 (TID 163, localhost, partition 162,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 162.0 in stage 2.0 (TID 163)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 161.0 in stage 2.0 (TID 162) in 7 ms on localhost (162/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 162.0 in stage 2.0 (TID 163). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 163.0 in stage 2.0 (TID 164, localhost, partition 163,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 163.0 in stage 2.0 (TID 164)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 162.0 in stage 2.0 (TID 163) in 6 ms on localhost (163/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 163.0 in stage 2.0 (TID 164). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 164.0 in stage 2.0 (TID 165, localhost, partition 164,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 164.0 in stage 2.0 (TID 165)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 163.0 in stage 2.0 (TID 164) in 5 ms on localhost (164/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 164.0 in stage 2.0 (TID 165). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 165.0 in stage 2.0 (TID 166, localhost, partition 165,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 165.0 in stage 2.0 (TID 166)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 164.0 in stage 2.0 (TID 165) in 5 ms on localhost (165/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 165.0 in stage 2.0 (TID 166). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 166.0 in stage 2.0 (TID 167, localhost, partition 166,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 166.0 in stage 2.0 (TID 167)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 165.0 in stage 2.0 (TID 166) in 5 ms on localhost (166/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 166.0 in stage 2.0 (TID 167). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 167.0 in stage 2.0 (TID 168, localhost, partition 167,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 167.0 in stage 2.0 (TID 168)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 166.0 in stage 2.0 (TID 167) in 9 ms on localhost (167/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 167.0 in stage 2.0 (TID 168). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 168.0 in stage 2.0 (TID 169, localhost, partition 168,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 168.0 in stage 2.0 (TID 169)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 167.0 in stage 2.0 (TID 168) in 5 ms on localhost (168/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 168.0 in stage 2.0 (TID 169). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 169.0 in stage 2.0 (TID 170, localhost, partition 169,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 169.0 in stage 2.0 (TID 170)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 168.0 in stage 2.0 (TID 169) in 5 ms on localhost (169/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 169.0 in stage 2.0 (TID 170). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 170.0 in stage 2.0 (TID 171, localhost, partition 170,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 170.0 in stage 2.0 (TID 171)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 169.0 in stage 2.0 (TID 170) in 7 ms on localhost (170/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 170.0 in stage 2.0 (TID 171). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 171.0 in stage 2.0 (TID 172, localhost, partition 171,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 171.0 in stage 2.0 (TID 172)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 170.0 in stage 2.0 (TID 171) in 6 ms on localhost (171/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 171.0 in stage 2.0 (TID 172). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 172.0 in stage 2.0 (TID 173, localhost, partition 172,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 172.0 in stage 2.0 (TID 173)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 171.0 in stage 2.0 (TID 172) in 7 ms on localhost (172/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 172.0 in stage 2.0 (TID 173). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 173.0 in stage 2.0 (TID 174, localhost, partition 173,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 173.0 in stage 2.0 (TID 174)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 172.0 in stage 2.0 (TID 173) in 5 ms on localhost (173/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 173.0 in stage 2.0 (TID 174). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 174.0 in stage 2.0 (TID 175, localhost, partition 174,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 174.0 in stage 2.0 (TID 175)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 173.0 in stage 2.0 (TID 174) in 5 ms on localhost (174/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 174.0 in stage 2.0 (TID 175). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 175.0 in stage 2.0 (TID 176, localhost, partition 175,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 175.0 in stage 2.0 (TID 176)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 174.0 in stage 2.0 (TID 175) in 5 ms on localhost (175/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 175.0 in stage 2.0 (TID 176). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 176.0 in stage 2.0 (TID 177, localhost, partition 176,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 176.0 in stage 2.0 (TID 177)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 175.0 in stage 2.0 (TID 176) in 5 ms on localhost (176/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 176.0 in stage 2.0 (TID 177). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 177.0 in stage 2.0 (TID 178, localhost, partition 177,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 177.0 in stage 2.0 (TID 178)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 176.0 in stage 2.0 (TID 177) in 6 ms on localhost (177/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 177.0 in stage 2.0 (TID 178). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 178.0 in stage 2.0 (TID 179, localhost, partition 178,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 178.0 in stage 2.0 (TID 179)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 177.0 in stage 2.0 (TID 178) in 7 ms on localhost (178/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 178.0 in stage 2.0 (TID 179). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 179.0 in stage 2.0 (TID 180, localhost, partition 179,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 179.0 in stage 2.0 (TID 180)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 178.0 in stage 2.0 (TID 179) in 5 ms on localhost (179/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 179.0 in stage 2.0 (TID 180). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 180.0 in stage 2.0 (TID 181, localhost, partition 180,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 180.0 in stage 2.0 (TID 181)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 179.0 in stage 2.0 (TID 180) in 5 ms on localhost (180/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 180.0 in stage 2.0 (TID 181). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 181.0 in stage 2.0 (TID 182, localhost, partition 181,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 181.0 in stage 2.0 (TID 182)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 180.0 in stage 2.0 (TID 181) in 5 ms on localhost (181/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 181.0 in stage 2.0 (TID 182). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 182.0 in stage 2.0 (TID 183, localhost, partition 182,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 182.0 in stage 2.0 (TID 183)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 181.0 in stage 2.0 (TID 182) in 5 ms on localhost (182/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 182.0 in stage 2.0 (TID 183). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 183.0 in stage 2.0 (TID 184, localhost, partition 183,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 183.0 in stage 2.0 (TID 184)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 182.0 in stage 2.0 (TID 183) in 5 ms on localhost (183/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 183.0 in stage 2.0 (TID 184). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 184.0 in stage 2.0 (TID 185, localhost, partition 184,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 184.0 in stage 2.0 (TID 185)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 183.0 in stage 2.0 (TID 184) in 5 ms on localhost (184/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 184.0 in stage 2.0 (TID 185). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 185.0 in stage 2.0 (TID 186, localhost, partition 185,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 185.0 in stage 2.0 (TID 186)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 184.0 in stage 2.0 (TID 185) in 6 ms on localhost (185/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 185.0 in stage 2.0 (TID 186). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 186.0 in stage 2.0 (TID 187, localhost, partition 186,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 186.0 in stage 2.0 (TID 187)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 185.0 in stage 2.0 (TID 186) in 6 ms on localhost (186/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 186.0 in stage 2.0 (TID 187). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 187.0 in stage 2.0 (TID 188, localhost, partition 187,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 187.0 in stage 2.0 (TID 188)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 186.0 in stage 2.0 (TID 187) in 6 ms on localhost (187/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 187.0 in stage 2.0 (TID 188). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 188.0 in stage 2.0 (TID 189, localhost, partition 188,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 188.0 in stage 2.0 (TID 189)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 187.0 in stage 2.0 (TID 188) in 7 ms on localhost (188/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 188.0 in stage 2.0 (TID 189). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 189.0 in stage 2.0 (TID 190, localhost, partition 189,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 189.0 in stage 2.0 (TID 190)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 188.0 in stage 2.0 (TID 189) in 5 ms on localhost (189/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 189.0 in stage 2.0 (TID 190). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 190.0 in stage 2.0 (TID 191, localhost, partition 190,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 190.0 in stage 2.0 (TID 191)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 189.0 in stage 2.0 (TID 190) in 8 ms on localhost (190/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 190.0 in stage 2.0 (TID 191). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 191.0 in stage 2.0 (TID 192, localhost, partition 191,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 191.0 in stage 2.0 (TID 192)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 190.0 in stage 2.0 (TID 191) in 5 ms on localhost (191/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 191.0 in stage 2.0 (TID 192). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 192.0 in stage 2.0 (TID 193, localhost, partition 192,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 192.0 in stage 2.0 (TID 193)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 191.0 in stage 2.0 (TID 192) in 5 ms on localhost (192/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 192.0 in stage 2.0 (TID 193). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 193.0 in stage 2.0 (TID 194, localhost, partition 193,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 193.0 in stage 2.0 (TID 194)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 192.0 in stage 2.0 (TID 193) in 5 ms on localhost (193/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 193.0 in stage 2.0 (TID 194). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 194.0 in stage 2.0 (TID 195, localhost, partition 194,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 194.0 in stage 2.0 (TID 195)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 193.0 in stage 2.0 (TID 194) in 5 ms on localhost (194/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 194.0 in stage 2.0 (TID 195). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 195.0 in stage 2.0 (TID 196, localhost, partition 195,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 195.0 in stage 2.0 (TID 196)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 194.0 in stage 2.0 (TID 195) in 5 ms on localhost (195/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 195.0 in stage 2.0 (TID 196). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 196.0 in stage 2.0 (TID 197, localhost, partition 196,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 196.0 in stage 2.0 (TID 197)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 195.0 in stage 2.0 (TID 196) in 5 ms on localhost (196/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 196.0 in stage 2.0 (TID 197). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 197.0 in stage 2.0 (TID 198, localhost, partition 197,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 197.0 in stage 2.0 (TID 198)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 196.0 in stage 2.0 (TID 197) in 5 ms on localhost (197/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 197.0 in stage 2.0 (TID 198). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 198.0 in stage 2.0 (TID 199, localhost, partition 198,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 198.0 in stage 2.0 (TID 199)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 197.0 in stage 2.0 (TID 198) in 5 ms on localhost (198/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 198.0 in stage 2.0 (TID 199). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 199.0 in stage 2.0 (TID 200, localhost, partition 199,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 199.0 in stage 2.0 (TID 200)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 198.0 in stage 2.0 (TID 199) in 5 ms on localhost (199/200)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 199.0 in stage 2.0 (TID 200). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 199.0 in stage 2.0 (TID 200) in 4 ms on localhost (200/200)

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 2 (collect at AccessLogJob.scala:195) finished in 1.711 s

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 2.0, whose tasks have all completed, from pool 

2019-05-25 16:48:34
[INFO]-[Thread: streaming-job-executor-0]-[org.apache.spark.Logging$class.logInfo()]: Job 2 finished: collect at AccessLogJob.scala:195, took 1.749266 s

2019-05-25 16:48:34
[INFO]-[Thread: streaming-job-executor-0]-[org.apache.spark.Logging$class.logInfo()]: Starting job: show at AccessLogJob.scala:196

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Registering RDD 21 (show at AccessLogJob.scala:196)

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 3 (show at AccessLogJob.scala:196) with 1 output partitions

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 4 (show at AccessLogJob.scala:196)

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List(ShuffleMapStage 3)

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 4 (MapPartitionsRDD[26] at show at AccessLogJob.scala:196), which has no missing parents

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_2 stored as values in memory (estimated size 18.6 KB, free 2.4 GB)

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.5 KB, free 2.4 GB)

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_2_piece0 in memory on localhost:60954 (size: 8.5 KB, free: 2.4 GB)

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 2 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[26] at show at AccessLogJob.scala:196)

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 4.0 with 1 tasks

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 4.0 (TID 201, localhost, partition 0,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 4.0 (TID 201)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 0.0 in stage 4.0 (TID 201). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 0.0 in stage 4.0 (TID 201) in 4 ms on localhost (1/1)

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 4 (show at AccessLogJob.scala:196) finished in 0.005 s

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 4.0, whose tasks have all completed, from pool 

2019-05-25 16:48:34
[INFO]-[Thread: streaming-job-executor-0]-[org.apache.spark.Logging$class.logInfo()]: Job 3 finished: show at AccessLogJob.scala:196, took 0.016422 s

2019-05-25 16:48:34
[INFO]-[Thread: streaming-job-executor-0]-[org.apache.spark.Logging$class.logInfo()]: Starting job: show at AccessLogJob.scala:196

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Size of output statuses for shuffle 1 is 82 bytes

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 4 (show at AccessLogJob.scala:196) with 199 output partitions

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 6 (show at AccessLogJob.scala:196)

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List(ShuffleMapStage 5)

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 6 (MapPartitionsRDD[26] at show at AccessLogJob.scala:196), which has no missing parents

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_3 stored as values in memory (estimated size 18.6 KB, free 2.4 GB)

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.5 KB, free 2.4 GB)

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_3_piece0 in memory on localhost:60954 (size: 8.5 KB, free: 2.4 GB)

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 3 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 199 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at show at AccessLogJob.scala:196)

2019-05-25 16:48:34
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 6.0 with 199 tasks

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 6.0 (TID 202, localhost, partition 1,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 6.0 (TID 202)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 0.0 in stage 6.0 (TID 202). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 1.0 in stage 6.0 (TID 203, localhost, partition 2,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 1.0 in stage 6.0 (TID 203)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 0.0 in stage 6.0 (TID 202) in 6 ms on localhost (1/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 1.0 in stage 6.0 (TID 203). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 2.0 in stage 6.0 (TID 204, localhost, partition 3,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 2.0 in stage 6.0 (TID 204)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 1.0 in stage 6.0 (TID 203) in 5 ms on localhost (2/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 2.0 in stage 6.0 (TID 204). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 3.0 in stage 6.0 (TID 205, localhost, partition 4,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 3.0 in stage 6.0 (TID 205)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 2.0 in stage 6.0 (TID 204) in 5 ms on localhost (3/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 3.0 in stage 6.0 (TID 205). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 4.0 in stage 6.0 (TID 206, localhost, partition 5,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 4.0 in stage 6.0 (TID 206)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 3.0 in stage 6.0 (TID 205) in 5 ms on localhost (4/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 4.0 in stage 6.0 (TID 206). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 5.0 in stage 6.0 (TID 207, localhost, partition 6,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 5.0 in stage 6.0 (TID 207)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 4.0 in stage 6.0 (TID 206) in 5 ms on localhost (5/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 5.0 in stage 6.0 (TID 207). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 6.0 in stage 6.0 (TID 208, localhost, partition 7,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 6.0 in stage 6.0 (TID 208)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 5.0 in stage 6.0 (TID 207) in 7 ms on localhost (6/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 6.0 in stage 6.0 (TID 208). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 7.0 in stage 6.0 (TID 209, localhost, partition 8,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 7.0 in stage 6.0 (TID 209)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 6.0 in stage 6.0 (TID 208) in 4 ms on localhost (7/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 7.0 in stage 6.0 (TID 209). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 8.0 in stage 6.0 (TID 210, localhost, partition 9,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 8.0 in stage 6.0 (TID 210)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 7.0 in stage 6.0 (TID 209) in 5 ms on localhost (8/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 8.0 in stage 6.0 (TID 210). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 9.0 in stage 6.0 (TID 211, localhost, partition 10,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 9.0 in stage 6.0 (TID 211)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 8.0 in stage 6.0 (TID 210) in 6 ms on localhost (9/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 9.0 in stage 6.0 (TID 211). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 10.0 in stage 6.0 (TID 212, localhost, partition 11,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 10.0 in stage 6.0 (TID 212)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 9.0 in stage 6.0 (TID 211) in 6 ms on localhost (10/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 10.0 in stage 6.0 (TID 212). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 11.0 in stage 6.0 (TID 213, localhost, partition 12,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 11.0 in stage 6.0 (TID 213)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 10.0 in stage 6.0 (TID 212) in 4 ms on localhost (11/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 11.0 in stage 6.0 (TID 213). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 12.0 in stage 6.0 (TID 214, localhost, partition 13,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 12.0 in stage 6.0 (TID 214)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 11.0 in stage 6.0 (TID 213) in 4 ms on localhost (12/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 12.0 in stage 6.0 (TID 214). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 13.0 in stage 6.0 (TID 215, localhost, partition 14,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 13.0 in stage 6.0 (TID 215)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 12.0 in stage 6.0 (TID 214) in 5 ms on localhost (13/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 13.0 in stage 6.0 (TID 215). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 14.0 in stage 6.0 (TID 216, localhost, partition 15,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 14.0 in stage 6.0 (TID 216)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 13.0 in stage 6.0 (TID 215) in 4 ms on localhost (14/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 14.0 in stage 6.0 (TID 216). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 15.0 in stage 6.0 (TID 217, localhost, partition 16,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 15.0 in stage 6.0 (TID 217)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 14.0 in stage 6.0 (TID 216) in 4 ms on localhost (15/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 15.0 in stage 6.0 (TID 217). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 16.0 in stage 6.0 (TID 218, localhost, partition 17,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 16.0 in stage 6.0 (TID 218)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 15.0 in stage 6.0 (TID 217) in 5 ms on localhost (16/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 16.0 in stage 6.0 (TID 218). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 17.0 in stage 6.0 (TID 219, localhost, partition 18,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 17.0 in stage 6.0 (TID 219)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 16.0 in stage 6.0 (TID 218) in 4 ms on localhost (17/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 17.0 in stage 6.0 (TID 219). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 18.0 in stage 6.0 (TID 220, localhost, partition 19,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 18.0 in stage 6.0 (TID 220)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 17.0 in stage 6.0 (TID 219) in 7 ms on localhost (18/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 18.0 in stage 6.0 (TID 220). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 19.0 in stage 6.0 (TID 221, localhost, partition 20,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 19.0 in stage 6.0 (TID 221)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 18.0 in stage 6.0 (TID 220) in 6 ms on localhost (19/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 19.0 in stage 6.0 (TID 221). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 20.0 in stage 6.0 (TID 222, localhost, partition 21,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 20.0 in stage 6.0 (TID 222)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 19.0 in stage 6.0 (TID 221) in 5 ms on localhost (20/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 20.0 in stage 6.0 (TID 222). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 21.0 in stage 6.0 (TID 223, localhost, partition 22,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 21.0 in stage 6.0 (TID 223)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 20.0 in stage 6.0 (TID 222) in 4 ms on localhost (21/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 21.0 in stage 6.0 (TID 223). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 22.0 in stage 6.0 (TID 224, localhost, partition 23,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 22.0 in stage 6.0 (TID 224)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 21.0 in stage 6.0 (TID 223) in 4 ms on localhost (22/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 22.0 in stage 6.0 (TID 224). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 23.0 in stage 6.0 (TID 225, localhost, partition 24,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 23.0 in stage 6.0 (TID 225)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 22.0 in stage 6.0 (TID 224) in 5 ms on localhost (23/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 23.0 in stage 6.0 (TID 225). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 24.0 in stage 6.0 (TID 226, localhost, partition 25,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 24.0 in stage 6.0 (TID 226)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 23.0 in stage 6.0 (TID 225) in 5 ms on localhost (24/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 24.0 in stage 6.0 (TID 226). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 25.0 in stage 6.0 (TID 227, localhost, partition 26,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 25.0 in stage 6.0 (TID 227)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 24.0 in stage 6.0 (TID 226) in 5 ms on localhost (25/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 25.0 in stage 6.0 (TID 227). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 26.0 in stage 6.0 (TID 228, localhost, partition 27,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 26.0 in stage 6.0 (TID 228)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 25.0 in stage 6.0 (TID 227) in 6 ms on localhost (26/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 26.0 in stage 6.0 (TID 228). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 27.0 in stage 6.0 (TID 229, localhost, partition 28,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 27.0 in stage 6.0 (TID 229)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 26.0 in stage 6.0 (TID 228) in 8 ms on localhost (27/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 27.0 in stage 6.0 (TID 229). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 28.0 in stage 6.0 (TID 230, localhost, partition 29,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 28.0 in stage 6.0 (TID 230)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 27.0 in stage 6.0 (TID 229) in 4 ms on localhost (28/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 28.0 in stage 6.0 (TID 230). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 29.0 in stage 6.0 (TID 231, localhost, partition 30,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 29.0 in stage 6.0 (TID 231)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 28.0 in stage 6.0 (TID 230) in 6 ms on localhost (29/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 29.0 in stage 6.0 (TID 231). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 30.0 in stage 6.0 (TID 232, localhost, partition 31,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 30.0 in stage 6.0 (TID 232)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 29.0 in stage 6.0 (TID 231) in 5 ms on localhost (30/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 30.0 in stage 6.0 (TID 232). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 31.0 in stage 6.0 (TID 233, localhost, partition 32,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 31.0 in stage 6.0 (TID 233)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 30.0 in stage 6.0 (TID 232) in 6 ms on localhost (31/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 31.0 in stage 6.0 (TID 233). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 32.0 in stage 6.0 (TID 234, localhost, partition 33,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 32.0 in stage 6.0 (TID 234)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 31.0 in stage 6.0 (TID 233) in 5 ms on localhost (32/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 32.0 in stage 6.0 (TID 234). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 33.0 in stage 6.0 (TID 235, localhost, partition 34,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 33.0 in stage 6.0 (TID 235)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 32.0 in stage 6.0 (TID 234) in 5 ms on localhost (33/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 33.0 in stage 6.0 (TID 235). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 34.0 in stage 6.0 (TID 236, localhost, partition 35,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 34.0 in stage 6.0 (TID 236)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 33.0 in stage 6.0 (TID 235) in 5 ms on localhost (34/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 34.0 in stage 6.0 (TID 236). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 35.0 in stage 6.0 (TID 237, localhost, partition 36,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 35.0 in stage 6.0 (TID 237)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 34.0 in stage 6.0 (TID 236) in 4 ms on localhost (35/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 35.0 in stage 6.0 (TID 237). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 36.0 in stage 6.0 (TID 238, localhost, partition 37,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 36.0 in stage 6.0 (TID 238)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 35.0 in stage 6.0 (TID 237) in 5 ms on localhost (36/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 36.0 in stage 6.0 (TID 238). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 37.0 in stage 6.0 (TID 239, localhost, partition 38,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 37.0 in stage 6.0 (TID 239)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 36.0 in stage 6.0 (TID 238) in 4 ms on localhost (37/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 37.0 in stage 6.0 (TID 239). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 38.0 in stage 6.0 (TID 240, localhost, partition 39,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 38.0 in stage 6.0 (TID 240)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 37.0 in stage 6.0 (TID 239) in 5 ms on localhost (38/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 38.0 in stage 6.0 (TID 240). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 39.0 in stage 6.0 (TID 241, localhost, partition 40,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 39.0 in stage 6.0 (TID 241)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 38.0 in stage 6.0 (TID 240) in 4 ms on localhost (39/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 39.0 in stage 6.0 (TID 241). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 40.0 in stage 6.0 (TID 242, localhost, partition 41,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 40.0 in stage 6.0 (TID 242)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 39.0 in stage 6.0 (TID 241) in 5 ms on localhost (40/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 40.0 in stage 6.0 (TID 242). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 41.0 in stage 6.0 (TID 243, localhost, partition 42,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 41.0 in stage 6.0 (TID 243)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 40.0 in stage 6.0 (TID 242) in 6 ms on localhost (41/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 41.0 in stage 6.0 (TID 243). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 42.0 in stage 6.0 (TID 244, localhost, partition 43,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 42.0 in stage 6.0 (TID 244)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 41.0 in stage 6.0 (TID 243) in 7 ms on localhost (42/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 42.0 in stage 6.0 (TID 244). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 43.0 in stage 6.0 (TID 245, localhost, partition 44,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 43.0 in stage 6.0 (TID 245)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 42.0 in stage 6.0 (TID 244) in 5 ms on localhost (43/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 43.0 in stage 6.0 (TID 245). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 44.0 in stage 6.0 (TID 246, localhost, partition 45,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 44.0 in stage 6.0 (TID 246)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 43.0 in stage 6.0 (TID 245) in 5 ms on localhost (44/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 44.0 in stage 6.0 (TID 246). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 45.0 in stage 6.0 (TID 247, localhost, partition 46,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 45.0 in stage 6.0 (TID 247)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 44.0 in stage 6.0 (TID 246) in 5 ms on localhost (45/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 45.0 in stage 6.0 (TID 247). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 46.0 in stage 6.0 (TID 248, localhost, partition 47,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 46.0 in stage 6.0 (TID 248)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 45.0 in stage 6.0 (TID 247) in 5 ms on localhost (46/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 46.0 in stage 6.0 (TID 248). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 47.0 in stage 6.0 (TID 249, localhost, partition 48,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 47.0 in stage 6.0 (TID 249)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 46.0 in stage 6.0 (TID 248) in 4 ms on localhost (47/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 47.0 in stage 6.0 (TID 249). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 48.0 in stage 6.0 (TID 250, localhost, partition 49,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 48.0 in stage 6.0 (TID 250)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 47.0 in stage 6.0 (TID 249) in 6 ms on localhost (48/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 48.0 in stage 6.0 (TID 250). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 49.0 in stage 6.0 (TID 251, localhost, partition 50,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 49.0 in stage 6.0 (TID 251)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 48.0 in stage 6.0 (TID 250) in 4 ms on localhost (49/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 49.0 in stage 6.0 (TID 251). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 50.0 in stage 6.0 (TID 252, localhost, partition 51,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 50.0 in stage 6.0 (TID 252)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 49.0 in stage 6.0 (TID 251) in 5 ms on localhost (50/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 50.0 in stage 6.0 (TID 252). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 51.0 in stage 6.0 (TID 253, localhost, partition 52,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 51.0 in stage 6.0 (TID 253)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 50.0 in stage 6.0 (TID 252) in 6 ms on localhost (51/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 51.0 in stage 6.0 (TID 253). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 52.0 in stage 6.0 (TID 254, localhost, partition 53,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 52.0 in stage 6.0 (TID 254)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 51.0 in stage 6.0 (TID 253) in 5 ms on localhost (52/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 52.0 in stage 6.0 (TID 254). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 53.0 in stage 6.0 (TID 255, localhost, partition 54,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 53.0 in stage 6.0 (TID 255)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 52.0 in stage 6.0 (TID 254) in 4 ms on localhost (53/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 53.0 in stage 6.0 (TID 255). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 54.0 in stage 6.0 (TID 256, localhost, partition 55,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 54.0 in stage 6.0 (TID 256)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 53.0 in stage 6.0 (TID 255) in 5 ms on localhost (54/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 54.0 in stage 6.0 (TID 256). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 55.0 in stage 6.0 (TID 257, localhost, partition 56,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 55.0 in stage 6.0 (TID 257)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 54.0 in stage 6.0 (TID 256) in 5 ms on localhost (55/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 55.0 in stage 6.0 (TID 257). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 56.0 in stage 6.0 (TID 258, localhost, partition 57,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 56.0 in stage 6.0 (TID 258)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 55.0 in stage 6.0 (TID 257) in 4 ms on localhost (56/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 56.0 in stage 6.0 (TID 258). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 57.0 in stage 6.0 (TID 259, localhost, partition 58,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 57.0 in stage 6.0 (TID 259)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 56.0 in stage 6.0 (TID 258) in 5 ms on localhost (57/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 57.0 in stage 6.0 (TID 259). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 58.0 in stage 6.0 (TID 260, localhost, partition 59,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 58.0 in stage 6.0 (TID 260)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 57.0 in stage 6.0 (TID 259) in 7 ms on localhost (58/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 58.0 in stage 6.0 (TID 260). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 59.0 in stage 6.0 (TID 261, localhost, partition 60,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 59.0 in stage 6.0 (TID 261)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 58.0 in stage 6.0 (TID 260) in 7 ms on localhost (59/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 59.0 in stage 6.0 (TID 261). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 60.0 in stage 6.0 (TID 262, localhost, partition 61,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 60.0 in stage 6.0 (TID 262)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 59.0 in stage 6.0 (TID 261) in 5 ms on localhost (60/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 60.0 in stage 6.0 (TID 262). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 61.0 in stage 6.0 (TID 263, localhost, partition 62,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 61.0 in stage 6.0 (TID 263)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 60.0 in stage 6.0 (TID 262) in 5 ms on localhost (61/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 61.0 in stage 6.0 (TID 263). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 62.0 in stage 6.0 (TID 264, localhost, partition 63,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 62.0 in stage 6.0 (TID 264)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 61.0 in stage 6.0 (TID 263) in 5 ms on localhost (62/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 62.0 in stage 6.0 (TID 264). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 63.0 in stage 6.0 (TID 265, localhost, partition 64,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 63.0 in stage 6.0 (TID 265)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 62.0 in stage 6.0 (TID 264) in 5 ms on localhost (63/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 63.0 in stage 6.0 (TID 265). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 64.0 in stage 6.0 (TID 266, localhost, partition 65,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 64.0 in stage 6.0 (TID 266)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 63.0 in stage 6.0 (TID 265) in 5 ms on localhost (64/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 64.0 in stage 6.0 (TID 266). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 65.0 in stage 6.0 (TID 267, localhost, partition 66,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 65.0 in stage 6.0 (TID 267)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 64.0 in stage 6.0 (TID 266) in 5 ms on localhost (65/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 65.0 in stage 6.0 (TID 267). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 66.0 in stage 6.0 (TID 268, localhost, partition 67,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 66.0 in stage 6.0 (TID 268)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 65.0 in stage 6.0 (TID 267) in 5 ms on localhost (66/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 66.0 in stage 6.0 (TID 268). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 67.0 in stage 6.0 (TID 269, localhost, partition 68,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 67.0 in stage 6.0 (TID 269)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 66.0 in stage 6.0 (TID 268) in 4 ms on localhost (67/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 67.0 in stage 6.0 (TID 269). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 68.0 in stage 6.0 (TID 270, localhost, partition 69,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 68.0 in stage 6.0 (TID 270)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 67.0 in stage 6.0 (TID 269) in 4 ms on localhost (68/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 68.0 in stage 6.0 (TID 270). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 69.0 in stage 6.0 (TID 271, localhost, partition 70,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 69.0 in stage 6.0 (TID 271)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 68.0 in stage 6.0 (TID 270) in 4 ms on localhost (69/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 69.0 in stage 6.0 (TID 271). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 70.0 in stage 6.0 (TID 272, localhost, partition 71,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 70.0 in stage 6.0 (TID 272)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 69.0 in stage 6.0 (TID 271) in 6 ms on localhost (70/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 70.0 in stage 6.0 (TID 272). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 71.0 in stage 6.0 (TID 273, localhost, partition 72,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 71.0 in stage 6.0 (TID 273)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 70.0 in stage 6.0 (TID 272) in 5 ms on localhost (71/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 71.0 in stage 6.0 (TID 273). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 72.0 in stage 6.0 (TID 274, localhost, partition 73,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 72.0 in stage 6.0 (TID 274)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 71.0 in stage 6.0 (TID 273) in 4 ms on localhost (72/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 72.0 in stage 6.0 (TID 274). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 73.0 in stage 6.0 (TID 275, localhost, partition 74,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 73.0 in stage 6.0 (TID 275)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 72.0 in stage 6.0 (TID 274) in 4 ms on localhost (73/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 73.0 in stage 6.0 (TID 275). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 74.0 in stage 6.0 (TID 276, localhost, partition 75,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 74.0 in stage 6.0 (TID 276)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 73.0 in stage 6.0 (TID 275) in 5 ms on localhost (74/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 74.0 in stage 6.0 (TID 276). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 75.0 in stage 6.0 (TID 277, localhost, partition 76,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 75.0 in stage 6.0 (TID 277)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 74.0 in stage 6.0 (TID 276) in 5 ms on localhost (75/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 75.0 in stage 6.0 (TID 277). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 76.0 in stage 6.0 (TID 278, localhost, partition 77,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 76.0 in stage 6.0 (TID 278)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 75.0 in stage 6.0 (TID 277) in 4 ms on localhost (76/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 76.0 in stage 6.0 (TID 278). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 77.0 in stage 6.0 (TID 279, localhost, partition 78,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 77.0 in stage 6.0 (TID 279)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 76.0 in stage 6.0 (TID 278) in 4 ms on localhost (77/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 77.0 in stage 6.0 (TID 279). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 78.0 in stage 6.0 (TID 280, localhost, partition 79,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 78.0 in stage 6.0 (TID 280)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 77.0 in stage 6.0 (TID 279) in 4 ms on localhost (78/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 78.0 in stage 6.0 (TID 280). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 79.0 in stage 6.0 (TID 281, localhost, partition 80,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 79.0 in stage 6.0 (TID 281)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 78.0 in stage 6.0 (TID 280) in 5 ms on localhost (79/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 79.0 in stage 6.0 (TID 281). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 80.0 in stage 6.0 (TID 282, localhost, partition 81,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 80.0 in stage 6.0 (TID 282)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 79.0 in stage 6.0 (TID 281) in 5 ms on localhost (80/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 80.0 in stage 6.0 (TID 282). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 81.0 in stage 6.0 (TID 283, localhost, partition 82,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 81.0 in stage 6.0 (TID 283)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 80.0 in stage 6.0 (TID 282) in 5 ms on localhost (81/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 81.0 in stage 6.0 (TID 283). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 82.0 in stage 6.0 (TID 284, localhost, partition 83,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 82.0 in stage 6.0 (TID 284)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 81.0 in stage 6.0 (TID 283) in 5 ms on localhost (82/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 82.0 in stage 6.0 (TID 284). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 83.0 in stage 6.0 (TID 285, localhost, partition 84,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 83.0 in stage 6.0 (TID 285)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 82.0 in stage 6.0 (TID 284) in 4 ms on localhost (83/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 83.0 in stage 6.0 (TID 285). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 84.0 in stage 6.0 (TID 286, localhost, partition 85,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 84.0 in stage 6.0 (TID 286)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 83.0 in stage 6.0 (TID 285) in 4 ms on localhost (84/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 84.0 in stage 6.0 (TID 286). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 85.0 in stage 6.0 (TID 287, localhost, partition 86,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 85.0 in stage 6.0 (TID 287)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 84.0 in stage 6.0 (TID 286) in 4 ms on localhost (85/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 85.0 in stage 6.0 (TID 287). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 86.0 in stage 6.0 (TID 288, localhost, partition 87,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 86.0 in stage 6.0 (TID 288)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 85.0 in stage 6.0 (TID 287) in 5 ms on localhost (86/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 86.0 in stage 6.0 (TID 288). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 87.0 in stage 6.0 (TID 289, localhost, partition 88,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 87.0 in stage 6.0 (TID 289)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 86.0 in stage 6.0 (TID 288) in 4 ms on localhost (87/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 87.0 in stage 6.0 (TID 289). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 88.0 in stage 6.0 (TID 290, localhost, partition 89,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 88.0 in stage 6.0 (TID 290)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 87.0 in stage 6.0 (TID 289) in 5 ms on localhost (88/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 88.0 in stage 6.0 (TID 290). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 89.0 in stage 6.0 (TID 291, localhost, partition 90,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 89.0 in stage 6.0 (TID 291)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 88.0 in stage 6.0 (TID 290) in 5 ms on localhost (89/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 89.0 in stage 6.0 (TID 291). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 90.0 in stage 6.0 (TID 292, localhost, partition 91,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 90.0 in stage 6.0 (TID 292)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 89.0 in stage 6.0 (TID 291) in 5 ms on localhost (90/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 90.0 in stage 6.0 (TID 292). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 91.0 in stage 6.0 (TID 293, localhost, partition 92,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 91.0 in stage 6.0 (TID 293)

2019-05-25 16:48:34
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 90.0 in stage 6.0 (TID 292) in 4 ms on localhost (91/199)

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:34
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 91.0 in stage 6.0 (TID 293). 1652 bytes result sent to driver

2019-05-25 16:48:34
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 92.0 in stage 6.0 (TID 294, localhost, partition 93,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 92.0 in stage 6.0 (TID 294)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 91.0 in stage 6.0 (TID 293) in 5 ms on localhost (92/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 92.0 in stage 6.0 (TID 294). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 93.0 in stage 6.0 (TID 295, localhost, partition 94,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 93.0 in stage 6.0 (TID 295)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 92.0 in stage 6.0 (TID 294) in 4 ms on localhost (93/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 93.0 in stage 6.0 (TID 295). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 94.0 in stage 6.0 (TID 296, localhost, partition 95,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 94.0 in stage 6.0 (TID 296)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 93.0 in stage 6.0 (TID 295) in 5 ms on localhost (94/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 94.0 in stage 6.0 (TID 296). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 95.0 in stage 6.0 (TID 297, localhost, partition 96,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 95.0 in stage 6.0 (TID 297)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 94.0 in stage 6.0 (TID 296) in 5 ms on localhost (95/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 95.0 in stage 6.0 (TID 297). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 96.0 in stage 6.0 (TID 298, localhost, partition 97,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 96.0 in stage 6.0 (TID 298)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 95.0 in stage 6.0 (TID 297) in 9 ms on localhost (96/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 96.0 in stage 6.0 (TID 298). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 97.0 in stage 6.0 (TID 299, localhost, partition 98,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 97.0 in stage 6.0 (TID 299)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 96.0 in stage 6.0 (TID 298) in 5 ms on localhost (97/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 97.0 in stage 6.0 (TID 299). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 98.0 in stage 6.0 (TID 300, localhost, partition 99,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 98.0 in stage 6.0 (TID 300)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 97.0 in stage 6.0 (TID 299) in 5 ms on localhost (98/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 98.0 in stage 6.0 (TID 300). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 99.0 in stage 6.0 (TID 301, localhost, partition 100,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 99.0 in stage 6.0 (TID 301)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 98.0 in stage 6.0 (TID 300) in 6 ms on localhost (99/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 99.0 in stage 6.0 (TID 301). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 100.0 in stage 6.0 (TID 302, localhost, partition 101,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 100.0 in stage 6.0 (TID 302)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 99.0 in stage 6.0 (TID 301) in 6 ms on localhost (100/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 100.0 in stage 6.0 (TID 302). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 101.0 in stage 6.0 (TID 303, localhost, partition 102,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 101.0 in stage 6.0 (TID 303)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 100.0 in stage 6.0 (TID 302) in 6 ms on localhost (101/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 101.0 in stage 6.0 (TID 303). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 102.0 in stage 6.0 (TID 304, localhost, partition 103,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 102.0 in stage 6.0 (TID 304)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 101.0 in stage 6.0 (TID 303) in 6 ms on localhost (102/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 102.0 in stage 6.0 (TID 304). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 103.0 in stage 6.0 (TID 305, localhost, partition 104,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 103.0 in stage 6.0 (TID 305)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 102.0 in stage 6.0 (TID 304) in 6 ms on localhost (103/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 103.0 in stage 6.0 (TID 305). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 104.0 in stage 6.0 (TID 306, localhost, partition 105,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 104.0 in stage 6.0 (TID 306)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 103.0 in stage 6.0 (TID 305) in 7 ms on localhost (104/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 104.0 in stage 6.0 (TID 306). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 105.0 in stage 6.0 (TID 307, localhost, partition 106,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 105.0 in stage 6.0 (TID 307)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 104.0 in stage 6.0 (TID 306) in 5 ms on localhost (105/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 105.0 in stage 6.0 (TID 307). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 106.0 in stage 6.0 (TID 308, localhost, partition 107,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 106.0 in stage 6.0 (TID 308)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 105.0 in stage 6.0 (TID 307) in 6 ms on localhost (106/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 106.0 in stage 6.0 (TID 308). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 107.0 in stage 6.0 (TID 309, localhost, partition 108,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 107.0 in stage 6.0 (TID 309)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 106.0 in stage 6.0 (TID 308) in 7 ms on localhost (107/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 107.0 in stage 6.0 (TID 309). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 108.0 in stage 6.0 (TID 310, localhost, partition 109,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 108.0 in stage 6.0 (TID 310)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 107.0 in stage 6.0 (TID 309) in 4 ms on localhost (108/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 108.0 in stage 6.0 (TID 310). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 109.0 in stage 6.0 (TID 311, localhost, partition 110,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 109.0 in stage 6.0 (TID 311)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 108.0 in stage 6.0 (TID 310) in 4 ms on localhost (109/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 109.0 in stage 6.0 (TID 311). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 110.0 in stage 6.0 (TID 312, localhost, partition 111,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 110.0 in stage 6.0 (TID 312)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 109.0 in stage 6.0 (TID 311) in 4 ms on localhost (110/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 110.0 in stage 6.0 (TID 312). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 111.0 in stage 6.0 (TID 313, localhost, partition 112,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 111.0 in stage 6.0 (TID 313)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 110.0 in stage 6.0 (TID 312) in 5 ms on localhost (111/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 111.0 in stage 6.0 (TID 313). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 112.0 in stage 6.0 (TID 314, localhost, partition 113,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 112.0 in stage 6.0 (TID 314)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 111.0 in stage 6.0 (TID 313) in 7 ms on localhost (112/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 112.0 in stage 6.0 (TID 314). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 113.0 in stage 6.0 (TID 315, localhost, partition 114,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 113.0 in stage 6.0 (TID 315)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 112.0 in stage 6.0 (TID 314) in 4 ms on localhost (113/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 113.0 in stage 6.0 (TID 315). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 114.0 in stage 6.0 (TID 316, localhost, partition 115,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 114.0 in stage 6.0 (TID 316)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 113.0 in stage 6.0 (TID 315) in 6 ms on localhost (114/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 114.0 in stage 6.0 (TID 316). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 115.0 in stage 6.0 (TID 317, localhost, partition 116,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 115.0 in stage 6.0 (TID 317)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 114.0 in stage 6.0 (TID 316) in 7 ms on localhost (115/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 115.0 in stage 6.0 (TID 317). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 116.0 in stage 6.0 (TID 318, localhost, partition 117,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 116.0 in stage 6.0 (TID 318)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 115.0 in stage 6.0 (TID 317) in 6 ms on localhost (116/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 116.0 in stage 6.0 (TID 318). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 117.0 in stage 6.0 (TID 319, localhost, partition 118,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 117.0 in stage 6.0 (TID 319)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 116.0 in stage 6.0 (TID 318) in 6 ms on localhost (117/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 117.0 in stage 6.0 (TID 319). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 118.0 in stage 6.0 (TID 320, localhost, partition 119,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 118.0 in stage 6.0 (TID 320)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 117.0 in stage 6.0 (TID 319) in 5 ms on localhost (118/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 118.0 in stage 6.0 (TID 320). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 119.0 in stage 6.0 (TID 321, localhost, partition 120,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 119.0 in stage 6.0 (TID 321)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 118.0 in stage 6.0 (TID 320) in 4 ms on localhost (119/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 119.0 in stage 6.0 (TID 321). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 120.0 in stage 6.0 (TID 322, localhost, partition 121,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 120.0 in stage 6.0 (TID 322)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 119.0 in stage 6.0 (TID 321) in 4 ms on localhost (120/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 120.0 in stage 6.0 (TID 322). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 121.0 in stage 6.0 (TID 323, localhost, partition 122,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 121.0 in stage 6.0 (TID 323)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 120.0 in stage 6.0 (TID 322) in 4 ms on localhost (121/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 121.0 in stage 6.0 (TID 323). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 122.0 in stage 6.0 (TID 324, localhost, partition 123,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 122.0 in stage 6.0 (TID 324)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 121.0 in stage 6.0 (TID 323) in 4 ms on localhost (122/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 122.0 in stage 6.0 (TID 324). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 123.0 in stage 6.0 (TID 325, localhost, partition 124,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 123.0 in stage 6.0 (TID 325)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 122.0 in stage 6.0 (TID 324) in 4 ms on localhost (123/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 123.0 in stage 6.0 (TID 325). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 124.0 in stage 6.0 (TID 326, localhost, partition 125,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 124.0 in stage 6.0 (TID 326)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 123.0 in stage 6.0 (TID 325) in 4 ms on localhost (124/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 22

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 124.0 in stage 6.0 (TID 326). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 125.0 in stage 6.0 (TID 327, localhost, partition 126,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 125.0 in stage 6.0 (TID 327)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 124.0 in stage 6.0 (TID 326) in 7 ms on localhost (125/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 125.0 in stage 6.0 (TID 327). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Starting task 126.0 in stage 6.0 (TID 328, localhost, partition 127,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 126.0 in stage 6.0 (TID 328)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 125.0 in stage 6.0 (TID 327) in 5 ms on localhost (126/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 126.0 in stage 6.0 (TID 328). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 127.0 in stage 6.0 (TID 329, localhost, partition 128,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 127.0 in stage 6.0 (TID 329)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 126.0 in stage 6.0 (TID 328) in 4 ms on localhost (127/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 127.0 in stage 6.0 (TID 329). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 128.0 in stage 6.0 (TID 330, localhost, partition 129,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 128.0 in stage 6.0 (TID 330)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 127.0 in stage 6.0 (TID 329) in 4 ms on localhost (128/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_2_piece0 on localhost:60954 in memory (size: 8.5 KB, free: 2.4 GB)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 128.0 in stage 6.0 (TID 330). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 129.0 in stage 6.0 (TID 331, localhost, partition 130,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 129.0 in stage 6.0 (TID 331)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 128.0 in stage 6.0 (TID 330) in 5 ms on localhost (129/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 129.0 in stage 6.0 (TID 331). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 130.0 in stage 6.0 (TID 332, localhost, partition 131,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 130.0 in stage 6.0 (TID 332)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 129.0 in stage 6.0 (TID 331) in 5 ms on localhost (130/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 130.0 in stage 6.0 (TID 332). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 131.0 in stage 6.0 (TID 333, localhost, partition 132,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 131.0 in stage 6.0 (TID 333)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 130.0 in stage 6.0 (TID 332) in 4 ms on localhost (131/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 131.0 in stage 6.0 (TID 333). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 132.0 in stage 6.0 (TID 334, localhost, partition 133,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 132.0 in stage 6.0 (TID 334)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 131.0 in stage 6.0 (TID 333) in 4 ms on localhost (132/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 132.0 in stage 6.0 (TID 334). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 133.0 in stage 6.0 (TID 335, localhost, partition 134,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 133.0 in stage 6.0 (TID 335)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 132.0 in stage 6.0 (TID 334) in 4 ms on localhost (133/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 133.0 in stage 6.0 (TID 335). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 134.0 in stage 6.0 (TID 336, localhost, partition 135,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 134.0 in stage 6.0 (TID 336)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 133.0 in stage 6.0 (TID 335) in 4 ms on localhost (134/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 134.0 in stage 6.0 (TID 336). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 135.0 in stage 6.0 (TID 337, localhost, partition 136,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 135.0 in stage 6.0 (TID 337)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 134.0 in stage 6.0 (TID 336) in 4 ms on localhost (135/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 135.0 in stage 6.0 (TID 337). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 136.0 in stage 6.0 (TID 338, localhost, partition 137,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 136.0 in stage 6.0 (TID 338)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 135.0 in stage 6.0 (TID 337) in 5 ms on localhost (136/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 136.0 in stage 6.0 (TID 338). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 137.0 in stage 6.0 (TID 339, localhost, partition 138,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 137.0 in stage 6.0 (TID 339)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 136.0 in stage 6.0 (TID 338) in 5 ms on localhost (137/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 137.0 in stage 6.0 (TID 339). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 138.0 in stage 6.0 (TID 340, localhost, partition 139,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 138.0 in stage 6.0 (TID 340)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 137.0 in stage 6.0 (TID 339) in 5 ms on localhost (138/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 138.0 in stage 6.0 (TID 340). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 139.0 in stage 6.0 (TID 341, localhost, partition 140,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 139.0 in stage 6.0 (TID 341)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 138.0 in stage 6.0 (TID 340) in 5 ms on localhost (139/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 139.0 in stage 6.0 (TID 341). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 140.0 in stage 6.0 (TID 342, localhost, partition 141,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 140.0 in stage 6.0 (TID 342)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 139.0 in stage 6.0 (TID 341) in 6 ms on localhost (140/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 140.0 in stage 6.0 (TID 342). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 141.0 in stage 6.0 (TID 343, localhost, partition 142,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 141.0 in stage 6.0 (TID 343)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 140.0 in stage 6.0 (TID 342) in 5 ms on localhost (141/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 141.0 in stage 6.0 (TID 343). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 142.0 in stage 6.0 (TID 344, localhost, partition 143,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 142.0 in stage 6.0 (TID 344)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 141.0 in stage 6.0 (TID 343) in 5 ms on localhost (142/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 142.0 in stage 6.0 (TID 344). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 143.0 in stage 6.0 (TID 345, localhost, partition 144,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 143.0 in stage 6.0 (TID 345)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 142.0 in stage 6.0 (TID 344) in 4 ms on localhost (143/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 143.0 in stage 6.0 (TID 345). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 144.0 in stage 6.0 (TID 346, localhost, partition 145,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 144.0 in stage 6.0 (TID 346)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 143.0 in stage 6.0 (TID 345) in 5 ms on localhost (144/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 144.0 in stage 6.0 (TID 346). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 145.0 in stage 6.0 (TID 347, localhost, partition 146,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 145.0 in stage 6.0 (TID 347)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 144.0 in stage 6.0 (TID 346) in 5 ms on localhost (145/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 145.0 in stage 6.0 (TID 347). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 146.0 in stage 6.0 (TID 348, localhost, partition 147,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 146.0 in stage 6.0 (TID 348)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 145.0 in stage 6.0 (TID 347) in 4 ms on localhost (146/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 146.0 in stage 6.0 (TID 348). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 147.0 in stage 6.0 (TID 349, localhost, partition 148,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 147.0 in stage 6.0 (TID 349)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 146.0 in stage 6.0 (TID 348) in 4 ms on localhost (147/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 147.0 in stage 6.0 (TID 349). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 148.0 in stage 6.0 (TID 350, localhost, partition 149,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 148.0 in stage 6.0 (TID 350)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 147.0 in stage 6.0 (TID 349) in 4 ms on localhost (148/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 148.0 in stage 6.0 (TID 350). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 149.0 in stage 6.0 (TID 351, localhost, partition 150,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 149.0 in stage 6.0 (TID 351)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 148.0 in stage 6.0 (TID 350) in 5 ms on localhost (149/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 149.0 in stage 6.0 (TID 351). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 150.0 in stage 6.0 (TID 352, localhost, partition 151,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 150.0 in stage 6.0 (TID 352)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 149.0 in stage 6.0 (TID 351) in 4 ms on localhost (150/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 150.0 in stage 6.0 (TID 352). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 151.0 in stage 6.0 (TID 353, localhost, partition 152,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 151.0 in stage 6.0 (TID 353)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 150.0 in stage 6.0 (TID 352) in 4 ms on localhost (151/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 151.0 in stage 6.0 (TID 353). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 152.0 in stage 6.0 (TID 354, localhost, partition 153,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 152.0 in stage 6.0 (TID 354)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 151.0 in stage 6.0 (TID 353) in 4 ms on localhost (152/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 152.0 in stage 6.0 (TID 354). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 153.0 in stage 6.0 (TID 355, localhost, partition 154,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 153.0 in stage 6.0 (TID 355)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 152.0 in stage 6.0 (TID 354) in 5 ms on localhost (153/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 153.0 in stage 6.0 (TID 355). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 154.0 in stage 6.0 (TID 356, localhost, partition 155,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 154.0 in stage 6.0 (TID 356)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 153.0 in stage 6.0 (TID 355) in 5 ms on localhost (154/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 154.0 in stage 6.0 (TID 356). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 155.0 in stage 6.0 (TID 357, localhost, partition 156,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 155.0 in stage 6.0 (TID 357)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 154.0 in stage 6.0 (TID 356) in 4 ms on localhost (155/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 155.0 in stage 6.0 (TID 357). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 156.0 in stage 6.0 (TID 358, localhost, partition 157,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 156.0 in stage 6.0 (TID 358)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 155.0 in stage 6.0 (TID 357) in 4 ms on localhost (156/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 156.0 in stage 6.0 (TID 358). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 157.0 in stage 6.0 (TID 359, localhost, partition 158,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 157.0 in stage 6.0 (TID 359)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 156.0 in stage 6.0 (TID 358) in 4 ms on localhost (157/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 157.0 in stage 6.0 (TID 359). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 158.0 in stage 6.0 (TID 360, localhost, partition 159,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 158.0 in stage 6.0 (TID 360)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 157.0 in stage 6.0 (TID 359) in 4 ms on localhost (158/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 158.0 in stage 6.0 (TID 360). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 159.0 in stage 6.0 (TID 361, localhost, partition 160,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 159.0 in stage 6.0 (TID 361)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 158.0 in stage 6.0 (TID 360) in 4 ms on localhost (159/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 159.0 in stage 6.0 (TID 361). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 160.0 in stage 6.0 (TID 362, localhost, partition 161,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 160.0 in stage 6.0 (TID 362)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 159.0 in stage 6.0 (TID 361) in 4 ms on localhost (160/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 160.0 in stage 6.0 (TID 362). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 161.0 in stage 6.0 (TID 363, localhost, partition 162,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 161.0 in stage 6.0 (TID 363)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 160.0 in stage 6.0 (TID 362) in 4 ms on localhost (161/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 161.0 in stage 6.0 (TID 363). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 162.0 in stage 6.0 (TID 364, localhost, partition 163,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 162.0 in stage 6.0 (TID 364)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 161.0 in stage 6.0 (TID 363) in 5 ms on localhost (162/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 162.0 in stage 6.0 (TID 364). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 163.0 in stage 6.0 (TID 365, localhost, partition 164,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 163.0 in stage 6.0 (TID 365)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 162.0 in stage 6.0 (TID 364) in 5 ms on localhost (163/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 163.0 in stage 6.0 (TID 365). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 164.0 in stage 6.0 (TID 366, localhost, partition 165,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 164.0 in stage 6.0 (TID 366)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 163.0 in stage 6.0 (TID 365) in 4 ms on localhost (164/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 164.0 in stage 6.0 (TID 366). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 165.0 in stage 6.0 (TID 367, localhost, partition 166,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 165.0 in stage 6.0 (TID 367)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 164.0 in stage 6.0 (TID 366) in 5 ms on localhost (165/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 165.0 in stage 6.0 (TID 367). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 166.0 in stage 6.0 (TID 368, localhost, partition 167,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 166.0 in stage 6.0 (TID 368)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 165.0 in stage 6.0 (TID 367) in 7 ms on localhost (166/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 166.0 in stage 6.0 (TID 368). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 167.0 in stage 6.0 (TID 369, localhost, partition 168,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 167.0 in stage 6.0 (TID 369)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 166.0 in stage 6.0 (TID 368) in 6 ms on localhost (167/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 167.0 in stage 6.0 (TID 369). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 168.0 in stage 6.0 (TID 370, localhost, partition 169,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 168.0 in stage 6.0 (TID 370)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 167.0 in stage 6.0 (TID 369) in 5 ms on localhost (168/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 168.0 in stage 6.0 (TID 370). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 169.0 in stage 6.0 (TID 371, localhost, partition 170,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 169.0 in stage 6.0 (TID 371)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 168.0 in stage 6.0 (TID 370) in 4 ms on localhost (169/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 169.0 in stage 6.0 (TID 371). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 170.0 in stage 6.0 (TID 372, localhost, partition 171,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 170.0 in stage 6.0 (TID 372)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 169.0 in stage 6.0 (TID 371) in 4 ms on localhost (170/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 170.0 in stage 6.0 (TID 372). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 171.0 in stage 6.0 (TID 373, localhost, partition 172,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 171.0 in stage 6.0 (TID 373)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 170.0 in stage 6.0 (TID 372) in 4 ms on localhost (171/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 171.0 in stage 6.0 (TID 373). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 172.0 in stage 6.0 (TID 374, localhost, partition 173,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 172.0 in stage 6.0 (TID 374)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 171.0 in stage 6.0 (TID 373) in 4 ms on localhost (172/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 172.0 in stage 6.0 (TID 374). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 173.0 in stage 6.0 (TID 375, localhost, partition 174,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 173.0 in stage 6.0 (TID 375)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 172.0 in stage 6.0 (TID 374) in 4 ms on localhost (173/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 173.0 in stage 6.0 (TID 375). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 174.0 in stage 6.0 (TID 376, localhost, partition 175,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 174.0 in stage 6.0 (TID 376)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 173.0 in stage 6.0 (TID 375) in 4 ms on localhost (174/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 174.0 in stage 6.0 (TID 376). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 175.0 in stage 6.0 (TID 377, localhost, partition 176,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 175.0 in stage 6.0 (TID 377)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 174.0 in stage 6.0 (TID 376) in 4 ms on localhost (175/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 175.0 in stage 6.0 (TID 377). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 176.0 in stage 6.0 (TID 378, localhost, partition 177,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 176.0 in stage 6.0 (TID 378)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 175.0 in stage 6.0 (TID 377) in 4 ms on localhost (176/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 176.0 in stage 6.0 (TID 378). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 177.0 in stage 6.0 (TID 379, localhost, partition 178,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 177.0 in stage 6.0 (TID 379)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 176.0 in stage 6.0 (TID 378) in 5 ms on localhost (177/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 177.0 in stage 6.0 (TID 379). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 178.0 in stage 6.0 (TID 380, localhost, partition 179,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 178.0 in stage 6.0 (TID 380)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 177.0 in stage 6.0 (TID 379) in 4 ms on localhost (178/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 178.0 in stage 6.0 (TID 380). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 179.0 in stage 6.0 (TID 381, localhost, partition 180,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 179.0 in stage 6.0 (TID 381)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 178.0 in stage 6.0 (TID 380) in 4 ms on localhost (179/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 179.0 in stage 6.0 (TID 381). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 180.0 in stage 6.0 (TID 382, localhost, partition 181,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 180.0 in stage 6.0 (TID 382)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 179.0 in stage 6.0 (TID 381) in 4 ms on localhost (180/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 180.0 in stage 6.0 (TID 382). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 181.0 in stage 6.0 (TID 383, localhost, partition 182,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 181.0 in stage 6.0 (TID 383)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 180.0 in stage 6.0 (TID 382) in 5 ms on localhost (181/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 181.0 in stage 6.0 (TID 383). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 182.0 in stage 6.0 (TID 384, localhost, partition 183,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 182.0 in stage 6.0 (TID 384)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 181.0 in stage 6.0 (TID 383) in 6 ms on localhost (182/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 182.0 in stage 6.0 (TID 384). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 183.0 in stage 6.0 (TID 385, localhost, partition 184,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 183.0 in stage 6.0 (TID 385)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 182.0 in stage 6.0 (TID 384) in 5 ms on localhost (183/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 183.0 in stage 6.0 (TID 385). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 184.0 in stage 6.0 (TID 386, localhost, partition 185,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 184.0 in stage 6.0 (TID 386)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 183.0 in stage 6.0 (TID 385) in 4 ms on localhost (184/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 184.0 in stage 6.0 (TID 386). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 185.0 in stage 6.0 (TID 387, localhost, partition 186,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 185.0 in stage 6.0 (TID 387)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 184.0 in stage 6.0 (TID 386) in 5 ms on localhost (185/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 185.0 in stage 6.0 (TID 387). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 186.0 in stage 6.0 (TID 388, localhost, partition 187,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 186.0 in stage 6.0 (TID 388)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 185.0 in stage 6.0 (TID 387) in 4 ms on localhost (186/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 186.0 in stage 6.0 (TID 388). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 187.0 in stage 6.0 (TID 389, localhost, partition 188,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 187.0 in stage 6.0 (TID 389)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 186.0 in stage 6.0 (TID 388) in 5 ms on localhost (187/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 187.0 in stage 6.0 (TID 389). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 188.0 in stage 6.0 (TID 390, localhost, partition 189,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 188.0 in stage 6.0 (TID 390)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 187.0 in stage 6.0 (TID 389) in 4 ms on localhost (188/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 188.0 in stage 6.0 (TID 390). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 189.0 in stage 6.0 (TID 391, localhost, partition 190,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 189.0 in stage 6.0 (TID 391)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 188.0 in stage 6.0 (TID 390) in 5 ms on localhost (189/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 189.0 in stage 6.0 (TID 391). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 190.0 in stage 6.0 (TID 392, localhost, partition 191,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 190.0 in stage 6.0 (TID 392)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 189.0 in stage 6.0 (TID 391) in 5 ms on localhost (190/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 190.0 in stage 6.0 (TID 392). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 191.0 in stage 6.0 (TID 393, localhost, partition 192,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 191.0 in stage 6.0 (TID 393)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 190.0 in stage 6.0 (TID 392) in 8 ms on localhost (191/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 191.0 in stage 6.0 (TID 393). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 192.0 in stage 6.0 (TID 394, localhost, partition 193,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 192.0 in stage 6.0 (TID 394)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 191.0 in stage 6.0 (TID 393) in 6 ms on localhost (192/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 192.0 in stage 6.0 (TID 394). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 193.0 in stage 6.0 (TID 395, localhost, partition 194,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 193.0 in stage 6.0 (TID 395)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 192.0 in stage 6.0 (TID 394) in 4 ms on localhost (193/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 193.0 in stage 6.0 (TID 395). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 194.0 in stage 6.0 (TID 396, localhost, partition 195,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 194.0 in stage 6.0 (TID 396)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 193.0 in stage 6.0 (TID 395) in 6 ms on localhost (194/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 194.0 in stage 6.0 (TID 396). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 195.0 in stage 6.0 (TID 397, localhost, partition 196,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 195.0 in stage 6.0 (TID 397)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 194.0 in stage 6.0 (TID 396) in 4 ms on localhost (195/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 195.0 in stage 6.0 (TID 397). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 196.0 in stage 6.0 (TID 398, localhost, partition 197,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 196.0 in stage 6.0 (TID 398)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 195.0 in stage 6.0 (TID 397) in 5 ms on localhost (196/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 196.0 in stage 6.0 (TID 398). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 197.0 in stage 6.0 (TID 399, localhost, partition 198,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 197.0 in stage 6.0 (TID 399)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 196.0 in stage 6.0 (TID 398) in 4 ms on localhost (197/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 197.0 in stage 6.0 (TID 399). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 198.0 in stage 6.0 (TID 400, localhost, partition 199,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 198.0 in stage 6.0 (TID 400)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 197.0 in stage 6.0 (TID 399) in 4 ms on localhost (198/199)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 198.0 in stage 6.0 (TID 400). 1652 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 198.0 in stage 6.0 (TID 400) in 4 ms on localhost (199/199)

2019-05-25 16:48:35
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 6 (show at AccessLogJob.scala:196) finished in 0.901 s

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 6.0, whose tasks have all completed, from pool 

2019-05-25 16:48:35
[INFO]-[Thread: streaming-job-executor-0]-[org.apache.spark.Logging$class.logInfo()]: Job 4 finished: show at AccessLogJob.scala:196, took 0.928142 s

2019-05-25 16:48:35
[INFO]-[Thread: streaming-job-executor-0]-[org.apache.spark.Logging$class.logInfo()]: Starting job: foreachPartition at AccessLogJob.scala:200

2019-05-25 16:48:35
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Registering RDD 32 (rdd at AccessLogJob.scala:198)

2019-05-25 16:48:35
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 5 (foreachPartition at AccessLogJob.scala:200) with 200 output partitions

2019-05-25 16:48:35
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 8 (foreachPartition at AccessLogJob.scala:200)

2019-05-25 16:48:35
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List(ShuffleMapStage 7)

2019-05-25 16:48:35
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:35
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 8 (MapPartitionsRDD[37] at rdd at AccessLogJob.scala:198), which has no missing parents

2019-05-25 16:48:35
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_4 stored as values in memory (estimated size 19.4 KB, free 2.4 GB)

2019-05-25 16:48:35
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.8 KB, free 2.4 GB)

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_4_piece0 in memory on localhost:60954 (size: 8.8 KB, free: 2.4 GB)

2019-05-25 16:48:35
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 4 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:35
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 200 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at rdd at AccessLogJob.scala:198)

2019-05-25 16:48:35
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 8.0 with 200 tasks

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 8.0 (TID 401, localhost, partition 0,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 8.0 (TID 401)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 0.0 in stage 8.0 (TID 401). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 1.0 in stage 8.0 (TID 402, localhost, partition 1,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 1.0 in stage 8.0 (TID 402)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 0.0 in stage 8.0 (TID 401) in 6 ms on localhost (1/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 1.0 in stage 8.0 (TID 402). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 2.0 in stage 8.0 (TID 403, localhost, partition 2,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 2.0 in stage 8.0 (TID 403)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 1.0 in stage 8.0 (TID 402) in 6 ms on localhost (2/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 2.0 in stage 8.0 (TID 403). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 3.0 in stage 8.0 (TID 404, localhost, partition 3,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 3.0 in stage 8.0 (TID 404)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 2.0 in stage 8.0 (TID 403) in 5 ms on localhost (3/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 3.0 in stage 8.0 (TID 404). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 4.0 in stage 8.0 (TID 405, localhost, partition 4,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 4.0 in stage 8.0 (TID 405)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 3.0 in stage 8.0 (TID 404) in 5 ms on localhost (4/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 4.0 in stage 8.0 (TID 405). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 5.0 in stage 8.0 (TID 406, localhost, partition 5,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 5.0 in stage 8.0 (TID 406)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 4.0 in stage 8.0 (TID 405) in 4 ms on localhost (5/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 5.0 in stage 8.0 (TID 406). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 6.0 in stage 8.0 (TID 407, localhost, partition 6,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 6.0 in stage 8.0 (TID 407)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 5.0 in stage 8.0 (TID 406) in 4 ms on localhost (6/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 6.0 in stage 8.0 (TID 407). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 7.0 in stage 8.0 (TID 408, localhost, partition 7,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 7.0 in stage 8.0 (TID 408)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 6.0 in stage 8.0 (TID 407) in 4 ms on localhost (7/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 7.0 in stage 8.0 (TID 408). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 8.0 in stage 8.0 (TID 409, localhost, partition 8,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 8.0 in stage 8.0 (TID 409)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 7.0 in stage 8.0 (TID 408) in 5 ms on localhost (8/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 8.0 in stage 8.0 (TID 409). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 9.0 in stage 8.0 (TID 410, localhost, partition 9,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 9.0 in stage 8.0 (TID 410)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 8.0 in stage 8.0 (TID 409) in 4 ms on localhost (9/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 9.0 in stage 8.0 (TID 410). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 10.0 in stage 8.0 (TID 411, localhost, partition 10,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 10.0 in stage 8.0 (TID 411)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 9.0 in stage 8.0 (TID 410) in 4 ms on localhost (10/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 10.0 in stage 8.0 (TID 411). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 11.0 in stage 8.0 (TID 412, localhost, partition 11,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 11.0 in stage 8.0 (TID 412)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 10.0 in stage 8.0 (TID 411) in 4 ms on localhost (11/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 11.0 in stage 8.0 (TID 412). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 12.0 in stage 8.0 (TID 413, localhost, partition 12,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 12.0 in stage 8.0 (TID 413)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 11.0 in stage 8.0 (TID 412) in 5 ms on localhost (12/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 12.0 in stage 8.0 (TID 413). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 13.0 in stage 8.0 (TID 414, localhost, partition 13,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 13.0 in stage 8.0 (TID 414)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 12.0 in stage 8.0 (TID 413) in 5 ms on localhost (13/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 13.0 in stage 8.0 (TID 414). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 14.0 in stage 8.0 (TID 415, localhost, partition 14,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 14.0 in stage 8.0 (TID 415)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 13.0 in stage 8.0 (TID 414) in 5 ms on localhost (14/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 14.0 in stage 8.0 (TID 415). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 15.0 in stage 8.0 (TID 416, localhost, partition 15,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 15.0 in stage 8.0 (TID 416)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 14.0 in stage 8.0 (TID 415) in 5 ms on localhost (15/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 15.0 in stage 8.0 (TID 416). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 16.0 in stage 8.0 (TID 417, localhost, partition 16,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 16.0 in stage 8.0 (TID 417)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 15.0 in stage 8.0 (TID 416) in 6 ms on localhost (16/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 16.0 in stage 8.0 (TID 417). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 17.0 in stage 8.0 (TID 418, localhost, partition 17,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 17.0 in stage 8.0 (TID 418)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 16.0 in stage 8.0 (TID 417) in 5 ms on localhost (17/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 17.0 in stage 8.0 (TID 418). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 18.0 in stage 8.0 (TID 419, localhost, partition 18,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 18.0 in stage 8.0 (TID 419)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 17.0 in stage 8.0 (TID 418) in 4 ms on localhost (18/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 18.0 in stage 8.0 (TID 419). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 19.0 in stage 8.0 (TID 420, localhost, partition 19,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 19.0 in stage 8.0 (TID 420)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 18.0 in stage 8.0 (TID 419) in 5 ms on localhost (19/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 19.0 in stage 8.0 (TID 420). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 20.0 in stage 8.0 (TID 421, localhost, partition 20,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 20.0 in stage 8.0 (TID 421)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 19.0 in stage 8.0 (TID 420) in 5 ms on localhost (20/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 20.0 in stage 8.0 (TID 421). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 21.0 in stage 8.0 (TID 422, localhost, partition 21,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 21.0 in stage 8.0 (TID 422)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 20.0 in stage 8.0 (TID 421) in 5 ms on localhost (21/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 21.0 in stage 8.0 (TID 422). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 22.0 in stage 8.0 (TID 423, localhost, partition 22,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 22.0 in stage 8.0 (TID 423)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 21.0 in stage 8.0 (TID 422) in 5 ms on localhost (22/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 22.0 in stage 8.0 (TID 423). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 23.0 in stage 8.0 (TID 424, localhost, partition 23,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 23.0 in stage 8.0 (TID 424)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 22.0 in stage 8.0 (TID 423) in 3 ms on localhost (23/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 23.0 in stage 8.0 (TID 424). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 24.0 in stage 8.0 (TID 425, localhost, partition 24,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 24.0 in stage 8.0 (TID 425)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 23.0 in stage 8.0 (TID 424) in 4 ms on localhost (24/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 24.0 in stage 8.0 (TID 425). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 25.0 in stage 8.0 (TID 426, localhost, partition 25,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 25.0 in stage 8.0 (TID 426)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 24.0 in stage 8.0 (TID 425) in 5 ms on localhost (25/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 25.0 in stage 8.0 (TID 426). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 26.0 in stage 8.0 (TID 427, localhost, partition 26,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 26.0 in stage 8.0 (TID 427)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 25.0 in stage 8.0 (TID 426) in 6 ms on localhost (26/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 26.0 in stage 8.0 (TID 427). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 27.0 in stage 8.0 (TID 428, localhost, partition 27,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 27.0 in stage 8.0 (TID 428)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 26.0 in stage 8.0 (TID 427) in 4 ms on localhost (27/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 27.0 in stage 8.0 (TID 428). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 28.0 in stage 8.0 (TID 429, localhost, partition 28,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 28.0 in stage 8.0 (TID 429)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 27.0 in stage 8.0 (TID 428) in 4 ms on localhost (28/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 28.0 in stage 8.0 (TID 429). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 29.0 in stage 8.0 (TID 430, localhost, partition 29,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 29.0 in stage 8.0 (TID 430)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 28.0 in stage 8.0 (TID 429) in 4 ms on localhost (29/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 29.0 in stage 8.0 (TID 430). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 30.0 in stage 8.0 (TID 431, localhost, partition 30,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 30.0 in stage 8.0 (TID 431)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 29.0 in stage 8.0 (TID 430) in 3 ms on localhost (30/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 30.0 in stage 8.0 (TID 431). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 31.0 in stage 8.0 (TID 432, localhost, partition 31,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 31.0 in stage 8.0 (TID 432)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 30.0 in stage 8.0 (TID 431) in 4 ms on localhost (31/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 31.0 in stage 8.0 (TID 432). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 32.0 in stage 8.0 (TID 433, localhost, partition 32,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 32.0 in stage 8.0 (TID 433)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 31.0 in stage 8.0 (TID 432) in 4 ms on localhost (32/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 32.0 in stage 8.0 (TID 433). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 33.0 in stage 8.0 (TID 434, localhost, partition 33,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 33.0 in stage 8.0 (TID 434)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 32.0 in stage 8.0 (TID 433) in 3 ms on localhost (33/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 33.0 in stage 8.0 (TID 434). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 34.0 in stage 8.0 (TID 435, localhost, partition 34,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 34.0 in stage 8.0 (TID 435)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 33.0 in stage 8.0 (TID 434) in 4 ms on localhost (34/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 34.0 in stage 8.0 (TID 435). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 35.0 in stage 8.0 (TID 436, localhost, partition 35,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 35.0 in stage 8.0 (TID 436)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 34.0 in stage 8.0 (TID 435) in 4 ms on localhost (35/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 35.0 in stage 8.0 (TID 436). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 36.0 in stage 8.0 (TID 437, localhost, partition 36,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 36.0 in stage 8.0 (TID 437)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 35.0 in stage 8.0 (TID 436) in 5 ms on localhost (36/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 36.0 in stage 8.0 (TID 437). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 37.0 in stage 8.0 (TID 438, localhost, partition 37,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 37.0 in stage 8.0 (TID 438)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 36.0 in stage 8.0 (TID 437) in 6 ms on localhost (37/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 37.0 in stage 8.0 (TID 438). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 38.0 in stage 8.0 (TID 439, localhost, partition 38,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 38.0 in stage 8.0 (TID 439)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 37.0 in stage 8.0 (TID 438) in 5 ms on localhost (38/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 38.0 in stage 8.0 (TID 439). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 39.0 in stage 8.0 (TID 440, localhost, partition 39,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 39.0 in stage 8.0 (TID 440)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 38.0 in stage 8.0 (TID 439) in 4 ms on localhost (39/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 39.0 in stage 8.0 (TID 440). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 40.0 in stage 8.0 (TID 441, localhost, partition 40,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 40.0 in stage 8.0 (TID 441)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 39.0 in stage 8.0 (TID 440) in 5 ms on localhost (40/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 40.0 in stage 8.0 (TID 441). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 41.0 in stage 8.0 (TID 442, localhost, partition 41,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 41.0 in stage 8.0 (TID 442)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 40.0 in stage 8.0 (TID 441) in 4 ms on localhost (41/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 41.0 in stage 8.0 (TID 442). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 42.0 in stage 8.0 (TID 443, localhost, partition 42,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 42.0 in stage 8.0 (TID 443)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 41.0 in stage 8.0 (TID 442) in 4 ms on localhost (42/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 42.0 in stage 8.0 (TID 443). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 43.0 in stage 8.0 (TID 444, localhost, partition 43,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 43.0 in stage 8.0 (TID 444)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 42.0 in stage 8.0 (TID 443) in 5 ms on localhost (43/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 43.0 in stage 8.0 (TID 444). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 44.0 in stage 8.0 (TID 445, localhost, partition 44,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 44.0 in stage 8.0 (TID 445)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 43.0 in stage 8.0 (TID 444) in 4 ms on localhost (44/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 44.0 in stage 8.0 (TID 445). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 45.0 in stage 8.0 (TID 446, localhost, partition 45,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 45.0 in stage 8.0 (TID 446)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 44.0 in stage 8.0 (TID 445) in 4 ms on localhost (45/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 45.0 in stage 8.0 (TID 446). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 46.0 in stage 8.0 (TID 447, localhost, partition 46,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 46.0 in stage 8.0 (TID 447)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 45.0 in stage 8.0 (TID 446) in 4 ms on localhost (46/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 46.0 in stage 8.0 (TID 447). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 47.0 in stage 8.0 (TID 448, localhost, partition 47,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 47.0 in stage 8.0 (TID 448)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 46.0 in stage 8.0 (TID 447) in 4 ms on localhost (47/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 47.0 in stage 8.0 (TID 448). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 48.0 in stage 8.0 (TID 449, localhost, partition 48,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 48.0 in stage 8.0 (TID 449)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 47.0 in stage 8.0 (TID 448) in 5 ms on localhost (48/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 48.0 in stage 8.0 (TID 449). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 49.0 in stage 8.0 (TID 450, localhost, partition 49,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 49.0 in stage 8.0 (TID 450)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 48.0 in stage 8.0 (TID 449) in 6 ms on localhost (49/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 49.0 in stage 8.0 (TID 450). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 50.0 in stage 8.0 (TID 451, localhost, partition 50,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 50.0 in stage 8.0 (TID 451)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 49.0 in stage 8.0 (TID 450) in 5 ms on localhost (50/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 50.0 in stage 8.0 (TID 451). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 51.0 in stage 8.0 (TID 452, localhost, partition 51,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 51.0 in stage 8.0 (TID 452)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 50.0 in stage 8.0 (TID 451) in 4 ms on localhost (51/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 51.0 in stage 8.0 (TID 452). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 52.0 in stage 8.0 (TID 453, localhost, partition 52,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 52.0 in stage 8.0 (TID 453)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 51.0 in stage 8.0 (TID 452) in 5 ms on localhost (52/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 52.0 in stage 8.0 (TID 453). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 53.0 in stage 8.0 (TID 454, localhost, partition 53,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 53.0 in stage 8.0 (TID 454)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 52.0 in stage 8.0 (TID 453) in 6 ms on localhost (53/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 53.0 in stage 8.0 (TID 454). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 54.0 in stage 8.0 (TID 455, localhost, partition 54,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 54.0 in stage 8.0 (TID 455)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 53.0 in stage 8.0 (TID 454) in 7 ms on localhost (54/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 54.0 in stage 8.0 (TID 455). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 55.0 in stage 8.0 (TID 456, localhost, partition 55,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 55.0 in stage 8.0 (TID 456)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 54.0 in stage 8.0 (TID 455) in 5 ms on localhost (55/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 55.0 in stage 8.0 (TID 456). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 56.0 in stage 8.0 (TID 457, localhost, partition 56,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 56.0 in stage 8.0 (TID 457)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 55.0 in stage 8.0 (TID 456) in 5 ms on localhost (56/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 56.0 in stage 8.0 (TID 457). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 57.0 in stage 8.0 (TID 458, localhost, partition 57,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 57.0 in stage 8.0 (TID 458)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 56.0 in stage 8.0 (TID 457) in 4 ms on localhost (57/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 57.0 in stage 8.0 (TID 458). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 58.0 in stage 8.0 (TID 459, localhost, partition 58,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 58.0 in stage 8.0 (TID 459)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 57.0 in stage 8.0 (TID 458) in 4 ms on localhost (58/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 58.0 in stage 8.0 (TID 459). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 59.0 in stage 8.0 (TID 460, localhost, partition 59,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 59.0 in stage 8.0 (TID 460)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 58.0 in stage 8.0 (TID 459) in 4 ms on localhost (59/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 59.0 in stage 8.0 (TID 460). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 60.0 in stage 8.0 (TID 461, localhost, partition 60,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 60.0 in stage 8.0 (TID 461)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 59.0 in stage 8.0 (TID 460) in 4 ms on localhost (60/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 60.0 in stage 8.0 (TID 461). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 61.0 in stage 8.0 (TID 462, localhost, partition 61,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 61.0 in stage 8.0 (TID 462)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 60.0 in stage 8.0 (TID 461) in 4 ms on localhost (61/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 61.0 in stage 8.0 (TID 462). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 62.0 in stage 8.0 (TID 463, localhost, partition 62,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 62.0 in stage 8.0 (TID 463)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 61.0 in stage 8.0 (TID 462) in 4 ms on localhost (62/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 62.0 in stage 8.0 (TID 463). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 63.0 in stage 8.0 (TID 464, localhost, partition 63,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 63.0 in stage 8.0 (TID 464)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 62.0 in stage 8.0 (TID 463) in 4 ms on localhost (63/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 63.0 in stage 8.0 (TID 464). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 64.0 in stage 8.0 (TID 465, localhost, partition 64,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 64.0 in stage 8.0 (TID 465)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 63.0 in stage 8.0 (TID 464) in 4 ms on localhost (64/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 64.0 in stage 8.0 (TID 465). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 65.0 in stage 8.0 (TID 466, localhost, partition 65,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 65.0 in stage 8.0 (TID 466)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 64.0 in stage 8.0 (TID 465) in 4 ms on localhost (65/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 65.0 in stage 8.0 (TID 466). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 66.0 in stage 8.0 (TID 467, localhost, partition 66,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 66.0 in stage 8.0 (TID 467)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 65.0 in stage 8.0 (TID 466) in 6 ms on localhost (66/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 66.0 in stage 8.0 (TID 467). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 67.0 in stage 8.0 (TID 468, localhost, partition 67,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 67.0 in stage 8.0 (TID 468)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 66.0 in stage 8.0 (TID 467) in 3 ms on localhost (67/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 67.0 in stage 8.0 (TID 468). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 68.0 in stage 8.0 (TID 469, localhost, partition 68,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 68.0 in stage 8.0 (TID 469)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 67.0 in stage 8.0 (TID 468) in 5 ms on localhost (68/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 68.0 in stage 8.0 (TID 469). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 69.0 in stage 8.0 (TID 470, localhost, partition 69,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 69.0 in stage 8.0 (TID 470)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 68.0 in stage 8.0 (TID 469) in 3 ms on localhost (69/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 69.0 in stage 8.0 (TID 470). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 70.0 in stage 8.0 (TID 471, localhost, partition 70,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 70.0 in stage 8.0 (TID 471)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 69.0 in stage 8.0 (TID 470) in 4 ms on localhost (70/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 70.0 in stage 8.0 (TID 471). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 71.0 in stage 8.0 (TID 472, localhost, partition 71,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 71.0 in stage 8.0 (TID 472)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 70.0 in stage 8.0 (TID 471) in 4 ms on localhost (71/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 71.0 in stage 8.0 (TID 472). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 72.0 in stage 8.0 (TID 473, localhost, partition 72,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 72.0 in stage 8.0 (TID 473)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 71.0 in stage 8.0 (TID 472) in 5 ms on localhost (72/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 72.0 in stage 8.0 (TID 473). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 73.0 in stage 8.0 (TID 474, localhost, partition 73,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 73.0 in stage 8.0 (TID 474)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 72.0 in stage 8.0 (TID 473) in 5 ms on localhost (73/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 73.0 in stage 8.0 (TID 474). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 74.0 in stage 8.0 (TID 475, localhost, partition 74,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 74.0 in stage 8.0 (TID 475)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 73.0 in stage 8.0 (TID 474) in 4 ms on localhost (74/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 74.0 in stage 8.0 (TID 475). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 75.0 in stage 8.0 (TID 476, localhost, partition 75,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 75.0 in stage 8.0 (TID 476)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 74.0 in stage 8.0 (TID 475) in 3 ms on localhost (75/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 75.0 in stage 8.0 (TID 476). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 76.0 in stage 8.0 (TID 477, localhost, partition 76,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 76.0 in stage 8.0 (TID 477)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 75.0 in stage 8.0 (TID 476) in 5 ms on localhost (76/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 76.0 in stage 8.0 (TID 477). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 77.0 in stage 8.0 (TID 478, localhost, partition 77,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 77.0 in stage 8.0 (TID 478)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 76.0 in stage 8.0 (TID 477) in 4 ms on localhost (77/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 77.0 in stage 8.0 (TID 478). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 78.0 in stage 8.0 (TID 479, localhost, partition 78,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 78.0 in stage 8.0 (TID 479)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 77.0 in stage 8.0 (TID 478) in 4 ms on localhost (78/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 78.0 in stage 8.0 (TID 479). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 79.0 in stage 8.0 (TID 480, localhost, partition 79,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 79.0 in stage 8.0 (TID 480)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 78.0 in stage 8.0 (TID 479) in 4 ms on localhost (79/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 79.0 in stage 8.0 (TID 480). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 80.0 in stage 8.0 (TID 481, localhost, partition 80,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 80.0 in stage 8.0 (TID 481)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 79.0 in stage 8.0 (TID 480) in 3 ms on localhost (80/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 80.0 in stage 8.0 (TID 481). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 81.0 in stage 8.0 (TID 482, localhost, partition 81,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 81.0 in stage 8.0 (TID 482)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 80.0 in stage 8.0 (TID 481) in 4 ms on localhost (81/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 81.0 in stage 8.0 (TID 482). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 82.0 in stage 8.0 (TID 483, localhost, partition 82,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 82.0 in stage 8.0 (TID 483)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 81.0 in stage 8.0 (TID 482) in 4 ms on localhost (82/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 82.0 in stage 8.0 (TID 483). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 83.0 in stage 8.0 (TID 484, localhost, partition 83,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 83.0 in stage 8.0 (TID 484)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 82.0 in stage 8.0 (TID 483) in 4 ms on localhost (83/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 83.0 in stage 8.0 (TID 484). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 84.0 in stage 8.0 (TID 485, localhost, partition 84,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 84.0 in stage 8.0 (TID 485)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 83.0 in stage 8.0 (TID 484) in 5 ms on localhost (84/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 84.0 in stage 8.0 (TID 485). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 85.0 in stage 8.0 (TID 486, localhost, partition 85,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 85.0 in stage 8.0 (TID 486)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 84.0 in stage 8.0 (TID 485) in 7 ms on localhost (85/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 85.0 in stage 8.0 (TID 486). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 86.0 in stage 8.0 (TID 487, localhost, partition 86,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 86.0 in stage 8.0 (TID 487)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 85.0 in stage 8.0 (TID 486) in 5 ms on localhost (86/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 86.0 in stage 8.0 (TID 487). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 87.0 in stage 8.0 (TID 488, localhost, partition 87,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 87.0 in stage 8.0 (TID 488)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 86.0 in stage 8.0 (TID 487) in 5 ms on localhost (87/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 87.0 in stage 8.0 (TID 488). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 88.0 in stage 8.0 (TID 489, localhost, partition 88,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 88.0 in stage 8.0 (TID 489)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 87.0 in stage 8.0 (TID 488) in 5 ms on localhost (88/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 88.0 in stage 8.0 (TID 489). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 89.0 in stage 8.0 (TID 490, localhost, partition 89,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 89.0 in stage 8.0 (TID 490)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 88.0 in stage 8.0 (TID 489) in 4 ms on localhost (89/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 89.0 in stage 8.0 (TID 490). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 90.0 in stage 8.0 (TID 491, localhost, partition 90,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 90.0 in stage 8.0 (TID 491)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 89.0 in stage 8.0 (TID 490) in 5 ms on localhost (90/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 90.0 in stage 8.0 (TID 491). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 91.0 in stage 8.0 (TID 492, localhost, partition 91,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 91.0 in stage 8.0 (TID 492)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 90.0 in stage 8.0 (TID 491) in 5 ms on localhost (91/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 91.0 in stage 8.0 (TID 492). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 92.0 in stage 8.0 (TID 493, localhost, partition 92,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 92.0 in stage 8.0 (TID 493)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 91.0 in stage 8.0 (TID 492) in 5 ms on localhost (92/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 92.0 in stage 8.0 (TID 493). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 93.0 in stage 8.0 (TID 494, localhost, partition 93,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 93.0 in stage 8.0 (TID 494)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 92.0 in stage 8.0 (TID 493) in 5 ms on localhost (93/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 93.0 in stage 8.0 (TID 494). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 94.0 in stage 8.0 (TID 495, localhost, partition 94,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 94.0 in stage 8.0 (TID 495)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 93.0 in stage 8.0 (TID 494) in 5 ms on localhost (94/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 94.0 in stage 8.0 (TID 495). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 95.0 in stage 8.0 (TID 496, localhost, partition 95,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 95.0 in stage 8.0 (TID 496)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 94.0 in stage 8.0 (TID 495) in 5 ms on localhost (95/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 95.0 in stage 8.0 (TID 496). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 96.0 in stage 8.0 (TID 497, localhost, partition 96,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 96.0 in stage 8.0 (TID 497)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 95.0 in stage 8.0 (TID 496) in 4 ms on localhost (96/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 96.0 in stage 8.0 (TID 497). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 97.0 in stage 8.0 (TID 498, localhost, partition 97,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 97.0 in stage 8.0 (TID 498)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 96.0 in stage 8.0 (TID 497) in 4 ms on localhost (97/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 97.0 in stage 8.0 (TID 498). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 98.0 in stage 8.0 (TID 499, localhost, partition 98,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 98.0 in stage 8.0 (TID 499)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 97.0 in stage 8.0 (TID 498) in 4 ms on localhost (98/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 98.0 in stage 8.0 (TID 499). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 99.0 in stage 8.0 (TID 500, localhost, partition 99,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 99.0 in stage 8.0 (TID 500)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 98.0 in stage 8.0 (TID 499) in 4 ms on localhost (99/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 99.0 in stage 8.0 (TID 500). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 100.0 in stage 8.0 (TID 501, localhost, partition 100,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 100.0 in stage 8.0 (TID 501)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 99.0 in stage 8.0 (TID 500) in 4 ms on localhost (100/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 100.0 in stage 8.0 (TID 501). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 101.0 in stage 8.0 (TID 502, localhost, partition 101,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 101.0 in stage 8.0 (TID 502)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 100.0 in stage 8.0 (TID 501) in 4 ms on localhost (101/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 101.0 in stage 8.0 (TID 502). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 102.0 in stage 8.0 (TID 503, localhost, partition 102,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 102.0 in stage 8.0 (TID 503)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 101.0 in stage 8.0 (TID 502) in 4 ms on localhost (102/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 102.0 in stage 8.0 (TID 503). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 103.0 in stage 8.0 (TID 504, localhost, partition 103,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 103.0 in stage 8.0 (TID 504)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 102.0 in stage 8.0 (TID 503) in 4 ms on localhost (103/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 103.0 in stage 8.0 (TID 504). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 104.0 in stage 8.0 (TID 505, localhost, partition 104,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 104.0 in stage 8.0 (TID 505)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 103.0 in stage 8.0 (TID 504) in 4 ms on localhost (104/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 104.0 in stage 8.0 (TID 505). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 105.0 in stage 8.0 (TID 506, localhost, partition 105,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 105.0 in stage 8.0 (TID 506)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 104.0 in stage 8.0 (TID 505) in 4 ms on localhost (105/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 105.0 in stage 8.0 (TID 506). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 106.0 in stage 8.0 (TID 507, localhost, partition 106,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 106.0 in stage 8.0 (TID 507)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 105.0 in stage 8.0 (TID 506) in 4 ms on localhost (106/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 106.0 in stage 8.0 (TID 507). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 107.0 in stage 8.0 (TID 508, localhost, partition 107,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 107.0 in stage 8.0 (TID 508)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 106.0 in stage 8.0 (TID 507) in 4 ms on localhost (107/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 107.0 in stage 8.0 (TID 508). 1627 bytes result sent to driver

2019-05-25 16:48:35
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 108.0 in stage 8.0 (TID 509, localhost, partition 108,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 108.0 in stage 8.0 (TID 509)

2019-05-25 16:48:35
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 107.0 in stage 8.0 (TID 508) in 5 ms on localhost (108/200)

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:35
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 108.0 in stage 8.0 (TID 509). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 109.0 in stage 8.0 (TID 510, localhost, partition 109,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 109.0 in stage 8.0 (TID 510)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 108.0 in stage 8.0 (TID 509) in 5 ms on localhost (109/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 109.0 in stage 8.0 (TID 510). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 110.0 in stage 8.0 (TID 511, localhost, partition 110,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 110.0 in stage 8.0 (TID 511)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 109.0 in stage 8.0 (TID 510) in 4 ms on localhost (110/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 110.0 in stage 8.0 (TID 511). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 111.0 in stage 8.0 (TID 512, localhost, partition 111,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 111.0 in stage 8.0 (TID 512)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 110.0 in stage 8.0 (TID 511) in 4 ms on localhost (111/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 111.0 in stage 8.0 (TID 512). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 112.0 in stage 8.0 (TID 513, localhost, partition 112,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 112.0 in stage 8.0 (TID 513)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 111.0 in stage 8.0 (TID 512) in 4 ms on localhost (112/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 112.0 in stage 8.0 (TID 513). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 113.0 in stage 8.0 (TID 514, localhost, partition 113,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 113.0 in stage 8.0 (TID 514)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 112.0 in stage 8.0 (TID 513) in 4 ms on localhost (113/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 113.0 in stage 8.0 (TID 514). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 114.0 in stage 8.0 (TID 515, localhost, partition 114,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 114.0 in stage 8.0 (TID 515)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 113.0 in stage 8.0 (TID 514) in 4 ms on localhost (114/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 114.0 in stage 8.0 (TID 515). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 115.0 in stage 8.0 (TID 516, localhost, partition 115,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 115.0 in stage 8.0 (TID 516)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 114.0 in stage 8.0 (TID 515) in 4 ms on localhost (115/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 115.0 in stage 8.0 (TID 516). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 116.0 in stage 8.0 (TID 517, localhost, partition 116,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 116.0 in stage 8.0 (TID 517)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 115.0 in stage 8.0 (TID 516) in 4 ms on localhost (116/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 116.0 in stage 8.0 (TID 517). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 117.0 in stage 8.0 (TID 518, localhost, partition 117,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 117.0 in stage 8.0 (TID 518)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 116.0 in stage 8.0 (TID 517) in 4 ms on localhost (117/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 117.0 in stage 8.0 (TID 518). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 118.0 in stage 8.0 (TID 519, localhost, partition 118,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 118.0 in stage 8.0 (TID 519)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 117.0 in stage 8.0 (TID 518) in 4 ms on localhost (118/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 118.0 in stage 8.0 (TID 519). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 119.0 in stage 8.0 (TID 520, localhost, partition 119,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 119.0 in stage 8.0 (TID 520)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 118.0 in stage 8.0 (TID 519) in 4 ms on localhost (119/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 119.0 in stage 8.0 (TID 520). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 120.0 in stage 8.0 (TID 521, localhost, partition 120,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 120.0 in stage 8.0 (TID 521)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 119.0 in stage 8.0 (TID 520) in 5 ms on localhost (120/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 120.0 in stage 8.0 (TID 521). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 121.0 in stage 8.0 (TID 522, localhost, partition 121,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 121.0 in stage 8.0 (TID 522)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 120.0 in stage 8.0 (TID 521) in 4 ms on localhost (121/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 121.0 in stage 8.0 (TID 522). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 122.0 in stage 8.0 (TID 523, localhost, partition 122,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 122.0 in stage 8.0 (TID 523)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 121.0 in stage 8.0 (TID 522) in 4 ms on localhost (122/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 122.0 in stage 8.0 (TID 523). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 123.0 in stage 8.0 (TID 524, localhost, partition 123,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 123.0 in stage 8.0 (TID 524)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 122.0 in stage 8.0 (TID 523) in 5 ms on localhost (123/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 123.0 in stage 8.0 (TID 524). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 124.0 in stage 8.0 (TID 525, localhost, partition 124,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 124.0 in stage 8.0 (TID 525)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 123.0 in stage 8.0 (TID 524) in 4 ms on localhost (124/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 124.0 in stage 8.0 (TID 525). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 125.0 in stage 8.0 (TID 526, localhost, partition 125,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 125.0 in stage 8.0 (TID 526)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 124.0 in stage 8.0 (TID 525) in 4 ms on localhost (125/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 125.0 in stage 8.0 (TID 526). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 126.0 in stage 8.0 (TID 527, localhost, partition 126,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 126.0 in stage 8.0 (TID 527)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 125.0 in stage 8.0 (TID 526) in 4 ms on localhost (126/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 126.0 in stage 8.0 (TID 527). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 127.0 in stage 8.0 (TID 528, localhost, partition 127,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 127.0 in stage 8.0 (TID 528)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 126.0 in stage 8.0 (TID 527) in 5 ms on localhost (127/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 127.0 in stage 8.0 (TID 528). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 128.0 in stage 8.0 (TID 529, localhost, partition 128,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 128.0 in stage 8.0 (TID 529)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 127.0 in stage 8.0 (TID 528) in 5 ms on localhost (128/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 128.0 in stage 8.0 (TID 529). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 129.0 in stage 8.0 (TID 530, localhost, partition 129,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 129.0 in stage 8.0 (TID 530)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 128.0 in stage 8.0 (TID 529) in 5 ms on localhost (129/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 129.0 in stage 8.0 (TID 530). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 130.0 in stage 8.0 (TID 531, localhost, partition 130,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 130.0 in stage 8.0 (TID 531)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 129.0 in stage 8.0 (TID 530) in 5 ms on localhost (130/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 130.0 in stage 8.0 (TID 531). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 131.0 in stage 8.0 (TID 532, localhost, partition 131,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 131.0 in stage 8.0 (TID 532)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 130.0 in stage 8.0 (TID 531) in 5 ms on localhost (131/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 131.0 in stage 8.0 (TID 532). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 132.0 in stage 8.0 (TID 533, localhost, partition 132,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 132.0 in stage 8.0 (TID 533)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 131.0 in stage 8.0 (TID 532) in 4 ms on localhost (132/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 132.0 in stage 8.0 (TID 533). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 133.0 in stage 8.0 (TID 534, localhost, partition 133,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 133.0 in stage 8.0 (TID 534)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 132.0 in stage 8.0 (TID 533) in 4 ms on localhost (133/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 133.0 in stage 8.0 (TID 534). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 134.0 in stage 8.0 (TID 535, localhost, partition 134,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 134.0 in stage 8.0 (TID 535)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 133.0 in stage 8.0 (TID 534) in 4 ms on localhost (134/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 134.0 in stage 8.0 (TID 535). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 135.0 in stage 8.0 (TID 536, localhost, partition 135,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 135.0 in stage 8.0 (TID 536)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 134.0 in stage 8.0 (TID 535) in 4 ms on localhost (135/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 135.0 in stage 8.0 (TID 536). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 136.0 in stage 8.0 (TID 537, localhost, partition 136,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 136.0 in stage 8.0 (TID 537)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 135.0 in stage 8.0 (TID 536) in 4 ms on localhost (136/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 136.0 in stage 8.0 (TID 537). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 137.0 in stage 8.0 (TID 538, localhost, partition 137,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 137.0 in stage 8.0 (TID 538)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 136.0 in stage 8.0 (TID 537) in 7 ms on localhost (137/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 137.0 in stage 8.0 (TID 538). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 138.0 in stage 8.0 (TID 539, localhost, partition 138,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 138.0 in stage 8.0 (TID 539)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 137.0 in stage 8.0 (TID 538) in 5 ms on localhost (138/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 138.0 in stage 8.0 (TID 539). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 139.0 in stage 8.0 (TID 540, localhost, partition 139,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 139.0 in stage 8.0 (TID 540)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 138.0 in stage 8.0 (TID 539) in 5 ms on localhost (139/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 139.0 in stage 8.0 (TID 540). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 140.0 in stage 8.0 (TID 541, localhost, partition 140,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 140.0 in stage 8.0 (TID 541)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 139.0 in stage 8.0 (TID 540) in 5 ms on localhost (140/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 140.0 in stage 8.0 (TID 541). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 141.0 in stage 8.0 (TID 542, localhost, partition 141,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 141.0 in stage 8.0 (TID 542)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 140.0 in stage 8.0 (TID 541) in 5 ms on localhost (141/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 141.0 in stage 8.0 (TID 542). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 142.0 in stage 8.0 (TID 543, localhost, partition 142,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 142.0 in stage 8.0 (TID 543)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 141.0 in stage 8.0 (TID 542) in 5 ms on localhost (142/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 142.0 in stage 8.0 (TID 543). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 143.0 in stage 8.0 (TID 544, localhost, partition 143,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 143.0 in stage 8.0 (TID 544)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 142.0 in stage 8.0 (TID 543) in 6 ms on localhost (143/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 143.0 in stage 8.0 (TID 544). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 144.0 in stage 8.0 (TID 545, localhost, partition 144,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 144.0 in stage 8.0 (TID 545)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 143.0 in stage 8.0 (TID 544) in 5 ms on localhost (144/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 144.0 in stage 8.0 (TID 545). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 145.0 in stage 8.0 (TID 546, localhost, partition 145,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 145.0 in stage 8.0 (TID 546)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 144.0 in stage 8.0 (TID 545) in 4 ms on localhost (145/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 145.0 in stage 8.0 (TID 546). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 146.0 in stage 8.0 (TID 547, localhost, partition 146,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 146.0 in stage 8.0 (TID 547)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 145.0 in stage 8.0 (TID 546) in 4 ms on localhost (146/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 146.0 in stage 8.0 (TID 547). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 147.0 in stage 8.0 (TID 548, localhost, partition 147,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 147.0 in stage 8.0 (TID 548)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 146.0 in stage 8.0 (TID 547) in 4 ms on localhost (147/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 147.0 in stage 8.0 (TID 548). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 148.0 in stage 8.0 (TID 549, localhost, partition 148,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 148.0 in stage 8.0 (TID 549)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 147.0 in stage 8.0 (TID 548) in 5 ms on localhost (148/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 148.0 in stage 8.0 (TID 549). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 149.0 in stage 8.0 (TID 550, localhost, partition 149,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 149.0 in stage 8.0 (TID 550)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 148.0 in stage 8.0 (TID 549) in 4 ms on localhost (149/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 149.0 in stage 8.0 (TID 550). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 150.0 in stage 8.0 (TID 551, localhost, partition 150,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 150.0 in stage 8.0 (TID 551)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 149.0 in stage 8.0 (TID 550) in 4 ms on localhost (150/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 150.0 in stage 8.0 (TID 551). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 151.0 in stage 8.0 (TID 552, localhost, partition 151,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 151.0 in stage 8.0 (TID 552)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 150.0 in stage 8.0 (TID 551) in 5 ms on localhost (151/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 151.0 in stage 8.0 (TID 552). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 152.0 in stage 8.0 (TID 553, localhost, partition 152,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 152.0 in stage 8.0 (TID 553)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 151.0 in stage 8.0 (TID 552) in 6 ms on localhost (152/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 152.0 in stage 8.0 (TID 553). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 153.0 in stage 8.0 (TID 554, localhost, partition 153,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 153.0 in stage 8.0 (TID 554)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 152.0 in stage 8.0 (TID 553) in 4 ms on localhost (153/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 153.0 in stage 8.0 (TID 554). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 154.0 in stage 8.0 (TID 555, localhost, partition 154,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 154.0 in stage 8.0 (TID 555)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 153.0 in stage 8.0 (TID 554) in 5 ms on localhost (154/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 154.0 in stage 8.0 (TID 555). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 155.0 in stage 8.0 (TID 556, localhost, partition 155,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 155.0 in stage 8.0 (TID 556)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 154.0 in stage 8.0 (TID 555) in 5 ms on localhost (155/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 155.0 in stage 8.0 (TID 556). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 156.0 in stage 8.0 (TID 557, localhost, partition 156,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 156.0 in stage 8.0 (TID 557)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 155.0 in stage 8.0 (TID 556) in 4 ms on localhost (156/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 156.0 in stage 8.0 (TID 557). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 157.0 in stage 8.0 (TID 558, localhost, partition 157,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 157.0 in stage 8.0 (TID 558)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 156.0 in stage 8.0 (TID 557) in 5 ms on localhost (157/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 157.0 in stage 8.0 (TID 558). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 158.0 in stage 8.0 (TID 559, localhost, partition 158,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 158.0 in stage 8.0 (TID 559)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 157.0 in stage 8.0 (TID 558) in 5 ms on localhost (158/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 158.0 in stage 8.0 (TID 559). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 159.0 in stage 8.0 (TID 560, localhost, partition 159,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 159.0 in stage 8.0 (TID 560)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 158.0 in stage 8.0 (TID 559) in 3 ms on localhost (159/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 159.0 in stage 8.0 (TID 560). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 160.0 in stage 8.0 (TID 561, localhost, partition 160,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 160.0 in stage 8.0 (TID 561)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 159.0 in stage 8.0 (TID 560) in 6 ms on localhost (160/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 160.0 in stage 8.0 (TID 561). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 161.0 in stage 8.0 (TID 562, localhost, partition 161,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 161.0 in stage 8.0 (TID 562)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 160.0 in stage 8.0 (TID 561) in 5 ms on localhost (161/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 161.0 in stage 8.0 (TID 562). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 162.0 in stage 8.0 (TID 563, localhost, partition 162,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 162.0 in stage 8.0 (TID 563)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 161.0 in stage 8.0 (TID 562) in 5 ms on localhost (162/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 162.0 in stage 8.0 (TID 563). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 163.0 in stage 8.0 (TID 564, localhost, partition 163,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 163.0 in stage 8.0 (TID 564)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 162.0 in stage 8.0 (TID 563) in 5 ms on localhost (163/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 163.0 in stage 8.0 (TID 564). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 164.0 in stage 8.0 (TID 565, localhost, partition 164,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 164.0 in stage 8.0 (TID 565)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 163.0 in stage 8.0 (TID 564) in 5 ms on localhost (164/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 164.0 in stage 8.0 (TID 565). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 165.0 in stage 8.0 (TID 566, localhost, partition 165,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 165.0 in stage 8.0 (TID 566)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 164.0 in stage 8.0 (TID 565) in 4 ms on localhost (165/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 165.0 in stage 8.0 (TID 566). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 166.0 in stage 8.0 (TID 567, localhost, partition 166,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 166.0 in stage 8.0 (TID 567)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 165.0 in stage 8.0 (TID 566) in 5 ms on localhost (166/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 166.0 in stage 8.0 (TID 567). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 167.0 in stage 8.0 (TID 568, localhost, partition 167,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 167.0 in stage 8.0 (TID 568)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 166.0 in stage 8.0 (TID 567) in 6 ms on localhost (167/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 167.0 in stage 8.0 (TID 568). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 168.0 in stage 8.0 (TID 569, localhost, partition 168,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 168.0 in stage 8.0 (TID 569)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 167.0 in stage 8.0 (TID 568) in 4 ms on localhost (168/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 168.0 in stage 8.0 (TID 569). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 169.0 in stage 8.0 (TID 570, localhost, partition 169,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 169.0 in stage 8.0 (TID 570)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 168.0 in stage 8.0 (TID 569) in 4 ms on localhost (169/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 169.0 in stage 8.0 (TID 570). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 170.0 in stage 8.0 (TID 571, localhost, partition 170,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 170.0 in stage 8.0 (TID 571)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 169.0 in stage 8.0 (TID 570) in 4 ms on localhost (170/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 170.0 in stage 8.0 (TID 571). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 171.0 in stage 8.0 (TID 572, localhost, partition 171,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 171.0 in stage 8.0 (TID 572)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 170.0 in stage 8.0 (TID 571) in 4 ms on localhost (171/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 171.0 in stage 8.0 (TID 572). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 172.0 in stage 8.0 (TID 573, localhost, partition 172,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 172.0 in stage 8.0 (TID 573)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 171.0 in stage 8.0 (TID 572) in 5 ms on localhost (172/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 172.0 in stage 8.0 (TID 573). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 173.0 in stage 8.0 (TID 574, localhost, partition 173,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 173.0 in stage 8.0 (TID 574)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 172.0 in stage 8.0 (TID 573) in 4 ms on localhost (173/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 173.0 in stage 8.0 (TID 574). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 174.0 in stage 8.0 (TID 575, localhost, partition 174,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 174.0 in stage 8.0 (TID 575)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 173.0 in stage 8.0 (TID 574) in 4 ms on localhost (174/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 174.0 in stage 8.0 (TID 575). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 175.0 in stage 8.0 (TID 576, localhost, partition 175,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 175.0 in stage 8.0 (TID 576)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 174.0 in stage 8.0 (TID 575) in 4 ms on localhost (175/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 175.0 in stage 8.0 (TID 576). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 176.0 in stage 8.0 (TID 577, localhost, partition 176,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 176.0 in stage 8.0 (TID 577)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 175.0 in stage 8.0 (TID 576) in 4 ms on localhost (176/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 176.0 in stage 8.0 (TID 577). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 177.0 in stage 8.0 (TID 578, localhost, partition 177,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 177.0 in stage 8.0 (TID 578)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 176.0 in stage 8.0 (TID 577) in 4 ms on localhost (177/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 177.0 in stage 8.0 (TID 578). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 178.0 in stage 8.0 (TID 579, localhost, partition 178,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 178.0 in stage 8.0 (TID 579)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 177.0 in stage 8.0 (TID 578) in 4 ms on localhost (178/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 178.0 in stage 8.0 (TID 579). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 179.0 in stage 8.0 (TID 580, localhost, partition 179,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 179.0 in stage 8.0 (TID 580)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 178.0 in stage 8.0 (TID 579) in 5 ms on localhost (179/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 179.0 in stage 8.0 (TID 580). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 180.0 in stage 8.0 (TID 581, localhost, partition 180,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 180.0 in stage 8.0 (TID 581)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 179.0 in stage 8.0 (TID 580) in 5 ms on localhost (180/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 180.0 in stage 8.0 (TID 581). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 181.0 in stage 8.0 (TID 582, localhost, partition 181,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 181.0 in stage 8.0 (TID 582)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 180.0 in stage 8.0 (TID 581) in 4 ms on localhost (181/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 181.0 in stage 8.0 (TID 582). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 182.0 in stage 8.0 (TID 583, localhost, partition 182,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 182.0 in stage 8.0 (TID 583)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 181.0 in stage 8.0 (TID 582) in 4 ms on localhost (182/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 182.0 in stage 8.0 (TID 583). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 183.0 in stage 8.0 (TID 584, localhost, partition 183,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 183.0 in stage 8.0 (TID 584)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 182.0 in stage 8.0 (TID 583) in 6 ms on localhost (183/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 183.0 in stage 8.0 (TID 584). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 184.0 in stage 8.0 (TID 585, localhost, partition 184,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 184.0 in stage 8.0 (TID 585)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 183.0 in stage 8.0 (TID 584) in 5 ms on localhost (184/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 184.0 in stage 8.0 (TID 585). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 185.0 in stage 8.0 (TID 586, localhost, partition 185,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 185.0 in stage 8.0 (TID 586)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 184.0 in stage 8.0 (TID 585) in 5 ms on localhost (185/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 185.0 in stage 8.0 (TID 586). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 186.0 in stage 8.0 (TID 587, localhost, partition 186,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 186.0 in stage 8.0 (TID 587)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 185.0 in stage 8.0 (TID 586) in 7 ms on localhost (186/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 186.0 in stage 8.0 (TID 587). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 187.0 in stage 8.0 (TID 588, localhost, partition 187,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 187.0 in stage 8.0 (TID 588)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 186.0 in stage 8.0 (TID 587) in 4 ms on localhost (187/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 187.0 in stage 8.0 (TID 588). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 188.0 in stage 8.0 (TID 589, localhost, partition 188,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 188.0 in stage 8.0 (TID 589)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 187.0 in stage 8.0 (TID 588) in 4 ms on localhost (188/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 188.0 in stage 8.0 (TID 589). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 189.0 in stage 8.0 (TID 590, localhost, partition 189,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 189.0 in stage 8.0 (TID 590)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 188.0 in stage 8.0 (TID 589) in 4 ms on localhost (189/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 189.0 in stage 8.0 (TID 590). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 190.0 in stage 8.0 (TID 591, localhost, partition 190,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 190.0 in stage 8.0 (TID 591)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 189.0 in stage 8.0 (TID 590) in 4 ms on localhost (190/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 190.0 in stage 8.0 (TID 591). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 191.0 in stage 8.0 (TID 592, localhost, partition 191,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 191.0 in stage 8.0 (TID 592)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 190.0 in stage 8.0 (TID 591) in 3 ms on localhost (191/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 191.0 in stage 8.0 (TID 592). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 192.0 in stage 8.0 (TID 593, localhost, partition 192,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 192.0 in stage 8.0 (TID 593)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 191.0 in stage 8.0 (TID 592) in 4 ms on localhost (192/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 192.0 in stage 8.0 (TID 593). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 193.0 in stage 8.0 (TID 594, localhost, partition 193,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 193.0 in stage 8.0 (TID 594)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 192.0 in stage 8.0 (TID 593) in 3 ms on localhost (193/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 193.0 in stage 8.0 (TID 594). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 194.0 in stage 8.0 (TID 595, localhost, partition 194,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 194.0 in stage 8.0 (TID 595)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 193.0 in stage 8.0 (TID 594) in 4 ms on localhost (194/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 194.0 in stage 8.0 (TID 595). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 195.0 in stage 8.0 (TID 596, localhost, partition 195,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 195.0 in stage 8.0 (TID 596)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 194.0 in stage 8.0 (TID 595) in 3 ms on localhost (195/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 195.0 in stage 8.0 (TID 596). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 196.0 in stage 8.0 (TID 597, localhost, partition 196,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 196.0 in stage 8.0 (TID 597)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 195.0 in stage 8.0 (TID 596) in 4 ms on localhost (196/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 1 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 196.0 in stage 8.0 (TID 597). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 197.0 in stage 8.0 (TID 598, localhost, partition 197,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 197.0 in stage 8.0 (TID 598)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Finished task 196.0 in stage 8.0 (TID 597) in 4 ms on localhost (197/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 197.0 in stage 8.0 (TID 598). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 198.0 in stage 8.0 (TID 599, localhost, partition 198,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 198.0 in stage 8.0 (TID 599)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 197.0 in stage 8.0 (TID 598) in 4 ms on localhost (198/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 198.0 in stage 8.0 (TID 599). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 199.0 in stage 8.0 (TID 600, localhost, partition 199,PROCESS_LOCAL, 1999 bytes)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Running task 199.0 in stage 8.0 (TID 600)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Finished task 198.0 in stage 8.0 (TID 599) in 4 ms on localhost (199/200)

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Getting 0 non-empty blocks out of 0 blocks

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Started 0 remote fetches in 0 ms

2019-05-25 16:48:36
[INFO]-[Thread: Executor task launch worker-1]-[org.apache.spark.Logging$class.logInfo()]: Finished task 199.0 in stage 8.0 (TID 600). 1627 bytes result sent to driver

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Finished task 199.0 in stage 8.0 (TID 600) in 3 ms on localhost (200/200)

2019-05-25 16:48:36
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 8.0, whose tasks have all completed, from pool 

2019-05-25 16:48:36
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 8 (foreachPartition at AccessLogJob.scala:200) finished in 0.824 s

2019-05-25 16:48:36
[INFO]-[Thread: streaming-job-executor-0]-[org.apache.spark.Logging$class.logInfo()]: Job 5 finished: foreachPartition at AccessLogJob.scala:200, took 0.841948 s

2019-05-25 16:48:36
[INFO]-[Thread: JobScheduler]-[org.apache.spark.Logging$class.logInfo()]: Finished job streaming job 1558774110000 ms.0 from job set of time 1558774110000 ms

2019-05-25 16:48:36
[INFO]-[Thread: JobScheduler]-[org.apache.spark.Logging$class.logInfo()]: Total delay: 6.378 s for time 1558774110000 ms (execution: 6.326 s)

2019-05-25 16:48:36
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: Checkpointing graph for time 1558774110000 ms

2019-05-25 16:48:36
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: Updating checkpoint data for time 1558774110000 ms

2019-05-25 16:48:36
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: Updated checkpoint data for time 1558774110000 ms

2019-05-25 16:48:36
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: Submitted checkpoint of time 1558774110000 ms writer queue

2019-05-25 16:48:36
[INFO]-[Thread: pool-24-thread-1]-[org.apache.spark.Logging$class.logInfo()]: Saving checkpoint for time 1558774110000 ms to file 'file:/C:/Users/511921540/Desktop/datamanger/log-streaming/checkpoint/checkpoint-1558774110000'

2019-05-25 16:48:36
[INFO]-[Thread: pool-24-thread-1]-[org.apache.spark.Logging$class.logInfo()]: Checkpoint for time 1558774110000 ms saved to file 'file:/C:/Users/511921540/Desktop/datamanger/log-streaming/checkpoint/checkpoint-1558774110000', took 3205 bytes and 48 ms

2019-05-25 16:48:36
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: Clearing checkpoint data for time 1558774110000 ms

2019-05-25 16:48:36
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: Cleared checkpoint data for time 1558774110000 ms

2019-05-25 16:48:36
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: Deleting batches ArrayBuffer()

2019-05-25 16:48:36
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: Attempting to clear 0 old log files in file:/C:/Users/511921540/Desktop/datamanger/log-streaming/checkpoint/receivedBlockMetadata older than 1558774080000: 

2019-05-25 16:48:36
[INFO]-[Thread: JobGenerator]-[org.apache.spark.Logging$class.logInfo()]: remove old batch metadata: 

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:host.name=LAPTOP-NFE4JUQT

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.version=1.7.0_80

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.vendor=Oracle Corporation

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.home=C:\Program Files\Java\jdk1.7.0_80\jre

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.class.path=C:\Program Files\Java\jdk1.7.0_80\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\jce.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\jfxrt.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\resources.jar;C:\Program Files\Java\jdk1.7.0_80\jre\lib\rt.jar;C:\Users\511921540\Desktop\datamanger\log-streaming\target\classes;D:\maven\repositories\org\scala-lang\scala-library\2.10.5\scala-library-2.10.5.jar;D:\maven\repositories\org\apache\spark\spark-core_2.10\1.6.3\spark-core_2.10-1.6.3.jar;D:\maven\repositories\org\apache\avro\avro-mapred\1.7.7\avro-mapred-1.7.7-hadoop2.jar;D:\maven\repositories\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7.jar;D:\maven\repositories\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7-tests.jar;D:\maven\repositories\com\twitter\chill_2.10\0.5.0\chill_2.10-0.5.0.jar;D:\maven\repositories\com\esotericsoftware\kryo\kryo\2.21\kryo-2.21.jar;D:\maven\repositories\com\esotericsoftware\reflectasm\reflectasm\1.07\reflectasm-1.07-shaded.jar;D:\maven\repositories\com\esotericsoftware\minlog\minlog\1.2\minlog-1.2.jar;D:\maven\repositories\org\objenesis\objenesis\1.2\objenesis-1.2.jar;D:\maven\repositories\com\twitter\chill-java\0.5.0\chill-java-0.5.0.jar;D:\maven\repositories\org\apache\xbean\xbean-asm5-shaded\4.4\xbean-asm5-shaded-4.4.jar;D:\maven\repositories\org\apache\hadoop\hadoop-client\2.2.0\hadoop-client-2.2.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-mapreduce-client-app\2.2.0\hadoop-mapreduce-client-app-2.2.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-mapreduce-client-common\2.2.0\hadoop-mapreduce-client-common-2.2.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-yarn-client\2.2.0\hadoop-yarn-client-2.2.0.jar;D:\maven\repositories\com\sun\jersey\jersey-test-framework\jersey-test-framework-grizzly2\1.9\jersey-test-framework-grizzly2-1.9.jar;D:\maven\repositories\com\sun\jersey\jersey-test-framework\jersey-test-framework-core\1.9\jersey-test-framework-core-1.9.jar;D:\maven\repositories\javax\servlet\javax.servlet-api\3.0.1\javax.servlet-api-3.0.1.jar;D:\maven\repositories\com\sun\jersey\jersey-grizzly2\1.9\jersey-grizzly2-1.9.jar;D:\maven\repositories\org\glassfish\grizzly\grizzly-http\2.1.2\grizzly-http-2.1.2.jar;D:\maven\repositories\org\glassfish\grizzly\grizzly-framework\2.1.2\grizzly-framework-2.1.2.jar;D:\maven\repositories\org\glassfish\gmbal\gmbal-api-only\3.0.0-b023\gmbal-api-only-3.0.0-b023.jar;D:\maven\repositories\org\glassfish\external\management-api\3.0.0-b012\management-api-3.0.0-b012.jar;D:\maven\repositories\org\glassfish\grizzly\grizzly-http-server\2.1.2\grizzly-http-server-2.1.2.jar;D:\maven\repositories\org\glassfish\grizzly\grizzly-rcm\2.1.2\grizzly-rcm-2.1.2.jar;D:\maven\repositories\org\glassfish\grizzly\grizzly-http-servlet\2.1.2\grizzly-http-servlet-2.1.2.jar;D:\maven\repositories\org\glassfish\javax.servlet\3.1\javax.servlet-3.1.jar;D:\maven\repositories\org\apache\hadoop\hadoop-yarn-server-common\2.2.0\hadoop-yarn-server-common-2.2.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.2.0\hadoop-mapreduce-client-shuffle-2.2.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-yarn-api\2.2.0\hadoop-yarn-api-2.2.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.2.0\hadoop-mapreduce-client-jobclient-2.2.0.jar;D:\maven\repositories\org\apache\spark\spark-launcher_2.10\1.6.3\spark-launcher_2.10-1.6.3.jar;D:\maven\repositories\org\apache\spark\spark-network-common_2.10\1.6.3\spark-network-common_2.10-1.6.3.jar;D:\maven\repositories\org\apache\spark\spark-network-shuffle_2.10\1.6.3\spark-network-shuffle_2.10-1.6.3.jar;D:\maven\repositories\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;D:\maven\repositories\com\fasterxml\jackson\core\jackson-annotations\2.4.4\jackson-annotations-2.4.4.jar;D:\maven\repositories\org\apache\spark\spark-unsafe_2.10\1.6.3\spark-unsafe_2.10-1.6.3.jar;D:\maven\repositories\net\java\dev\jets3t\jets3t\0.7.1\jets3t-0.7.1.jar;D:\maven\repositories\org\apache\curator\curator-recipes\2.4.0\curator-recipes-2.4.0.jar;D:\maven\repositories\org\apache\curator\curator-framework\2.4.0\curator-framework-2.4.0.jar;D:\maven\repositories\org\apache\curator\curator-client\2.4.0\curator-client-2.4.0.jar;D:\maven\repositories\org\eclipse\jetty\orbit\javax.servlet\3.0.0.v201112011016\javax.servlet-3.0.0.v201112011016.jar;D:\maven\repositories\org\apache\commons\commons-lang3\3.3.2\commons-lang3-3.3.2.jar;D:\maven\repositories\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;D:\maven\repositories\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;D:\maven\repositories\org\slf4j\slf4j-api\1.7.10\slf4j-api-1.7.10.jar;D:\maven\repositories\org\slf4j\jul-to-slf4j\1.7.10\jul-to-slf4j-1.7.10.jar;D:\maven\repositories\org\slf4j\jcl-over-slf4j\1.7.10\jcl-over-slf4j-1.7.10.jar;D:\maven\repositories\log4j\log4j\1.2.17\log4j-1.2.17.jar;D:\maven\repositories\org\slf4j\slf4j-log4j12\1.7.10\slf4j-log4j12-1.7.10.jar;D:\maven\repositories\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;D:\maven\repositories\org\xerial\snappy\snappy-java\1.1.2.6\snappy-java-1.1.2.6.jar;D:\maven\repositories\net\jpountz\lz4\lz4\1.3.0\lz4-1.3.0.jar;D:\maven\repositories\org\roaringbitmap\RoaringBitmap\0.5.11\RoaringBitmap-0.5.11.jar;D:\maven\repositories\commons-net\commons-net\2.2\commons-net-2.2.jar;D:\maven\repositories\com\typesafe\akka\akka-remote_2.10\2.3.11\akka-remote_2.10-2.3.11.jar;D:\maven\repositories\com\typesafe\akka\akka-actor_2.10\2.3.11\akka-actor_2.10-2.3.11.jar;D:\maven\repositories\com\typesafe\config\1.2.1\config-1.2.1.jar;D:\maven\repositories\org\uncommons\maths\uncommons-maths\1.2.2a\uncommons-maths-1.2.2a.jar;D:\maven\repositories\com\typesafe\akka\akka-slf4j_2.10\2.3.11\akka-slf4j_2.10-2.3.11.jar;D:\maven\repositories\org\json4s\json4s-jackson_2.10\3.2.10\json4s-jackson_2.10-3.2.10.jar;D:\maven\repositories\org\json4s\json4s-core_2.10\3.2.10\json4s-core_2.10-3.2.10.jar;D:\maven\repositories\org\json4s\json4s-ast_2.10\3.2.10\json4s-ast_2.10-3.2.10.jar;D:\maven\repositories\org\scala-lang\scalap\2.10.0\scalap-2.10.0.jar;D:\maven\repositories\org\scala-lang\scala-compiler\2.10.0\scala-compiler-2.10.0.jar;D:\maven\repositories\com\sun\jersey\jersey-server\1.9\jersey-server-1.9.jar;D:\maven\repositories\asm\asm\3.1\asm-3.1.jar;D:\maven\repositories\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;D:\maven\repositories\org\apache\mesos\mesos\0.21.1\mesos-0.21.1-shaded-protobuf.jar;D:\maven\repositories\io\netty\netty-all\4.0.29.Final\netty-all-4.0.29.Final.jar;D:\maven\repositories\com\clearspring\analytics\stream\2.7.0\stream-2.7.0.jar;D:\maven\repositories\io\dropwizard\metrics\metrics-core\3.1.2\metrics-core-3.1.2.jar;D:\maven\repositories\io\dropwizard\metrics\metrics-jvm\3.1.2\metrics-jvm-3.1.2.jar;D:\maven\repositories\io\dropwizard\metrics\metrics-json\3.1.2\metrics-json-3.1.2.jar;D:\maven\repositories\io\dropwizard\metrics\metrics-graphite\3.1.2\metrics-graphite-3.1.2.jar;D:\maven\repositories\com\fasterxml\jackson\core\jackson-databind\2.4.4\jackson-databind-2.4.4.jar;D:\maven\repositories\com\fasterxml\jackson\core\jackson-core\2.4.4\jackson-core-2.4.4.jar;D:\maven\repositories\com\fasterxml\jackson\module\jackson-module-scala_2.10\2.4.4\jackson-module-scala_2.10-2.4.4.jar;D:\maven\repositories\org\scala-lang\scala-reflect\2.10.4\scala-reflect-2.10.4.jar;D:\maven\repositories\com\thoughtworks\paranamer\paranamer\2.6\paranamer-2.6.jar;D:\maven\repositories\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;D:\maven\repositories\oro\oro\2.0.8\oro-2.0.8.jar;D:\maven\repositories\org\tachyonproject\tachyon-client\0.8.2\tachyon-client-0.8.2.jar;D:\maven\repositories\org\tachyonproject\tachyon-underfs-hdfs\0.8.2\tachyon-underfs-hdfs-0.8.2.jar;D:\maven\repositories\org\tachyonproject\tachyon-underfs-s3\0.8.2\tachyon-underfs-s3-0.8.2.jar;D:\maven\repositories\org\tachyonproject\tachyon-underfs-local\0.8.2\tachyon-underfs-local-0.8.2.jar;D:\maven\repositories\net\razorvine\pyrolite\4.9\pyrolite-4.9.jar;D:\maven\repositories\net\sf\py4j\py4j\0.9\py4j-0.9.jar;D:\maven\repositories\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;D:\maven\repositories\org\apache\spark\spark-streaming_2.10\1.6.3\spark-streaming_2.10-1.6.3.jar;D:\maven\repositories\org\apache\spark\spark-sql_2.10\1.6.3\spark-sql_2.10-1.6.3.jar;D:\maven\repositories\org\apache\spark\spark-catalyst_2.10\1.6.3\spark-catalyst_2.10-1.6.3.jar;D:\maven\repositories\org\codehaus\janino\janino\2.7.8\janino-2.7.8.jar;D:\maven\repositories\org\codehaus\janino\commons-compiler\2.7.8\commons-compiler-2.7.8.jar;D:\maven\repositories\org\apache\parquet\parquet-column\1.7.0\parquet-column-1.7.0.jar;D:\maven\repositories\org\apache\parquet\parquet-common\1.7.0\parquet-common-1.7.0.jar;D:\maven\repositories\org\apache\parquet\parquet-encoding\1.7.0\parquet-encoding-1.7.0.jar;D:\maven\repositories\org\apache\parquet\parquet-generator\1.7.0\parquet-generator-1.7.0.jar;D:\maven\repositories\org\apache\parquet\parquet-hadoop\1.7.0\parquet-hadoop-1.7.0.jar;D:\maven\repositories\org\apache\parquet\parquet-format\2.3.0-incubating\parquet-format-2.3.0-incubating.jar;D:\maven\repositories\org\apache\parquet\parquet-jackson\1.7.0\parquet-jackson-1.7.0.jar;D:\maven\repositories\org\apache\spark\spark-streaming-kafka_2.10\1.6.3\spark-streaming-kafka_2.10-1.6.3.jar;D:\maven\repositories\org\apache\kafka\kafka_2.10\0.8.2.1\kafka_2.10-0.8.2.1.jar;D:\maven\repositories\org\apache\kafka\kafka-clients\0.8.2.1\kafka-clients-0.8.2.1.jar;D:\maven\repositories\com\101tec\zkclient\0.3\zkclient-0.3.jar;D:\maven\repositories\net\sf\json-lib\json-lib\2.4\json-lib-2.4-jdk15.jar;D:\maven\repositories\commons-beanutils\commons-beanutils\1.8.0\commons-beanutils-1.8.0.jar;D:\maven\repositories\commons-collections\commons-collections\3.2.1\commons-collections-3.2.1.jar;D:\maven\repositories\commons-lang\commons-lang\2.5\commons-lang-2.5.jar;D:\maven\repositories\commons-logging\commons-logging\1.1.1\commons-logging-1.1.1.jar;D:\maven\repositories\net\sf\ezmorph\ezmorph\1.0.6\ezmorph-1.0.6.jar;D:\maven\repositories\mysql\mysql-connector-java\5.1.29\mysql-connector-java-5.1.29.jar;D:\maven\repositories\com\jolbox\bonecp\0.8.0.RELEASE\bonecp-0.8.0.RELEASE.jar;D:\maven\repositories\com\google\guava\guava\15.0\guava-15.0.jar;D:\maven\repositories\org\apache\hbase\hbase-client\1.0.1\hbase-client-1.0.1.jar;D:\maven\repositories\org\apache\hbase\hbase-annotations\1.0.1\hbase-annotations-1.0.1.jar;D:\maven\repositories\org\apache\hbase\hbase-common\1.0.1\hbase-common-1.0.1.jar;D:\maven\repositories\org\apache\hbase\hbase-protocol\1.0.1\hbase-protocol-1.0.1.jar;D:\maven\repositories\commons-codec\commons-codec\1.9\commons-codec-1.9.jar;D:\maven\repositories\commons-io\commons-io\2.4\commons-io-2.4.jar;D:\maven\repositories\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;D:\maven\repositories\org\apache\htrace\htrace-core\3.1.0-incubating\htrace-core-3.1.0-incubating.jar;D:\maven\repositories\org\codehaus\jackson\jackson-mapper-asl\1.8.8\jackson-mapper-asl-1.8.8.jar;D:\maven\repositories\org\jruby\jcodings\jcodings\1.0.8\jcodings-1.0.8.jar;D:\maven\repositories\org\jruby\joni\joni\2.1.2\joni-2.1.2.jar;D:\maven\repositories\org\apache\hadoop\hadoop-auth\2.5.1\hadoop-auth-2.5.1.jar;D:\maven\repositories\org\apache\httpcomponents\httpclient\4.2.5\httpclient-4.2.5.jar;D:\maven\repositories\org\apache\httpcomponents\httpcore\4.2.4\httpcore-4.2.4.jar;D:\maven\repositories\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;D:\maven\repositories\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;D:\maven\repositories\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;D:\maven\repositories\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;D:\maven\repositories\org\apache\hadoop\hadoop-common\2.5.1\hadoop-common-2.5.1.jar;D:\maven\repositories\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;D:\maven\repositories\commons-el\commons-el\1.0\commons-el-1.0.jar;D:\maven\repositories\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;D:\maven\repositories\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;D:\maven\repositories\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;D:\maven\repositories\com\jcraft\jsch\0.1.42\jsch-0.1.42.jar;D:\maven\repositories\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;D:\maven\repositories\org\tukaani\xz\1.0\xz-1.0.jar;D:\maven\repositories\com\github\stephenc\findbugs\findbugs-annotations\1.3.9-1\findbugs-annotations-1.3.9-1.jar;D:\maven\repositories\junit\junit\4.11\junit-4.11.jar;D:\maven\repositories\org\hamcrest\hamcrest-core\1.3\hamcrest-core-1.3.jar;D:\maven\repositories\org\apache\hbase\hbase-server\1.0.1\hbase-server-1.0.1.jar;D:\maven\repositories\org\apache\hbase\hbase-prefix-tree\1.0.1\hbase-prefix-tree-1.0.1.jar;D:\maven\repositories\org\apache\hbase\hbase-common\1.0.1\hbase-common-1.0.1-tests.jar;D:\maven\repositories\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;D:\maven\repositories\org\apache\hbase\hbase-hadoop-compat\1.0.1\hbase-hadoop-compat-1.0.1.jar;D:\maven\repositories\org\apache\hbase\hbase-hadoop2-compat\1.0.1\hbase-hadoop2-compat-1.0.1.jar;D:\maven\repositories\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;D:\maven\repositories\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;D:\maven\repositories\org\apache\commons\commons-math\2.2\commons-math-2.2.jar;D:\maven\repositories\org\apache\zookeeper\zookeeper\3.4.6\zookeeper-3.4.6.jar;D:\maven\repositories\org\mortbay\jetty\jetty\6.1.26\jetty-6.1.26.jar;D:\maven\repositories\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;D:\maven\repositories\org\mortbay\jetty\jetty-sslengine\6.1.26\jetty-sslengine-6.1.26.jar;D:\maven\repositories\org\mortbay\jetty\jsp-2.1\6.1.14\jsp-2.1-6.1.14.jar;D:\maven\repositories\org\mortbay\jetty\jsp-api-2.1\6.1.14\jsp-api-2.1-6.1.14.jar;D:\maven\repositories\org\mortbay\jetty\servlet-api-2.5\6.1.14\servlet-api-2.5-6.1.14.jar;D:\maven\repositories\org\codehaus\jackson\jackson-core-asl\1.8.8\jackson-core-asl-1.8.8.jar;D:\maven\repositories\org\codehaus\jackson\jackson-jaxrs\1.8.8\jackson-jaxrs-1.8.8.jar;D:\maven\repositories\tomcat\jasper-compiler\5.5.23\jasper-compiler-5.5.23.jar;D:\maven\repositories\tomcat\jasper-runtime\5.5.23\jasper-runtime-5.5.23.jar;D:\maven\repositories\org\jamon\jamon-runtime\2.3.1\jamon-runtime-2.3.1.jar;D:\maven\repositories\com\lmax\disruptor\3.3.0\disruptor-3.3.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-hdfs\2.5.1\hadoop-hdfs-2.5.1.jar;D:\maven\repositories\commons-daemon\commons-daemon\1.0.13\commons-daemon-1.0.13.jar;D:\maven\repositories\org\apache\hadoop\hadoop-mapreduce-client-core\2.6.0\hadoop-mapreduce-client-core-2.6.0.jar;D:\maven\repositories\org\apache\hadoop\hadoop-yarn-common\2.6.0\hadoop-yarn-common-2.6.0.jar;D:\maven\repositories\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;D:\maven\repositories\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;D:\maven\repositories\javax\activation\activation\1.1\activation-1.1.jar;D:\maven\repositories\javax\servlet\servlet-api\2.5\servlet-api-2.5.jar;D:\maven\repositories\com\sun\jersey\jersey-client\1.9\jersey-client-1.9.jar;D:\maven\repositories\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;D:\maven\repositories\com\google\inject\guice\3.0\guice-3.0.jar;D:\maven\repositories\javax\inject\javax.inject\1\javax.inject-1.jar;D:\maven\repositories\aopalliance\aopalliance\1.0\aopalliance-1.0.jar;D:\maven\repositories\com\sun\jersey\jersey-json\1.9\jersey-json-1.9.jar;D:\maven\repositories\org\codehaus\jettison\jettison\1.1\jettison-1.1.jar;D:\maven\repositories\com\sun\xml\bind\jaxb-impl\2.2.3-1\jaxb-impl-2.2.3-1.jar;D:\maven\repositories\com\sun\jersey\contribs\jersey-guice\1.9\jersey-guice-1.9.jar;D:\maven\repositories\org\apache\avro\avro\1.7.4\avro-1.7.4.jar;D:\maven\repositories\org\apache\hadoop\hadoop-annotations\2.6.0\hadoop-annotations-2.6.0.jar;D:\maven\repositories\com\google\inject\extensions\guice-servlet\3.0\guice-servlet-3.0.jar;D:\maven\repositories\io\netty\netty\3.6.2.Final\netty-3.6.2.Final.jar;D:\IDEA\IntelliJ IDEA 2018.1\lib\idea_rt.jar

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.library.path=C:\Program Files\Java\jdk1.7.0_80\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files (x86)\Java\jdk1.8.0_144\bin;D:\hadoop\hadoop-2.7.1\bin;D:\hadoop\hadoop-2.7.1\sbin;C:\ProgramData\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\MySQL\MySQL Utilities 1.6\;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;D:\OpenVPN\bin;D:\Program Files (x86)\OpenVPN\bin;D:\maven\apache-maven-3.5.3\bin;C:\Program Files\nodejs\;D:\git\Git\cmd;C:\Program Files\TortoiseSVN\bin;C:\Program Files (x86)\OpenVPN\bin;D:\python-anncode;D:\maven\apache-maven-3.5.3\bin;D:\python-anncode\Library\mingw-w64\bin;D:\python-anncode\Library\usr\bin;D:\python-anncode\Library\bin;D:\python-anncode\Scripts;C:\Users\511921540\AppData\Local\Microsoft\WindowsApps;C:\Users\511921540\AppData\Roaming\npm;;.

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.io.tmpdir=C:\Users\511921~1\AppData\Local\Temp\

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:java.compiler=<NA>

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:os.name=Windows 8.1

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:os.arch=amd64

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:os.version=6.3

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:user.name=511921540

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:user.home=C:\Users\511921540

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.Environment.logEnv()]: Client environment:user.dir=C:\Users\511921540\Desktop\datamanger\log-streaming

2019-05-25 16:48:37
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@beb534b

2019-05-25 16:48:38
[INFO]-[Thread: ZkClient-EventThread-90-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Terminate ZkClient event thread.

2019-05-25 16:48:38
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:38
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:38
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:38
[ERROR]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:38
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:38
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774119200

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:39
[INFO]-[Thread: Thread-18]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:39
[INFO]-[Thread: Thread-18]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:39
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:39
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 0.0 (TID 0)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:39
[WARN]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:39
[ERROR]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 0.0 failed 1 times; aborting job

2019-05-25 16:48:39
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 0.0, whose tasks have all completed, from pool 

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 0

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 0 (start at AccessLogJob.scala:578) failed in 10.620 s

2019-05-25 16:48:39
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:39
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:39
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 6 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 9 (start at AccessLogJob.scala:578)

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 9 (Receiver 0 ParallelCollectionRDD[38] at makeRDD at ReceiverTracker.scala:588), which has no missing parents

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_5 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:39
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_5_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 5 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 9 (Receiver 0 ParallelCollectionRDD[38] at makeRDD at ReceiverTracker.scala:588)

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 9.0 with 1 tasks

2019-05-25 16:48:39
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 9.0 (TID 601, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 9.0 (TID 601)

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774119400

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:39
[INFO]-[Thread: Thread-21]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:39
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774119315-99654386], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@eb2575b

2019-05-25 16:48:39
[INFO]-[Thread: ZkClient-EventThread-105-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:39
[ERROR]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774119600

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:39
[INFO]-[Thread: Thread-21]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:39
[INFO]-[Thread: Thread-21]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:39
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:39
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 9.0 (TID 601)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:39
[WARN]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 9.0 (TID 601, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:39
[ERROR]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 9.0 failed 1 times; aborting job

2019-05-25 16:48:39
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 9.0, whose tasks have all completed, from pool 

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 9

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 9 (start at AccessLogJob.scala:578) failed in 0.320 s

2019-05-25 16:48:39
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 601, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:39
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:39
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 7 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 10 (start at AccessLogJob.scala:578)

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 10 (Receiver 0 ParallelCollectionRDD[39] at makeRDD at ReceiverTracker.scala:588), which has no missing parents

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_6 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:39
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_6_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 6 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 10 (Receiver 0 ParallelCollectionRDD[39] at makeRDD at ReceiverTracker.scala:588)

2019-05-25 16:48:39
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 10.0 with 1 tasks

2019-05-25 16:48:39
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 10.0 (TID 602, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 10.0 (TID 602)

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774119800

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:39
[INFO]-[Thread: Thread-23]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:39
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774119675-8b093dc7], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@5ab3ca86

2019-05-25 16:48:39
[INFO]-[Thread: ZkClient-EventThread-108-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:39
[ERROR]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:39
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774120000

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:40
[INFO]-[Thread: Thread-23]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:40
[INFO]-[Thread: Thread-23]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:40
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 10.0 (TID 602)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:40
[WARN]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 10.0 (TID 602, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:40
[ERROR]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 10.0 failed 1 times; aborting job

2019-05-25 16:48:40
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 10.0, whose tasks have all completed, from pool 

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 10

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 10 (start at AccessLogJob.scala:578) failed in 0.360 s

2019-05-25 16:48:40
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 602, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:40
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:40
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 8 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 11 (start at AccessLogJob.scala:578)

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 11 (Receiver 0 ParallelCollectionRDD[40] at makeRDD at ReceiverTracker.scala:588), which has no missing parents

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_7 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_7_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:40
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_7_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 7 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 11 (Receiver 0 ParallelCollectionRDD[40] at makeRDD at ReceiverTracker.scala:588)

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 11.0 with 1 tasks

2019-05-25 16:48:40
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 11.0 (TID 603, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 11.0 (TID 603)

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774120200

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:40
[INFO]-[Thread: Thread-25]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:40
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774120070-90a52853], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@385e57a0

2019-05-25 16:48:40
[INFO]-[Thread: ZkClient-EventThread-111-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:40
[ERROR]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774120400

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:40
[INFO]-[Thread: Thread-25]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:40
[INFO]-[Thread: Thread-25]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:40
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 11.0 (TID 603)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:40
[WARN]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 11.0 (TID 603, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:40
[ERROR]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 11.0 failed 1 times; aborting job

2019-05-25 16:48:40
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 11.0, whose tasks have all completed, from pool 

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 11

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 11 (start at AccessLogJob.scala:578) failed in 0.365 s

2019-05-25 16:48:40
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 603, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:40
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:40
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 9 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 12 (start at AccessLogJob.scala:578)

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 12 (Receiver 0 ParallelCollectionRDD[41] at makeRDD at ReceiverTracker.scala:588), which has no missing parents

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_8 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:40
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_8_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 8 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 12 (Receiver 0 ParallelCollectionRDD[41] at makeRDD at ReceiverTracker.scala:588)

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 12.0 with 1 tasks

2019-05-25 16:48:40
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 12.0 (TID 604, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 12.0 (TID 604)

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774120600

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:40
[INFO]-[Thread: Thread-27]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:40
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774120486-30db0e22], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@26498c0e

2019-05-25 16:48:40
[INFO]-[Thread: ZkClient-EventThread-114-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:40
[ERROR]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774120800

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:40
[INFO]-[Thread: Thread-27]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:40
[INFO]-[Thread: Thread-27]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:40
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 12.0 (TID 604)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:40
[WARN]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 12.0 (TID 604, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:40
[ERROR]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 12.0 failed 1 times; aborting job

2019-05-25 16:48:40
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 12.0, whose tasks have all completed, from pool 

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 12

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 12 (start at AccessLogJob.scala:578) failed in 0.353 s

2019-05-25 16:48:40
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 604, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:40
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:40
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 10 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 13 (start at AccessLogJob.scala:578)

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 13 (Receiver 0 ParallelCollectionRDD[42] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_9 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_9_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:40
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_9_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 9 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 13 (Receiver 0 ParallelCollectionRDD[42] at start at AccessLogJob.scala:578)

2019-05-25 16:48:40
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 13.0 with 1 tasks

2019-05-25 16:48:40
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 13.0 (TID 605, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 13.0 (TID 605)

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774121000

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:40
[INFO]-[Thread: Thread-29]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:40
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774120851-a3de1b1c], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@13f037fa

2019-05-25 16:48:40
[INFO]-[Thread: ZkClient-EventThread-117-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:40
[ERROR]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:40
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774121200

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:41
[INFO]-[Thread: Thread-29]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:41
[INFO]-[Thread: Thread-29]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:41
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:41
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 13.0 (TID 605)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:41
[WARN]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 13.0 (TID 605, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:41
[ERROR]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 13.0 failed 1 times; aborting job

2019-05-25 16:48:41
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 13.0, whose tasks have all completed, from pool 

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 13

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 13 (start at AccessLogJob.scala:578) failed in 0.373 s

2019-05-25 16:48:41
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 605, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:41
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:41
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 11 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 14 (start at AccessLogJob.scala:578)

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 14 (Receiver 0 ParallelCollectionRDD[43] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_10 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_10_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:41
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_10_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 10 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 14 (Receiver 0 ParallelCollectionRDD[43] at start at AccessLogJob.scala:578)

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 14.0 with 1 tasks

2019-05-25 16:48:41
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 14.0 (TID 606, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 14.0 (TID 606)

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774121400

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:41
[INFO]-[Thread: Thread-31]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:41
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774121239-7fe1da98], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@2c04fa9

2019-05-25 16:48:41
[INFO]-[Thread: ZkClient-EventThread-120-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:41
[ERROR]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774121600

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:41
[INFO]-[Thread: Thread-31]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:41
[INFO]-[Thread: Thread-31]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:41
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:41
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 14.0 (TID 606)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:41
[WARN]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 14.0 (TID 606, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:41
[ERROR]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 14.0 failed 1 times; aborting job

2019-05-25 16:48:41
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 14.0, whose tasks have all completed, from pool 

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 14

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 14 (start at AccessLogJob.scala:578) failed in 0.384 s

2019-05-25 16:48:41
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 606, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:41
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:41
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 12 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 15 (start at AccessLogJob.scala:578)

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 15 (Receiver 0 ParallelCollectionRDD[44] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_11 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_11_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:41
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_11_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 11 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 15 (Receiver 0 ParallelCollectionRDD[44] at start at AccessLogJob.scala:578)

2019-05-25 16:48:41
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 15.0 with 1 tasks

2019-05-25 16:48:41
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 15.0 (TID 607, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 15.0 (TID 607)

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774121800

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:41
[INFO]-[Thread: Thread-33]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:41
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774121645-8ca2e8eb], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@6560ed60

2019-05-25 16:48:41
[INFO]-[Thread: ZkClient-EventThread-123-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:41
[ERROR]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:41
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774122000

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:42
[INFO]-[Thread: Thread-33]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:42
[INFO]-[Thread: Thread-33]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:42
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 15.0 (TID 607)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:42
[WARN]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 15.0 (TID 607, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:42
[ERROR]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 15.0 failed 1 times; aborting job

2019-05-25 16:48:42
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 15.0, whose tasks have all completed, from pool 

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 15

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 15 (start at AccessLogJob.scala:578) failed in 0.382 s

2019-05-25 16:48:42
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 607, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:42
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:42
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 13 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 16 (start at AccessLogJob.scala:578)

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 16 (Receiver 0 ParallelCollectionRDD[45] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_12 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_12_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:42
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_12_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 12 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 16 (Receiver 0 ParallelCollectionRDD[45] at start at AccessLogJob.scala:578)

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 16.0 with 1 tasks

2019-05-25 16:48:42
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 16.0 (TID 608, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 16.0 (TID 608)

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774122200

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:42
[INFO]-[Thread: Thread-35]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:42
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774122061-9d6ab292], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@5ff23277

2019-05-25 16:48:42
[INFO]-[Thread: ZkClient-EventThread-126-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:42
[ERROR]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774122400

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:42
[INFO]-[Thread: Thread-35]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:42
[INFO]-[Thread: Thread-35]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:42
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 16.0 (TID 608)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:42
[WARN]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 16.0 (TID 608, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:42
[ERROR]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 16.0 failed 1 times; aborting job

2019-05-25 16:48:42
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 16.0, whose tasks have all completed, from pool 

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 16

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 16 (start at AccessLogJob.scala:578) failed in 0.370 s

2019-05-25 16:48:42
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 608, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:42
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:42
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 14 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 17 (start at AccessLogJob.scala:578)

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 17 (Receiver 0 ParallelCollectionRDD[46] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_13 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_13_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:42
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_13_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 13 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 17 (Receiver 0 ParallelCollectionRDD[46] at start at AccessLogJob.scala:578)

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 17.0 with 1 tasks

2019-05-25 16:48:42
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 17.0 (TID 609, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 17.0 (TID 609)

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774122600

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:42
[INFO]-[Thread: Thread-37]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:42
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774122440-28be8f48], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@46cd31f6

2019-05-25 16:48:42
[INFO]-[Thread: ZkClient-EventThread-129-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:42
[ERROR]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774122800

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:42
[INFO]-[Thread: Thread-37]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:42
[INFO]-[Thread: Thread-37]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:42
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 17.0 (TID 609)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:42
[WARN]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 17.0 (TID 609, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:42
[ERROR]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 17.0 failed 1 times; aborting job

2019-05-25 16:48:42
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 17.0, whose tasks have all completed, from pool 

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 17

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 17 (start at AccessLogJob.scala:578) failed in 0.377 s

2019-05-25 16:48:42
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 609, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:42
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:42
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 15 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 18 (start at AccessLogJob.scala:578)

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 18 (Receiver 0 ParallelCollectionRDD[47] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_14 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_14_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:42
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_14_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 14 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 18 (Receiver 0 ParallelCollectionRDD[47] at start at AccessLogJob.scala:578)

2019-05-25 16:48:42
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 18.0 with 1 tasks

2019-05-25 16:48:42
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 18.0 (TID 610, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 18.0 (TID 610)

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774123000

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:42
[INFO]-[Thread: Thread-39]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:42
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774122835-69d0afd6], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@569ce876

2019-05-25 16:48:42
[INFO]-[Thread: ZkClient-EventThread-132-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:42
[ERROR]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:42
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774123200

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:43
[INFO]-[Thread: Thread-39]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:43
[INFO]-[Thread: Thread-39]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:43
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:43
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 18.0 (TID 610)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:43
[WARN]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 18.0 (TID 610, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:43
[ERROR]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 18.0 failed 1 times; aborting job

2019-05-25 16:48:43
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 18.0, whose tasks have all completed, from pool 

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 18

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 18 (start at AccessLogJob.scala:578) failed in 0.385 s

2019-05-25 16:48:43
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 610, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:43
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:43
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 16 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 19 (start at AccessLogJob.scala:578)

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 19 (Receiver 0 ParallelCollectionRDD[48] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_15 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:43
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_15_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 15 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 19 (Receiver 0 ParallelCollectionRDD[48] at start at AccessLogJob.scala:578)

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 19.0 with 1 tasks

2019-05-25 16:48:43
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 19.0 (TID 611, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 19.0 (TID 611)

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774123400

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:43
[INFO]-[Thread: Thread-41]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:43
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774123244-350344b7], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@4f1501e9

2019-05-25 16:48:43
[INFO]-[Thread: ZkClient-EventThread-135-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:43
[ERROR]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774123600

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:43
[INFO]-[Thread: Thread-41]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:43
[INFO]-[Thread: Thread-41]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:43
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:43
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 19.0 (TID 611)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:43
[WARN]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 19.0 (TID 611, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:43
[ERROR]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 19.0 failed 1 times; aborting job

2019-05-25 16:48:43
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 19.0, whose tasks have all completed, from pool 

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 19

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 19 (start at AccessLogJob.scala:578) failed in 0.369 s

2019-05-25 16:48:43
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 611, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:43
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:43
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 17 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 20 (start at AccessLogJob.scala:578)

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 20 (Receiver 0 ParallelCollectionRDD[49] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_16 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_16_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:43
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_16_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 16 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 20 (Receiver 0 ParallelCollectionRDD[49] at start at AccessLogJob.scala:578)

2019-05-25 16:48:43
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 20.0 with 1 tasks

2019-05-25 16:48:43
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 20.0 (TID 612, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 20.0 (TID 612)

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774123800

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:43
[INFO]-[Thread: Thread-43]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:43
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774123640-80f9c5ce], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@420a7d80

2019-05-25 16:48:43
[INFO]-[Thread: ZkClient-EventThread-138-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:43
[ERROR]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:43
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774124000

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:44
[INFO]-[Thread: Thread-43]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:44
[INFO]-[Thread: Thread-43]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:44
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 20.0 (TID 612)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:44
[WARN]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 20.0 (TID 612, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:44
[ERROR]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 20.0 failed 1 times; aborting job

2019-05-25 16:48:44
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 20.0, whose tasks have all completed, from pool 

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 20

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 20 (start at AccessLogJob.scala:578) failed in 0.379 s

2019-05-25 16:48:44
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 612, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:44
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:44
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 18 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 21 (start at AccessLogJob.scala:578)

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 21 (Receiver 0 ParallelCollectionRDD[50] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_17 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_17_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:44
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_17_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 17 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 21 (Receiver 0 ParallelCollectionRDD[50] at start at AccessLogJob.scala:578)

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 21.0 with 1 tasks

2019-05-25 16:48:44
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 21.0 (TID 613, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 21.0 (TID 613)

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774124200

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:44
[INFO]-[Thread: Thread-45]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:44
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774124036-ae854cfa], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@7288dcb1

2019-05-25 16:48:44
[INFO]-[Thread: ZkClient-EventThread-141-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:44
[ERROR]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774124400

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:44
[INFO]-[Thread: Thread-45]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:44
[INFO]-[Thread: Thread-45]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:44
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 21.0 (TID 613)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:44
[WARN]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 21.0 (TID 613, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:44
[ERROR]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 21.0 failed 1 times; aborting job

2019-05-25 16:48:44
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 21.0, whose tasks have all completed, from pool 

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 21

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 21 (start at AccessLogJob.scala:578) failed in 0.385 s

2019-05-25 16:48:44
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 613, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:44
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:44
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 19 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 22 (start at AccessLogJob.scala:578)

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 22 (Receiver 0 ParallelCollectionRDD[51] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_18 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_18_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:44
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_18_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 18 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 22 (Receiver 0 ParallelCollectionRDD[51] at start at AccessLogJob.scala:578)

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 22.0 with 1 tasks

2019-05-25 16:48:44
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 22.0 (TID 614, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 22.0 (TID 614)

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774124600

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:44
[INFO]-[Thread: Thread-47]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:44
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774124438-fc4176bc], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@479bb53e

2019-05-25 16:48:44
[INFO]-[Thread: ZkClient-EventThread-144-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:44
[ERROR]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774124800

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:44
[INFO]-[Thread: Thread-47]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:44
[INFO]-[Thread: Thread-47]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:44
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 22.0 (TID 614)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:44
[WARN]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 22.0 (TID 614, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:44
[ERROR]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 22.0 failed 1 times; aborting job

2019-05-25 16:48:44
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 22.0, whose tasks have all completed, from pool 

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 22

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 22 (start at AccessLogJob.scala:578) failed in 0.378 s

2019-05-25 16:48:44
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 614, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:44
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:44
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 20 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 23 (start at AccessLogJob.scala:578)

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 23 (Receiver 0 ParallelCollectionRDD[52] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_19 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_19_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:44
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_19_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 19 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 23 (Receiver 0 ParallelCollectionRDD[52] at start at AccessLogJob.scala:578)

2019-05-25 16:48:44
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 23.0 with 1 tasks

2019-05-25 16:48:44
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 23.0 (TID 615, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 23.0 (TID 615)

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774125000

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:44
[INFO]-[Thread: Thread-49]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:44
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774124848-abd022a5], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@543fee6f

2019-05-25 16:48:44
[INFO]-[Thread: ZkClient-EventThread-147-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:44
[ERROR]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:44
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774125200

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:45
[INFO]-[Thread: Thread-49]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:45
[INFO]-[Thread: Thread-49]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:45
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:45
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 23.0 (TID 615)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:45
[WARN]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 23.0 (TID 615, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:45
[ERROR]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 23.0 failed 1 times; aborting job

2019-05-25 16:48:45
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 23.0, whose tasks have all completed, from pool 

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 23

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 23 (start at AccessLogJob.scala:578) failed in 0.373 s

2019-05-25 16:48:45
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 615, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:45
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:45
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 21 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 24 (start at AccessLogJob.scala:578)

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 24 (Receiver 0 ParallelCollectionRDD[53] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_20 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_20_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:45
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_20_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 20 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 24 (Receiver 0 ParallelCollectionRDD[53] at start at AccessLogJob.scala:578)

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 24.0 with 1 tasks

2019-05-25 16:48:45
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 24.0 (TID 616, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 24.0 (TID 616)

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774125400

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:45
[INFO]-[Thread: Thread-51]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:45
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774125242-3884271c], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@60293932

2019-05-25 16:48:45
[INFO]-[Thread: ZkClient-EventThread-150-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:45
[ERROR]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774125600

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:45
[INFO]-[Thread: Thread-51]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:45
[INFO]-[Thread: Thread-51]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:45
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:45
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 24.0 (TID 616)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:45
[WARN]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 24.0 (TID 616, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:45
[ERROR]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 24.0 failed 1 times; aborting job

2019-05-25 16:48:45
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 24.0, whose tasks have all completed, from pool 

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 24

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 24 (start at AccessLogJob.scala:578) failed in 0.378 s

2019-05-25 16:48:45
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 616, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:45
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:45
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 22 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 25 (start at AccessLogJob.scala:578)

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 25 (Receiver 0 ParallelCollectionRDD[54] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_21 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:45
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_21_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 21 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 25 (Receiver 0 ParallelCollectionRDD[54] at start at AccessLogJob.scala:578)

2019-05-25 16:48:45
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 25.0 with 1 tasks

2019-05-25 16:48:45
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 25.0 (TID 617, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 25.0 (TID 617)

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774125800

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:45
[INFO]-[Thread: Thread-53]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:45
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774125638-f9c20f5f], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@379fd241

2019-05-25 16:48:45
[INFO]-[Thread: ZkClient-EventThread-153-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:45
[ERROR]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:45
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774126000

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:46
[INFO]-[Thread: Thread-53]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:46
[INFO]-[Thread: Thread-53]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:46
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 25.0 (TID 617)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:46
[WARN]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 25.0 (TID 617, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:46
[ERROR]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 25.0 failed 1 times; aborting job

2019-05-25 16:48:46
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 25.0, whose tasks have all completed, from pool 

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 25

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 25 (start at AccessLogJob.scala:578) failed in 0.384 s

2019-05-25 16:48:46
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 617, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:46
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:46
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 23 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 26 (start at AccessLogJob.scala:578)

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 26 (Receiver 0 ParallelCollectionRDD[55] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_22 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:46
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_22_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 22 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 26 (Receiver 0 ParallelCollectionRDD[55] at start at AccessLogJob.scala:578)

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 26.0 with 1 tasks

2019-05-25 16:48:46
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 26.0 (TID 618, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 26.0 (TID 618)

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774126200

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:46
[INFO]-[Thread: Thread-55]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:46
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774126049-1f2c4885], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@29fc8df5

2019-05-25 16:48:46
[INFO]-[Thread: ZkClient-EventThread-156-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:46
[ERROR]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774126400

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:46
[INFO]-[Thread: Thread-55]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:46
[INFO]-[Thread: Thread-55]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:46
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 26.0 (TID 618)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:46
[WARN]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 26.0 (TID 618, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:46
[ERROR]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 26.0 failed 1 times; aborting job

2019-05-25 16:48:46
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 26.0, whose tasks have all completed, from pool 

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 26

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 26 (start at AccessLogJob.scala:578) failed in 0.370 s

2019-05-25 16:48:46
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 618, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:46
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:46
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 24 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 27 (start at AccessLogJob.scala:578)

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 27 (Receiver 0 ParallelCollectionRDD[56] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_23 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_23_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:46
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_23_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 23 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 27 (Receiver 0 ParallelCollectionRDD[56] at start at AccessLogJob.scala:578)

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 27.0 with 1 tasks

2019-05-25 16:48:46
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 27.0 (TID 619, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 27.0 (TID 619)

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774126600

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:46
[INFO]-[Thread: Thread-57]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:46
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774126437-cd4eaec6], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@748406cf

2019-05-25 16:48:46
[INFO]-[Thread: ZkClient-EventThread-159-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:46
[ERROR]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774126800

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:46
[INFO]-[Thread: Thread-57]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:46
[INFO]-[Thread: Thread-57]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:46
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 27.0 (TID 619)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:46
[WARN]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 27.0 (TID 619, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:46
[ERROR]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 27.0 failed 1 times; aborting job

2019-05-25 16:48:46
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 27.0, whose tasks have all completed, from pool 

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 27

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 27 (start at AccessLogJob.scala:578) failed in 0.382 s

2019-05-25 16:48:46
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 619, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:46
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:46
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 25 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 28 (start at AccessLogJob.scala:578)

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 28 (Receiver 0 ParallelCollectionRDD[57] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_24 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_24_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:46
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_24_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 24 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 28 (Receiver 0 ParallelCollectionRDD[57] at start at AccessLogJob.scala:578)

2019-05-25 16:48:46
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 28.0 with 1 tasks

2019-05-25 16:48:46
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 28.0 (TID 620, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 28.0 (TID 620)

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774127000

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:46
[INFO]-[Thread: Thread-59]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:46
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774126840-ff6e01bf], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@4c7a8d86

2019-05-25 16:48:46
[INFO]-[Thread: ZkClient-EventThread-162-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:46
[ERROR]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:46
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774127200

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:47
[INFO]-[Thread: Thread-59]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:47
[INFO]-[Thread: Thread-59]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:47
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:47
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 28.0 (TID 620)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:47
[WARN]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 28.0 (TID 620, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:47
[ERROR]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 28.0 failed 1 times; aborting job

2019-05-25 16:48:47
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 28.0, whose tasks have all completed, from pool 

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 28

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 28 (start at AccessLogJob.scala:578) failed in 0.379 s

2019-05-25 16:48:47
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 620, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:47
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:47
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 26 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 29 (start at AccessLogJob.scala:578)

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 29 (Receiver 0 ParallelCollectionRDD[58] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_25 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_25_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:47
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_25_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 25 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 29 (Receiver 0 ParallelCollectionRDD[58] at start at AccessLogJob.scala:578)

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 29.0 with 1 tasks

2019-05-25 16:48:47
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 29.0 (TID 621, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 29.0 (TID 621)

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774127400

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:47
[INFO]-[Thread: Thread-61]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:47
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774127246-1db8c96d], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@20d0ac06

2019-05-25 16:48:47
[INFO]-[Thread: ZkClient-EventThread-165-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:47
[ERROR]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774127600

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:47
[INFO]-[Thread: Thread-61]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:47
[INFO]-[Thread: Thread-61]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:47
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:47
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 29.0 (TID 621)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:47
[WARN]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 29.0 (TID 621, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:47
[ERROR]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 29.0 failed 1 times; aborting job

2019-05-25 16:48:47
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 29.0, whose tasks have all completed, from pool 

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 29

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 29 (start at AccessLogJob.scala:578) failed in 0.373 s

2019-05-25 16:48:47
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 621, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:47
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:47
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 27 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 30 (start at AccessLogJob.scala:578)

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 30 (Receiver 0 ParallelCollectionRDD[59] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_26 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_26_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:47
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_26_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 26 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 30 (Receiver 0 ParallelCollectionRDD[59] at start at AccessLogJob.scala:578)

2019-05-25 16:48:47
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 30.0 with 1 tasks

2019-05-25 16:48:47
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 30.0 (TID 622, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 30.0 (TID 622)

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774127800

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:47
[INFO]-[Thread: Thread-63]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:47
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774127635-d1388ecc], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@3969e244

2019-05-25 16:48:47
[INFO]-[Thread: ZkClient-EventThread-168-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:47
[ERROR]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:47
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774128000

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:48
[INFO]-[Thread: Thread-63]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:48
[INFO]-[Thread: Thread-63]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:48
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 30.0 (TID 622)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:48
[WARN]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 30.0 (TID 622, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:48
[ERROR]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 30.0 failed 1 times; aborting job

2019-05-25 16:48:48
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 30.0, whose tasks have all completed, from pool 

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 30

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 30 (start at AccessLogJob.scala:578) failed in 0.385 s

2019-05-25 16:48:48
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 622, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:48
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:48
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 28 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 31 (start at AccessLogJob.scala:578)

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 31 (Receiver 0 ParallelCollectionRDD[60] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_27 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_27_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:48
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_27_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 27 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 31 (Receiver 0 ParallelCollectionRDD[60] at start at AccessLogJob.scala:578)

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 31.0 with 1 tasks

2019-05-25 16:48:48
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 31.0 (TID 623, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 31.0 (TID 623)

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774128200

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:48
[INFO]-[Thread: Thread-65]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:48
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774128045-3871341f], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@6e588e8e

2019-05-25 16:48:48
[INFO]-[Thread: ZkClient-EventThread-171-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:48
[ERROR]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774128400

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:48
[INFO]-[Thread: Thread-65]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:48
[INFO]-[Thread: Thread-65]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:48
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 31.0 (TID 623)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:48
[WARN]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 31.0 (TID 623, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:48
[ERROR]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 31.0 failed 1 times; aborting job

2019-05-25 16:48:48
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 31.0, whose tasks have all completed, from pool 

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 31

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 31 (start at AccessLogJob.scala:578) failed in 0.369 s

2019-05-25 16:48:48
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 623, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:48
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:48
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 29 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 32 (start at AccessLogJob.scala:578)

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 32 (Receiver 0 ParallelCollectionRDD[61] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_28 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_28_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:48
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_28_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 28 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 32 (Receiver 0 ParallelCollectionRDD[61] at start at AccessLogJob.scala:578)

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 32.0 with 1 tasks

2019-05-25 16:48:48
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 32.0 (TID 624, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 32.0 (TID 624)

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774128600

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:48
[INFO]-[Thread: Thread-67]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:48
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774128434-3fa8d446], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@4f1b1256

2019-05-25 16:48:48
[INFO]-[Thread: ZkClient-EventThread-174-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:48
[ERROR]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774128800

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:48
[INFO]-[Thread: Thread-67]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:48
[INFO]-[Thread: Thread-67]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:48
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 32.0 (TID 624)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:48
[WARN]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 32.0 (TID 624, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:48
[ERROR]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 32.0 failed 1 times; aborting job

2019-05-25 16:48:48
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 32.0, whose tasks have all completed, from pool 

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 32

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 32 (start at AccessLogJob.scala:578) failed in 0.383 s

2019-05-25 16:48:48
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 624, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:48
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:48
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 30 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 33 (start at AccessLogJob.scala:578)

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 33 (Receiver 0 ParallelCollectionRDD[62] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_29 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:48
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_29_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 29 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 33 (Receiver 0 ParallelCollectionRDD[62] at start at AccessLogJob.scala:578)

2019-05-25 16:48:48
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 33.0 with 1 tasks

2019-05-25 16:48:48
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 33.0 (TID 625, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 33.0 (TID 625)

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774129000

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:48
[INFO]-[Thread: Thread-69]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:48
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774128842-d433b061], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:48
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@5c3e1ebe

2019-05-25 16:48:48
[INFO]-[Thread: ZkClient-EventThread-177-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:49
[INFO]-[Thread: ZkClient-EventThread-177-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Terminate ZkClient event thread.

2019-05-25 16:48:49
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:49
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:49
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:49
[ERROR]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:49
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:49
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774130200

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:50
[INFO]-[Thread: Thread-69]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:50
[INFO]-[Thread: Thread-69]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:50
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:50
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 33.0 (TID 625)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:50
[WARN]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 33.0 (TID 625, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:50
[ERROR]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 33.0 failed 1 times; aborting job

2019-05-25 16:48:50
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 33.0, whose tasks have all completed, from pool 

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 33

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 33 (start at AccessLogJob.scala:578) failed in 1.379 s

2019-05-25 16:48:50
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 625, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1295)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1248)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:50
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 31 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 34 (start at AccessLogJob.scala:578)

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 34 (Receiver 0 ParallelCollectionRDD[63] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_30 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_30_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_30_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 30 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 34 (Receiver 0 ParallelCollectionRDD[63] at start at AccessLogJob.scala:578)

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 34.0 with 1 tasks

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 34.0 (TID 626, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 34.0 (TID 626)

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774130400

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:50
[INFO]-[Thread: Thread-71]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774130240-2d93e0c5], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@33e4c776

2019-05-25 16:48:50
[INFO]-[Thread: ZkClient-EventThread-180-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:50
[ERROR]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774130600

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:50
[INFO]-[Thread: Thread-71]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:50
[INFO]-[Thread: Thread-71]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:50
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:50
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 34.0 (TID 626)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:50
[WARN]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 34.0 (TID 626, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:50
[ERROR]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 34.0 failed 1 times; aborting job

2019-05-25 16:48:50
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 34.0, whose tasks have all completed, from pool 

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 34

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 34 (start at AccessLogJob.scala:578) failed in 0.373 s

2019-05-25 16:48:50
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 626, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:50
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 32 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 35 (start at AccessLogJob.scala:578)

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 35 (Receiver 0 ParallelCollectionRDD[64] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_31 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_31_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_31_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 31 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 35 (Receiver 0 ParallelCollectionRDD[64] at start at AccessLogJob.scala:578)

2019-05-25 16:48:50
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 35.0 with 1 tasks

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 35.0 (TID 627, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 35.0 (TID 627)

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774130800

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 33

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:50
[INFO]-[Thread: Thread-73]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_12_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 32

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774130670-af5de092], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@539743f9

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_11_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 31

2019-05-25 16:48:50
[INFO]-[Thread: ZkClient-EventThread-183-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:50
[ERROR]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_10_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:50
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 30

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_9_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 29

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_8_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 28

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_7_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 27

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_6_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 40

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_19_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 39

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_18_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 38

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_17_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 37

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_16_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 36

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_15_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 35

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_14_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 34

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_13_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_30_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 50

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_29_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 49

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_28_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 48

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_27_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 26

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_5_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 25

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_25_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 45

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_24_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 44

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_23_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 43

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_22_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 42

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_21_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 41

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_20_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 47

2019-05-25 16:48:50
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Removed broadcast_26_piece0 on localhost:60954 in memory (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:50
[INFO]-[Thread: Spark Context Cleaner]-[org.apache.spark.Logging$class.logInfo()]: Cleaned accumulator 46

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774131000

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:51
[INFO]-[Thread: Thread-73]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:51
[INFO]-[Thread: Thread-73]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:51
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 35.0 (TID 627)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:51
[WARN]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 35.0 (TID 627, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:51
[ERROR]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 35.0 failed 1 times; aborting job

2019-05-25 16:48:51
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 35.0, whose tasks have all completed, from pool 

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 35

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 35 (start at AccessLogJob.scala:578) failed in 0.367 s

2019-05-25 16:48:51
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 627, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:51
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:51
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 33 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 36 (start at AccessLogJob.scala:578)

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 36 (Receiver 0 ParallelCollectionRDD[65] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_32 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_32_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:51
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_32_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 32 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 36 (Receiver 0 ParallelCollectionRDD[65] at start at AccessLogJob.scala:578)

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 36.0 with 1 tasks

2019-05-25 16:48:51
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 36.0 (TID 628, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 36.0 (TID 628)

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774131200

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:51
[INFO]-[Thread: Thread-75]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:51
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774131050-d7490b36], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@2a099e7c

2019-05-25 16:48:51
[INFO]-[Thread: ZkClient-EventThread-186-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:51
[ERROR]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774131400

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:51
[INFO]-[Thread: Thread-75]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:51
[INFO]-[Thread: Thread-75]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:51
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 36.0 (TID 628)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:51
[WARN]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 36.0 (TID 628, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:51
[ERROR]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 36.0 failed 1 times; aborting job

2019-05-25 16:48:51
[INFO]-[Thread: task-result-getter-0]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 36.0, whose tasks have all completed, from pool 

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 36

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 36 (start at AccessLogJob.scala:578) failed in 0.396 s

2019-05-25 16:48:51
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 628, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:51
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:51
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 34 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 37 (start at AccessLogJob.scala:578)

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 37 (Receiver 0 ParallelCollectionRDD[66] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_33 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_33_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:51
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_33_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 33 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 37 (Receiver 0 ParallelCollectionRDD[66] at start at AccessLogJob.scala:578)

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 37.0 with 1 tasks

2019-05-25 16:48:51
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 37.0 (TID 629, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 37.0 (TID 629)

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774131600

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:51
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:51
[INFO]-[Thread: Thread-77]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774131491-d47f42b2], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@590a9242

2019-05-25 16:48:51
[INFO]-[Thread: ZkClient-EventThread-189-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:51
[ERROR]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774131800

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:51
[INFO]-[Thread: Thread-77]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:51
[INFO]-[Thread: Thread-77]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:51
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 37.0 (TID 629)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:51
[WARN]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 37.0 (TID 629, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:51
[ERROR]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 37.0 failed 1 times; aborting job

2019-05-25 16:48:51
[INFO]-[Thread: task-result-getter-1]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 37.0, whose tasks have all completed, from pool 

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 37

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 37 (start at AccessLogJob.scala:578) failed in 0.329 s

2019-05-25 16:48:51
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 629, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:51
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:51
[INFO]-[Thread: dispatcher-event-loop-2]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 35 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 38 (start at AccessLogJob.scala:578)

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 38 (Receiver 0 ParallelCollectionRDD[67] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_34 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_34_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:51
[INFO]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_34_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 34 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 38 (Receiver 0 ParallelCollectionRDD[67] at start at AccessLogJob.scala:578)

2019-05-25 16:48:51
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 38.0 with 1 tasks

2019-05-25 16:48:51
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 38.0 (TID 630, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 38.0 (TID 630)

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774132000

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:51
[INFO]-[Thread: Thread-79]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:51
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774131856-daa71d50], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@2055483d

2019-05-25 16:48:51
[INFO]-[Thread: ZkClient-EventThread-192-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:51
[ERROR]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:51
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774132200

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:52
[INFO]-[Thread: Thread-79]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:52
[INFO]-[Thread: Thread-79]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:52
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:52
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 38.0 (TID 630)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:52
[WARN]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 38.0 (TID 630, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:52
[ERROR]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 38.0 failed 1 times; aborting job

2019-05-25 16:48:52
[INFO]-[Thread: task-result-getter-2]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 38.0, whose tasks have all completed, from pool 

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 38

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 38 (start at AccessLogJob.scala:578) failed in 0.369 s

2019-05-25 16:48:52
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 630, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:52
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:52
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 36 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 39 (start at AccessLogJob.scala:578)

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 39 (Receiver 0 ParallelCollectionRDD[68] at start at AccessLogJob.scala:578), which has no missing parents

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_35 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_35_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:52
[INFO]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_35_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 35 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 39 (Receiver 0 ParallelCollectionRDD[68] at start at AccessLogJob.scala:578)

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 39.0 with 1 tasks

2019-05-25 16:48:52
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 39.0 (TID 631, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 39.0 (TID 631)

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774132400

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:52
[INFO]-[Thread: dispatcher-event-loop-0]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774132271-22957b33], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@7b88efc1

2019-05-25 16:48:52
[INFO]-[Thread: Thread-81]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:52
[INFO]-[Thread: ZkClient-EventThread-195-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:52
[ERROR]-[Thread: dispatcher-event-loop-3]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped timer for BlockGenerator after time 1558774132600

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for block pushing thread to terminate

2019-05-25 16:48:52
[INFO]-[Thread: Thread-81]-[org.apache.spark.Logging$class.logInfo()]: Pushing out the last 0 blocks

2019-05-25 16:48:52
[INFO]-[Thread: Thread-81]-[org.apache.spark.Logging$class.logInfo()]: Stopped block pushing thread

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped BlockGenerator

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Waiting for receiver to be stopped

2019-05-25 16:48:52
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Stopped receiver with error: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:52
[ERROR]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logError()]: Exception in task 0.0 in stage 39.0 (TID 631)
org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:52
[WARN]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logWarning()]: Lost task 0.0 in stage 39.0 (TID 631, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:52
[ERROR]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logError()]: Task 0 in stage 39.0 failed 1 times; aborting job

2019-05-25 16:48:52
[INFO]-[Thread: task-result-getter-3]-[org.apache.spark.Logging$class.logInfo()]: Removed TaskSet 39.0, whose tasks have all completed, from pool 

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Cancelling stage 39

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: ResultStage 39 (start at AccessLogJob.scala:578) failed in 0.362 s

2019-05-25 16:48:52
[ERROR]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logError()]: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 39.0 failed 1 times, most recent failure: Lost task 0.0 in stage 39.0 (TID 631, localhost): org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more

2019-05-25 16:48:52
[INFO]-[Thread: submit-job-thread-pool-0]-[org.apache.spark.Logging$class.logInfo()]: Restarting Receiver 0

2019-05-25 16:48:52
[INFO]-[Thread: dispatcher-event-loop-7]-[org.apache.spark.Logging$class.logInfo()]: Receiver 0 started

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Got job 37 (start at AccessLogJob.scala:578) with 1 output partitions

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Final stage: ResultStage 40 (start at AccessLogJob.scala:578)

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Parents of final stage: List()

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Missing parents: List()

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting ResultStage 40 (Receiver 0 ParallelCollectionRDD[69] at makeRDD at ReceiverTracker.scala:588), which has no missing parents

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_36 stored as values in memory (estimated size 57.8 KB, free 2.4 GB)

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Block broadcast_36_piece0 stored as bytes in memory (estimated size 19.2 KB, free 2.4 GB)

2019-05-25 16:48:52
[INFO]-[Thread: dispatcher-event-loop-6]-[org.apache.spark.Logging$class.logInfo()]: Added broadcast_36_piece0 in memory on localhost:60954 (size: 19.2 KB, free: 2.4 GB)

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Created broadcast 36 from broadcast at DAGScheduler.scala:1006

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Submitting 1 missing tasks from ResultStage 40 (Receiver 0 ParallelCollectionRDD[69] at makeRDD at ReceiverTracker.scala:588)

2019-05-25 16:48:52
[INFO]-[Thread: dag-scheduler-event-loop]-[org.apache.spark.Logging$class.logInfo()]: Adding task set 40.0 with 1 tasks

2019-05-25 16:48:52
[INFO]-[Thread: dispatcher-event-loop-5]-[org.apache.spark.Logging$class.logInfo()]: Starting task 0.0 in stage 40.0 (TID 632, localhost, partition 0,PROCESS_LOCAL, 3024 bytes)

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Running task 0.0 in stage 40.0 (TID 632)

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started timer for BlockGenerator at time 1558774132800

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Started BlockGenerator

2019-05-25 16:48:52
[INFO]-[Thread: Thread-83]-[org.apache.spark.Logging$class.logInfo()]: Started block pushing thread

2019-05-25 16:48:52
[INFO]-[Thread: dispatcher-event-loop-1]-[org.apache.spark.Logging$class.logInfo()]: Registered receiver for stream 0 from 10.12.0.198:60928

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting receiver

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Starting Kafka Consumer Stream with group: xc-zuul-access-log-kafka-group

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Connecting to Zookeeper: hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Verifying properties

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property group.id is overridden to xc-zuul-access-log-kafka-group

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connect is overridden to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: Property zookeeper.connection.timeout.ms is overridden to 10000

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[kafka.utils.Logging$class.info()]: [xc-zuul-access-log-kafka-group_LAPTOP-NFE4JUQT-1558774132632-297a9bdb], Connecting to zookeeper instance at hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.zookeeper.ZooKeeper.<init>()]: Initiating client connection, connectString=hlg-5p26-wangyongzhi:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@659d7e1d

2019-05-25 16:48:52
[INFO]-[Thread: ZkClient-EventThread-198-hlg-5p26-wangyongzhi:2181]-[org.I0Itec.zkclient.ZkEventThread.run()]: Starting ZkClient event thread.

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping receiver with message: Error starting receiver 0: org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Called receiver onStop

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Deregistering receiver 0

2019-05-25 16:48:52
[ERROR]-[Thread: dispatcher-event-loop-4]-[org.apache.spark.Logging$class.logError()]: Deregistered receiver for stream 0: Error starting receiver 0 - org.I0Itec.zkclient.exception.ZkException: Unable to connect to hlg-5p26-wangyongzhi:2181
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:66)
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:171)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:126)
	at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:143)
	at kafka.consumer.Consumer$.create(ConsumerConnector.scala:94)
	at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:575)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:565)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.SparkContext$$anonfun$37.apply(SparkContext.scala:1992)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.UnknownHostException: hlg-5p26-wangyongzhi
	at java.net.InetAddress.getAllByName0(InetAddress.java:1252)
	at java.net.InetAddress.getAllByName(InetAddress.java:1164)
	at java.net.InetAddress.getAllByName(InetAddress.java:1098)
	at org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.I0Itec.zkclient.ZkConnection.connect(ZkConnection.java:64)
	... 20 more


2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopped receiver 0

2019-05-25 16:48:52
[INFO]-[Thread: Executor task launch worker-0]-[org.apache.spark.Logging$class.logInfo()]: Stopping BlockGenerator
